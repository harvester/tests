<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>harvester_e2e_tests.integration.test_vm_functions API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>harvester_e2e_tests.integration.test_vm_functions</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from time import sleep
from datetime import datetime, timedelta
from contextlib import contextmanager

import json
import re
import pytest
import yaml
from paramiko.ssh_exception import ChannelException

pytest_plugins = [
    &#34;harvester_e2e_tests.fixtures.api_client&#34;,
    &#34;harvester_e2e_tests.fixtures.images&#34;,
    &#34;harvester_e2e_tests.fixtures.virtualmachines&#34;
]

# GLOBAL Vars:
MAX = 999999


@pytest.fixture(scope=&#39;module&#39;)
def bogus_vlan_net(request, api_client):
    &#34;&#34;&#34;bogus vlan network fixture (no dhcp) on mgmt network

    Args:
        request (FixtureRequest): https://docs.pytest.org/en/7.1.x/_modules/_pytest/fixtures.html#FixtureRequest # noqa
        api_client (HarvesterAPI): HarvesterAPI client

    Yields:
        dict: created bogus network attachment definition dictionary
    &#34;&#34;&#34;
    original_vlan_id = request.config.getoption(&#39;--vlan-id&#39;)

    existing_vm_net_code, existing_vm_net_data = api_client.networks.get()
    assert existing_vm_net_code == 200, &#39;we should be able to fetch vm networks from harvester&#39;
    existing_vm_net_list = existing_vm_net_data.get(&#39;items&#39;, [])
    vlans_to_exclude = set()
    vlans_to_exclude.add(1)
    for existing_vm_net in existing_vm_net_list:
        existing_vm_net_config = existing_vm_net.get(&#39;spec&#39;, {}).get(&#39;config&#39;, &#39;{}&#39;)
        assert existing_vm_net_config != &#39;{}&#39;, &#39;existing vm net should exist&#39;
        existing_vm_net_config_dict = json.loads(existing_vm_net_config)
        assert existing_vm_net_config_dict.get(
            &#39;vlan&#39;, 0) != 0, &#39;we should be able to get the vlan off the config&#39;
        existing_vm_net_vlan = existing_vm_net_config_dict.get(&#39;vlan&#39;)
        vlans_to_exclude.add(existing_vm_net_vlan)

    if original_vlan_id != -1:
        vlans_to_exclude.add(original_vlan_id)

    vlan_ids = set(range(2, 4095))  # 4094 is the last, 1 should always be excluded.
    code, data = api_client.networks.get()
    for net in data[&#39;items&#39;]:
        config = json.loads(net[&#39;spec&#39;].get(&#39;config&#39;, &#39;{}&#39;))
        if config.get(&#39;vlan&#39;):
            try:
                # try to remove the key, but VLAN may be used in both &#39;mgmt&#39;
                # and other cluster network(s) so it might have already been removed
                vlan_ids.remove(config[&#39;vlan&#39;])
            except KeyError:
                print(f&#34;key, {config[&#39;vlan&#39;]} was already removed by another cluster network&#34;)

    vlan_id = vlan_ids.pop()  # Remove and return an arbitrary set element.
    vm_network_name = f&#39;bogus-net-{vlan_id}&#39;
    code, data = api_client.networks.create(vm_network_name, vlan_id)
    assert 201 == code, (
        f&#34;Failed to create N.A.D. {vm_network_name} with error {code}, {data}&#34;
    )

    yield data

    api_client.networks.delete(vm_network_name)


@pytest.fixture(scope=&#34;module&#34;)
def image(api_client, image_opensuse, unique_name, wait_timeout):
    unique_image_id = f&#39;image-{unique_name}&#39;
    code, data = api_client.images.create_by_url(
        unique_image_id, image_opensuse.url, display_name=f&#34;{unique_name}-{image_opensuse.name}&#34;
    )

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.images.get(unique_image_id)
        if 100 == data.get(&#39;status&#39;, {}).get(&#39;progress&#39;, 0):
            break
        sleep(3)
    else:
        raise AssertionError(
            &#34;Failed to create Image with error:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    yield dict(id=f&#34;{data[&#39;metadata&#39;][&#39;namespace&#39;]}/{unique_image_id}&#34;,
               user=image_opensuse.ssh_user)

    code, data = api_client.images.delete(unique_image_id)


@pytest.fixture(scope=&#34;module&#34;)
def unique_vm_name(unique_name):
    return f&#34;vm-{unique_name}&#34;


@pytest.fixture(scope=&#34;class&#34;)
def small_volume(api_client, unique_name):
    vol_name, size = f&#34;sv-{unique_name}&#34;, 3
    vol_spec = api_client.volumes.Spec(size)
    code, data = api_client.volumes.create(vol_name, vol_spec)

    assert 201 == code, (code, data)

    yield vol_name, size

    code, data = api_client.volumes.delete(vol_name)


@pytest.fixture(scope=&#34;class&#34;)
def stopped_vm(api_client, ssh_keypair, wait_timeout, image, unique_vm_name):
    unique_vm_name = f&#34;stopped-{datetime.now().strftime(&#39;%m%S%f&#39;)}-{unique_vm_name}&#34;
    cpu, mem = 1, 2
    pub_key, pri_key = ssh_keypair
    vm_spec = api_client.vms.Spec(cpu, mem)
    vm_spec.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])
    vm_spec.run_strategy = &#34;Halted&#34;

    userdata = yaml.safe_load(vm_spec.user_data)
    userdata[&#39;ssh_authorized_keys&#39;] = [pub_key]
    vm_spec.user_data = yaml.dump(userdata)

    code, data = api_client.vms.create(unique_vm_name, vm_spec)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;):
            break
        sleep(1)

    yield unique_vm_name, image[&#39;user&#39;]

    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)

    api_client.vms.delete(unique_vm_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 404 == code:
            break
        sleep(3)

    for vol in vm_spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        api_client.volumes.delete(vol_name)


@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.dependency(name=&#34;minimal_vm&#34;)
def test_minimal_vm(api_client, image, unique_vm_name, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/ # noqa

    Steps:
        1. Create a VM with 1 CPU 2 Memory and other default values
        2. Save
    Exepected Result:
        - VM should created
        - VM should Started
    &#34;&#34;&#34;
    cpu, mem = 1, 2
    vm = api_client.vms.Spec(cpu, mem)
    vm.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])

    code, data = api_client.vms.create(unique_vm_name, vm)

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code and &#34;Running&#34; == data.get(&#39;status&#39;, {}).get(&#39;phase&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to create Minimal VM({cpu} core, {mem} RAM) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )


@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.dependency(depends=[&#34;minimal_vm&#34;])
class TestVMOperations:
    &#34;&#34;&#34;
    To cover tests:
    - https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/ # noqa
    &#34;&#34;&#34;

    @pytest.mark.dependency(name=&#34;pause_vm&#34;, depends=[&#34;minimal_vm&#34;])
    def test_pause(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Pause the VM was created
        Exepected Result:
            - VM should change status into `Paused`
        &#39;&#39;&#39;
        code, data = api_client.vms.pause(unique_vm_name)
        assert 204 == code, &#34;`Pause` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if [c for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []) if &#34;Paused&#34; == c[&#39;type&#39;]]:
                conditions = data[&#39;status&#39;][&#39;conditions&#39;]
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to pause VM({unique_vm_name}) with errors:\n&#34;
                f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        assert &#34;Paused&#34; == conditions[-1].get(&#39;type&#39;), conditions
        assert &#34;PausedByUser&#34; == conditions[-1].get(&#39;reason&#39;), conditions

    @pytest.mark.dependency(depends=[&#34;pause_vm&#34;])
    def test_unpause(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Unpause the VM was paused
        Exepected Result:
            - VM&#39;s status should not be `Paused`
        &#39;&#39;&#39;
        code, data = api_client.vms.unpause(unique_vm_name)
        assert 204 == code, &#34;`Unpause` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            cond_types = set(c[&#39;type&#39;] for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []))
            if {&#34;AgentConnected&#34;} &amp; cond_types and not {&#34;Paused&#34;} &amp; cond_types:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to unpause VM({unique_vm_name}) with errors:\n&#34;
                f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    @pytest.mark.dependency(name=&#34;stop_vm&#34;, depends=[&#34;minimal_vm&#34;])
    def test_stop(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Stop the VM was created and not stopped
        Exepected Result:
            - VM&#39;s status should be changed to `Stopped`
            - VM&#39;s `RunStrategy` should be changed to `Halted`
        &#39;&#39;&#39;
        code, data = api_client.vms.stop(unique_vm_name)
        assert 204 == code, &#34;`Stop` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        code, data = api_client.vms.get(unique_vm_name)
        assert &#34;Halted&#34; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
        assert &#34;Stopped&#34; == data[&#39;status&#39;][&#39;printableStatus&#39;]

    @pytest.mark.dependency(name=&#34;start_vm&#34;, depends=[&#34;stop_vm&#34;])
    def test_start(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Start the VM was created and stopped
        Exepected Result:
            - VM should change status into `Running`
        &#39;&#39;&#39;
        code, data = api_client.vms.start(unique_vm_name)
        assert 204 == code, &#34;`Start return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            strategy = data[&#39;spec&#39;][&#39;runStrategy&#39;]
            pstats = data[&#39;status&#39;][&#39;printableStatus&#39;]
            if &#34;Halted&#34; != strategy and &#34;Running&#34; == pstats:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_restart(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Restart the VM was created
        Exepected Result:
            - VM&#39;s ActivePods should be updated (which means the VM restarted)
            - VM&#39;s status should update to `Running`
            - VM&#39;s qemu-agent should be connected
        &#39;&#39;&#39;
        code, data = api_client.vms.get_status(unique_vm_name)
        assert 200 == code, (
            f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

        old_pods = set(data[&#39;status&#39;][&#39;activePods&#39;].items())

        code, data = api_client.vms.restart(unique_vm_name)
        assert 204 == code, &#34;`Restart return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if old_pods.difference(data[&#39;status&#39;].get(&#39;activePods&#39;, old_pods).items()):
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to Restart VM({unique_vm_name}), activePods is not updated.\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Restart VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_softreboot(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Softreboot the VM was created
        Exepected Result:
            - VM&#39;s qemu-agent should disconnected (which means the VM rebooting)
            - VM&#39;s qemu-agent should re-connected (which means the VM boot into OS)
            - VM&#39;s status should be changed to `Running`
        &#39;&#39;&#39;
        code, data = api_client.vms.get_status(unique_vm_name)
        assert 200 == code, (
            f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
        old_agent = data[&#39;status&#39;][&#39;conditions&#39;][-1]
        assert &#34;AgentConnected&#34; == old_agent[&#39;type&#39;], (code, data)

        api_client.vms.softreboot(unique_vm_name)
        # Wait until agent disconnected (leaving OS)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if &#34;AgentConnected&#34; not in data[&#39;status&#39;][&#39;conditions&#39;][-1][&#39;type&#39;]:
                break
            sleep(5)
        # then wait agent connected again (Entering OS)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase, conds = data[&#39;status&#39;][&#39;phase&#39;], data[&#39;status&#39;].get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Softreboot VM({unique_vm_name}) with errors:\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        old_t = datetime.strptime(old_agent[&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)
        new_t = datetime.strptime(conds[-1][&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)

        assert new_t &gt; old_t, (
            &#34;Agent&#39;s probe time is not updated.\t&#34;
            f&#34;Before softreboot: {old_t}, After softreboot: {new_t}\n&#34;
            f&#34;Last API Status({code}): {data}&#34;
        )

    def test_migrate(self, api_client, unique_vm_name, wait_timeout):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/ # noqa

        Steps:
            1. migrate the VM was created
        Exepected Result:
            - VM&#39;s host Node should be changed to another one
        &#34;&#34;&#34;
        code, host_data = api_client.hosts.get()
        assert 200 == code, (code, host_data)
        code, data = api_client.vms.get_status(unique_vm_name)
        cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
        assert cur_host, (
            f&#34;VMI exists but `nodeName` is empty.\n&#34;
            f&#34;{data}&#34;
        )

        new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;]
                        if cur_host != h[&#39;id&#39;] and not h[&#39;spec&#39;].get(&#39;taint&#39;))

        code, data = api_client.vms.migrate(unique_vm_name, new_host)
        assert 204 == code, (code, data)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            migrating = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
            if not migrating and new_host == data[&#39;status&#39;][&#39;nodeName&#39;]:
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to Migrate VM({unique_vm_name}) from {cur_host} to {new_host}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_abort_migrate(self, api_client, unique_vm_name, wait_timeout):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/live-migration/abort-live-migration/

        Steps:
            1. Abort the VM was created and migrating
        Exepected Result:
            - VM should able to perform migrate
            - VM should stay in current host when migrating be aborted.
        &#34;&#34;&#34;
        code, host_data = api_client.hosts.get()
        assert 200 == code, (code, host_data)
        code, data = api_client.vms.get_status(unique_vm_name)
        cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
        assert cur_host, (
            f&#34;VMI exists but `nodeName` is empty.\n&#34;
            f&#34;{data}&#34;
        )

        new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;]
                        if cur_host != h[&#39;id&#39;] and not h[&#39;spec&#39;].get(&#39;taint&#39;))

        code, data = api_client.vms.migrate(unique_vm_name, new_host)
        assert 204 == code, (code, data)

        states = [&#34;Aborting migration&#34;, &#34;Migrating&#34;]
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            m_state = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
            if m_state == states[-1]:
                states.pop()
                if states:
                    code, err = api_client.vms.abort_migrate(unique_vm_name)
                    assert 204 == code, (code, err)
                else:
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to abort VM({unique_vm_name})&#39;s migration, stuck on {states[-1]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        assert cur_host == data[&#39;status&#39;][&#39;nodeName&#39;], (
            f&#34;Failed to abort VM({unique_vm_name})&#39;s migration,&#34;
            f&#34;VM been moved to {data[&#39;status&#39;][&#39;nodeName&#39;]} is not the origin host {cur_host}\n&#34;
        )

    def test_delete(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Delete the VM was created
            2. Delete Volumes was belonged to the VM
        Exepected Result:
            - VM should able to be deleted and success
            - Volumes should able to be deleted and success
        &#39;&#39;&#39;

        code, data = api_client.vms.delete(unique_vm_name)
        assert 200 == code, (code, data)

        spec = api_client.vms.Spec.from_dict(data)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        fails, check = [], dict()
        for vol in spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            check[vol_name] = api_client.volumes.delete(vol_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            l_check = dict()
            for vol_name, (code, data) in check.items():
                if 200 != code:
                    fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
                else:
                    code, data = api_client.volumes.get(vol_name)
                    if 404 != code:
                        l_check[vol_name] = (code, data)
            check = l_check
            if not check:
                break
            sleep(5)
        else:
            for vol_name, (code, data) in check.items():
                fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))

        assert not fails, (
            f&#34;Failed to delete VM({unique_vm_name})&#39;s volumes with errors:\n&#34;
            &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
        )


@pytest.mark.p0
@pytest.mark.virtualmachines
def test_create_stopped_vm(api_client, stopped_vm, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/ # noqa

    Steps:
        1. Create a VM with 1 CPU 2 Memory and runStrategy is `Halted`
        2. Save
    Exepected Result:
        - VM should created
        - VM should Stooped
        - VMI should not exist
    &#34;&#34;&#34;
    unique_vm_name, _ = stopped_vm
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if (code == 200
                and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Create a Stopped VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    code, data = api_client.vms.get_status(unique_vm_name)
    assert 404 == code, (code, data)


@pytest.mark.p0
@pytest.mark.virtualmachines
class TestVMClone:
    def test_clone_running_vm(self, api_client, ssh_keypair, wait_timeout,
                              host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/ # noqa
        - (new) https://github.com/harvester/tests/issues/361

        Steps:
            1. Create a VM with 1 CPU 2 Memory
            2. Start the VM and write some data
            3. Clone the VM into VM-cloned
            4. Verify VM-Cloned

        Exepected Result:
            - Cloned-VM should be available and starting
            - Cloned-VM should becomes `Running`
            - Written data should available in Cloned-VM
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.start(unique_vm_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into VM to make some data
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(f&#39;echo {unique_vm_name!r} &gt; ~/vmname&#39;)
                assert not err, (out, err)
                sh.exec_command(&#39;sync&#39;)

        # Clone VM into new VM
        cloned_name = f&#34;cloned-{unique_vm_name}&#34;
        code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
        assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

        # Check VM started
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(cloned_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into new VM to check VM is cloned as old one
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )

                out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
            assert unique_vm_name in out, (
                f&#34;cloud-init writefile failed\n&#34;
                f&#34;Executed stdout: {out}\n&#34;
                f&#34;Executed stderr: {err}&#34;
            )

        # Remove cloned VM and volumes
        code, data = api_client.vms.get(cloned_name)
        cloned_spec = api_client.vms.Spec.from_dict(data)
        api_client.vms.delete(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )
        for vol in cloned_spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)

    def test_clone_stopped_vm(self, api_client, ssh_keypair, wait_timeout,
                              host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/ # noqa
        - (new) https://github.com/harvester/tests/issues/361

        Steps:
            1. Create a VM with 1 CPU 2 Memory
            2. Start the VM and write some data
            3. Stop the VM
            4. Clone the VM into VM-cloned
            5. Verify VM-Cloned

        Exepected Result:
            - Cloned-VM should be available and stopped
            - Cloned-VM should able to start and becomes `Running`
            - Written data should available in Cloned-VM
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.start(unique_vm_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into VM to make some data
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(f&#39;echo &#34;stopped-{unique_vm_name}&#34; &gt; ~/vmname&#39;)
                assert not err, (out, err)
                sh.exec_command(&#39;sync&#39;)

        # Stop the VM
        code, data = api_client.vms.stop(unique_vm_name)
        assert 204 == code, &#34;`Stop` return unexpected status code&#34;
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        # Clone VM into new VM
        cloned_name = f&#34;cloned-{unique_vm_name}&#34;
        code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
        assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

        # Check cloned VM is available and stooped
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if (200 == code
               and &#34;Halted&#34; == data[&#39;spec&#39;].get(&#39;runStrategy&#39;)
               and &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Cloned VM {cloned_name} is not available and stopped&#34;
                f&#34;Status({code}): {data}&#34;
            )

        # Check cloned VM started
        api_client.vms.start(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(cloned_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into new VM to check VM is cloned as old one
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )

                out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
            assert f&#34;stopped-{unique_vm_name}&#34; in out, (
                f&#34;cloud-init writefile failed\n&#34;
                f&#34;Executed stdout: {out}\n&#34;
                f&#34;Executed stderr: {err}&#34;
            )

        # Remove cloned VM and volumes
        code, data = api_client.vms.get(cloned_name)
        cloned_spec = api_client.vms.Spec.from_dict(data)
        api_client.vms.delete(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )
        for vol in cloned_spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)


@pytest.mark.p0
@pytest.mark.virtualmachines
class TestVMWithVolumes:
    def test_create_with_two_volumes(self, api_client, ssh_keypair, wait_timeout,
                                     host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/ # noqa

        Steps:
            1. Create a VM with 1 CPU 2 Memory and 2 disk volumes
            2. Start the VM
            3. Verify the VM

        Exepected Result:
            - VM should able to start and becomes `Running`
            - 2 disk volumes should be available in the VM
            - Disk size in VM should be the same as its volume configured
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;RerunOnFailure&#34;
        volumes = [(&#39;disk-1&#39;, 1), (&#39;disk-2&#39;, 2)]
        for name, size in volumes:
            vm_spec.add_volume(name, size)

        # Start VM with 2 additional volumes
        code, data = api_client.vms.update(unique_vm_name, vm_spec)
        assert 200 == code, (code, data)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        # Log into VM to verify added volumes
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(&#34;lsblk -r&#34;)
                assert not err, (out, err)

        assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
            f&#34;Added Volumes amount is not correct.\n&#34;
            f&#34;lsblk output: {out}&#34;
        )
        fails = []
        for _, size in volumes:
            if not re.search(f&#34;vd.*{size}G 0 disk&#34;, out):
                fails.append(f&#34;Volume size {size}G not found&#34;)

        assert not fails, (
            f&#34;lsblk output: {out}\n&#34;
            &#34;\n&#34;.join(fails)
        )

        # Tear down: Stop VM and remove added volumes
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;Halted&#34;
        vol_names, vols, claims = [n for n, s in volumes], [], []
        for vd in vm_spec.volumes:
            if vd[&#39;disk&#39;][&#39;name&#39;] in vol_names:
                claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
            else:
                vols.append(vd)
        else:
            vm_spec.volumes = vols

        api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if (code == 200
                    and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                    and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)

        for vol_name in claims:
            api_client.volumes.delete(vol_name)

    def test_create_with_existing_volume(self, api_client, ssh_keypair, wait_timeout,
                                         host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/ # noqa

        Steps:
            1. Create a data volume
            2. Create a VM with 1 CPU 2 Memory and the existing data volume
            3. Start the VM
            4. Verify the VM

        Exepected Result:
            - VM should able to start and becomes `Running`
            - Disk volume should be available in the VM
            - Disk size in VM should be the same as its volume configured
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair

        vol_name, size = &#39;disk-existing&#39;, 3
        vol_spec = api_client.volumes.Spec(size)
        code, data = api_client.volumes.create(f&#34;{unique_vm_name}-{vol_name}&#34;, vol_spec)

        assert 201 == code, (code, data)

        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;RerunOnFailure&#34;
        vm_spec.add_existing_volume(vol_name, f&#34;{unique_vm_name}-{vol_name}&#34;)

        # Start VM with added existing volume
        code, data = api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        # Log into VM to verify added volumes
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(&#34;lsblk -r&#34;)
                assert not err, (out, err)

        assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
            f&#34;Added Volumes amount is not correct.\n&#34;
            f&#34;lsblk output: {out}&#34;
        )

        assert f&#34;{size}G 0 disk&#34; in out, (
            f&#34;existing Volume {size}G not found\n&#34;
            f&#34;lsblk output: {out}&#34;
        )

        # Tear down: Stop VM and remove added volumes
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;Halted&#34;
        vols, claims = [], []
        for vd in vm_spec.volumes:
            if vd[&#39;disk&#39;][&#39;name&#39;] == vol_name:
                claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
            else:
                vols.append(vd)
        else:
            vm_spec.volumes = vols

        api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if (code == 200
                    and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                    and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)

        for claim in claims:
            api_client.volumes.delete(claim)


@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.negative
@pytest.mark.parametrize(&#34;resource&#34;, [dict(cpu=MAX), dict(mem=MAX), dict(disk=MAX),
                                      dict(mem=MAX, cpu=MAX), dict(mem=MAX, cpu=MAX, disk=MAX)],
                         ids=[&#39;cpu&#39;, &#39;mem&#39;, &#39;disk&#39;, &#39;mem-and-cpu&#39;, &#39;mem-cpu-and-disk&#39;])
def test_create_vm_no_available_resources(resource, api_client, image,
                                          wait_timeout, unique_vm_name, sleep_timeout):
    &#34;&#34;&#34;Creates a VM with outlandish resources for varying elements (purposefully negative test)

    Prerequisite:
        Setting opensuse-image-url set to a valid URL for
        an opensuse image.

    Manual Test Doc(s):
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-cpu-not-in-cluster/ # noqa
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-memory-not-in-cluster/ # noqa
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-both-cpu-and-memory-not-in-cluster/ # noqa


    Args:
        request (FixtureRequest): https://docs.pytest.org/en/7.1.x/_modules/_pytest/fixtures.html#FixtureRequest # noqa
        resource (dict): dict of name(s) &amp; value that can be deconstructed
        api_client (HarvesterAPI): HarvesterAPI client
        image (str): corresponding image from fixture
        wait_timeout (int): seconds for wait timeout from fixture
        unique_vm_name (str): string of unique vm name

    Raises:
        AssertionError: when vm can not be created, all vms should be allowed to be created

    Steps:
    1. build vm object specs for outlandish resource(s) under test
    2. request to build the vm, assert that succeeds
    3. check for conditions of guest not running and vm being unschedulable
    4. delete vm and volumes

    Expected Result:
    - building vm with outlandish resource requests to be successful
    - asserting that the status condition of the vm that is built to not be running
    - asserting that the status condition of the vm that is built to be unschedulable
    - assert deleting vm and volumes to be successful
    &#34;&#34;&#34;
    unique_name_for_vm = f&#34;{&#39;&#39;.join(resource.keys())}-{unique_vm_name}&#34;
    overall_vm_obj = dict(cpu=1, mem=2, disk=10)
    overall_vm_obj.update(resource)

    vm = api_client.vms.Spec(overall_vm_obj[&#39;cpu&#39;], overall_vm_obj[&#39;mem&#39;])
    vm.add_image(&#34;disk-0&#34;, image[&#39;id&#39;], size=overall_vm_obj.get(&#39;disk&#39;))
    code, data = api_client.vms.create(unique_name_for_vm, vm)
    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name_for_vm)
        if 200 == code and len(data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [])) &gt; 1:
            checks = dict(GuestNotRunning=False, Unschedulable=False)
            for condition in data[&#39;status&#39;][&#39;conditions&#39;]:
                if condition.get(&#39;reason&#39;) in checks:
                    checks[condition[&#39;reason&#39;]] = True

            assert all(checks.values()), (
                &#34;The VM miss condition:\n&#34;
                &#34; and &#34;.join(k for k, v in checks.items() if not v)
            )
            code, data = api_client.vms.delete(unique_name_for_vm)
            assert 200 == code, (code, data)

            spec = api_client.vms.Spec.from_dict(data)
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to create VM({overall_vm_obj.get(&#39;cpu&#39;)} core, \n&#34;
            f&#34;{overall_vm_obj.get(&#39;mem&#39;)} RAM, \n&#34;
            f&#34;{overall_vm_obj.get(&#39;disk&#39;)} DISK) with errors:\n&#34;
            f&#34;Phase: {data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)}\t&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_name_for_vm)
        if 404 == code:
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({unique_name_for_vm}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    fails, check = [], dict()
    for vol in spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        check[vol_name] = api_client.volumes.delete(vol_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        l_check = dict()
        for vol_name, (code, data) in check.items():
            if 200 != code:
                fails.append(
                    (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
            else:
                code, data = api_client.volumes.get(vol_name)
                if 404 != code:
                    l_check[vol_name] = (code, data)
        check = l_check
        if not check:
            break
        sleep(sleep_timeout)
    else:
        for vol_name, (code, data) in check.items():
            fails.append(
                (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
    assert not fails, (
        f&#34;Failed to delete VM({unique_vm_name})&#39;s volumes with errors:\n&#34;
        &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
    )


@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.parametrize(&#34;machine_types&#34;, [(&#34;pc&#34;, &#34;q35&#34;), (&#34;q35&#34;, &#34;pc&#34;)],
                         ids=[&#39;pc_to_q35&#39;, &#39;q35_to_pc&#39;])
def test_update_vm_machine_type(api_client, image, unique_vm_name,
                                wait_timeout, machine_types, sleep_timeout):
    &#34;&#34;&#34;Create a VM with machine type then update to another

    Prerequisite:
        Setting opensuse-image-url set to a valid URL for
        an opensuse image.

    Manual Test Doc(s):
        - https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-pc/ # noqa
        - https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-q35/ # noqa

    Args:
        api_client (HarvesterAPI): HarvesterAPI client
        image (str): corresponding image from fixture
        wait_timeout (int): seconds for wait timeout from fixture
        unique_vm_name (str): fixture at module level based unique vm name
        machine_types (tuple)(str): deconstructed to provide starting type and desired end type

    Raises:
        AssertionError: failure to create, stop, update, or start

    Steps:
    1. build vm with starting machine type
    2. power down vm with starting machine type
    3. update vm from machine type starting to machine type ending
    4. power up vm
    5. delete vm and volumes

    Expected Result:
    - building a vm with machine type starting to be successful
    - powering down the vm with machine type starting to be successful
    - modifying the existing machine type starting and updating to ending to be successful
    - powering up the modified vm to be successful and that now has the machine type ending
    - deleting the vm to be successful
    &#34;&#34;&#34;
    cpu, mem = 1, 2
    starting_machine_type, ending_machine_type = machine_types
    vm = api_client.vms.Spec(cpu, mem)
    vm.machine_type = starting_machine_type

    vm.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])
    unique_name_for_vm = f&#34;{&#39;&#39;.join(starting_machine_type)}-{unique_vm_name}&#34;

    code, vm_create_data = api_client.vms.create(unique_name_for_vm, vm)

    assert 201 == code, (code, vm_create_data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name_for_vm)
        if 200 == code and &#34;Running&#34; == data.get(&#39;status&#39;, {}).get(&#39;phase&#39;):
            code, data = api_client.vms.stop(unique_name_for_vm)
            assert 204 == code, &#34;`Stop` return unexpected status code&#34;
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to create VM({cpu} core, {mem} RAM) with errors:\n&#34;
            f&#34;Phase: {data.get(&#39;status&#39;, {}).get(&#39;phase&#39;,&#39;&#39;)}\t&#34;
            f&#34;Status: {data.get(&#39;status&#39;, {})}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name_for_vm)
        if 404 == code:
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to Stop VM({unique_name_for_vm}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    code, data = api_client.vms.get(unique_name_for_vm)
    assert &#34;Halted&#34; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
    assert &#34;Stopped&#34; == data[&#39;status&#39;][&#39;printableStatus&#39;]
    code, vm_to_modify = api_client.vms.get(unique_name_for_vm)
    assert code == 200
    spec = api_client.vms.Spec.from_dict(vm_to_modify)
    spec.machine_type = ending_machine_type
    code, data = api_client.vms.update(unique_name_for_vm, spec)
    result = api_client.vms.Spec.from_dict(data)
    if 200 == code and result.machine_type == ending_machine_type:
        code, data = api_client.vms.start(unique_name_for_vm)
        assert 204 == code, &#34;`Start return unexpected status code&#34;
    else:
        raise AssertionError(
            f&#34;Failed to Update VM({unique_name_for_vm}) with errors:\n&#34;
            f&#34;Phase: {data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)}\t&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_name_for_vm)
        strategy = data[&#39;spec&#39;][&#39;runStrategy&#39;]
        pstats = data[&#39;status&#39;][&#39;printableStatus&#39;]
        if &#34;Halted&#34; != strategy and &#34;Running&#34; == pstats:
            code, data = api_client.vms.delete(unique_name_for_vm)
            assert 200 == code, (code, data)

            spec = api_client.vms.Spec.from_dict(data)
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_name_for_vm}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_name_for_vm)
        if 404 == code:
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({unique_name_for_vm}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    fails, check = [], dict()
    for vol in spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        check[vol_name] = api_client.volumes.delete(vol_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        l_check = dict()
        for vol_name, (code, data) in check.items():
            if 200 != code:
                fails.append(
                    (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
            else:
                code, data = api_client.volumes.get(vol_name)
                if 404 != code:
                    l_check[vol_name] = (code, data)
        check = l_check
        if not check:
            break
    else:
        for vol_name, (code, data) in check.items():
            fails.append(
                (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
    assert not fails, (
        f&#34;Failed to delete VM({unique_name_for_vm})&#39;s volumes with errors:\n&#34;
        &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
    )


@pytest.mark.p0
@pytest.mark.negative
@pytest.mark.virtualmachines
def test_vm_with_bogus_vlan(api_client, image, unique_vm_name,
                            wait_timeout, bogus_vlan_net, sleep_timeout):
    &#34;&#34;&#34;test building a VM with a VM (VLAN) Network has a bogus VLAN ID (no DHCP)

    Prerequisite:
        Setting opensuse-image-url set to a valid URL for
        an opensuse image.

    Manual Test Doc(s):
        - N/A

    Args:
        api_client (HarvesterAPI): HarvesterAPI client_
        image (str): corresponding image from fixture_
        unique_vm_name (str): fixture at module level based unique vm name
        wait_timeout (int): seconds for wait timeout from fixture
        bogus_vlan_net (dict): the data dict that contains info surrounding vm net

    Raises:
        AssertionError: fails to create, delete, or delete volumes

    Steps:
    1. build vm with a single virtio network interface
    that has a bogus vlan vm network (no dhcp)
    2. delete vm and volumes

    Expected Result:
    - assert vlan vm network can be created successfully (fixture level)
    - assert vm can be created successfully
    - assert &#39;ipAddresses&#39; not in the status of running vm&#39;s interfaces
    - assert can delete vm and volumes
    &#34;&#34;&#34;
    cpu, mem = 1, 2
    bvn = bogus_vlan_net
    vm = api_client.vms.Spec(cpu, mem)
    net_uid = f&#34;{bvn[&#39;metadata&#39;][&#39;namespace&#39;]}/{bvn[&#39;metadata&#39;][&#39;name&#39;]}&#34;
    vm = api_client.vms.Spec(cpu, mem)
    vm.add_network(&#39;no-dhcp&#39;, net_uid)
    vm.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])
    code, data = api_client.vms.create(unique_vm_name, vm)

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code and &#34;Running&#34; == data.get(&#39;status&#39;, {}).get(&#39;phase&#39;):
            code, data = api_client.vms.get_status(unique_vm_name)
            assert 200 == code, (code, data)
            assert data[&#39;status&#39;][&#39;interfaces&#39;][1] is not None
            assert &#39;infoSource&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][1]
            assert &#39;mac&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][1]
            assert data[&#39;status&#39;][&#39;interfaces&#39;][1][&#39;mac&#39;] is not None
            assert &#39;name&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][1]
            # checking that ipAddress/es are not present due to
            # vlan that was used not having dhcp so no assignment
            # kubevirt v1 virtualmachineinstancenetworkinterface
            assert &#39;ipAddresses&#39; not in data[&#39;status&#39;][&#39;interfaces&#39;][1]
            assert &#39;ipAddress&#39; not in data[&#39;status&#39;][&#39;interfaces&#39;][1]
            code, data = api_client.vms.delete(unique_vm_name)
            assert 200 == code, (code, data)
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to create VM({cpu} core, {mem} RAM) with errors:\n&#34;
            f&#34;Phase: {data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)}\t&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    spec = api_client.vms.Spec.from_dict(data)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if 404 == code:
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    fails, check = [], dict()
    for vol in spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        check[vol_name] = api_client.volumes.delete(vol_name)

    while endtime &gt; datetime.now():
        l_check = dict()
        for vol_name, (code, data) in check.items():
            if 200 != code:
                fails.append(
                    (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
            else:
                code, data = api_client.volumes.get(vol_name)
                if 404 != code:
                    l_check[vol_name] = (code, data)
        check = l_check
        if not check:
            break
        sleep(sleep_timeout)
    else:
        for vol_name, (code, data) in check.items():
            fails.append(
                (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))

    assert not fails, (
        f&#34;Failed to delete VM({unique_vm_name})&#39;s volumes with errors:\n&#34;
        &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
    )


@pytest.mark.p0
@pytest.mark.virtualmachines
class TestHotPlugVolume:
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/volumes/support-volume-hot-unplug/

    Steps:
        1. Create and start VM
        2. Create Data volume
        3. Attach data volume
        4. Detach data volume
    Exepected Result:
        - VM should started successfully
        - Data volume should attached and available in VM
        - Data volume should detached and unavailable in VM
        - VM should not be reboot or restart while attaching/detaching volume
    &#34;&#34;&#34;

    disk_name = &#34;disk-hot-plug&#34;

    @contextmanager
    def login_to_vm_from_host(
        self, host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
    ):
        with host_shell.login(host_ip, jumphost=True) as host_sh:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=host_sh.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as vm_sh:
                yield (vm_sh, host_sh)

    @pytest.mark.dependency(name=&#34;hot_plug_volume&#34;)
    def test_add(
        self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, small_volume, stopped_vm
    ):
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair

        # Start VM
        code, data = api_client.vms.start(unique_vm_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        # Log into VM to verify OS is ready
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with self.login_to_vm_from_host(
            host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
        ) as (sh, host_sh):
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )

        # attach volume
        vol_name, vol_size = small_volume
        code, data = api_client.vms.add_volume(unique_vm_name, self.disk_name, vol_name)

        assert 204 == code, (code, data)

        # Login to VM to verify volume hot pluged
        with self.login_to_vm_from_host(
            host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
        ) as (sh, host_sh):
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                scsi, err = sh.exec_command(
                    &#34;ls -d /sys/block/sd*/device/scsi_device/*&#34;
                    &#34; | awk -F &#39;[/]&#39; &#39;{print $4,$7}&#39;&#34;
                )
                if scsi:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;Hot pluged Volume {vol_name} unavailable after {wait_timeout}s\n&#34;
                    f&#34;STDOUT: {scsi}, STDERR: {err}&#34;
                )

            out, err = sh.exec_command(
                f&#34;lsblk -r | grep {scsi.split()[0]}&#34;
            )

        assert f&#34;{vol_size}G 0 disk&#34; in out, (
            f&#34;existing Volume {vol_size}G not found\n&#34;
            f&#34;lsblk output: {out}&#34;
        )

    @pytest.mark.dependency(depends=[&#34;hot_plug_volume&#34;])
    def test_remove(
        self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, small_volume, stopped_vm
    ):
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair

        # remove volume
        vol_name, vol_size = small_volume
        code, data = api_client.vms.remove_volume(unique_vm_name, self.disk_name)

        assert 204 == code, (code, data)

        # Log into VM to verify volume removed
        code, data = api_client.vms.get_status(unique_vm_name)
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with self.login_to_vm_from_host(
            host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
        ) as (sh, host_sh):
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                scsi, err = sh.exec_command(
                    &#34;ls -d1 /sys/block/* | grep &#39;sd&#39;&#34;
                )
                if not scsi:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;Hot pluged Volume {vol_name} still available after {wait_timeout}s\n&#34;
                    f&#34;STDOUT: {scsi}, STDERR: {err}&#34;
                )

            out, err = sh.exec_command(
                &#34;lsblk -r | grep &#39;sd&#39;&#34;
            )

        assert not scsi, &#34;SCSI device still available in `/sys/block/`&#34;
        assert not out, &#34;SCSI device still available in `lsblk -r`&#34;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.bogus_vlan_net"><code class="name flex">
<span>def <span class="ident">bogus_vlan_net</span></span>(<span>request, api_client)</span>
</code></dt>
<dd>
<div class="desc"><p>bogus vlan network fixture (no dhcp) on mgmt network</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>request</code></strong> :&ensp;<code>FixtureRequest</code></dt>
<dd><a href="https://docs.pytest.org/en/7.1.x/_modules/_pytest/fixtures.html#FixtureRequest">https://docs.pytest.org/en/7.1.x/_modules/_pytest/fixtures.html#FixtureRequest</a> # noqa</dd>
<dt><strong><code>api_client</code></strong> :&ensp;<code>HarvesterAPI</code></dt>
<dd>HarvesterAPI client</dd>
</dl>
<h2 id="yields">Yields</h2>
<dl>
<dt><code>dict</code></dt>
<dd>created bogus network attachment definition dictionary</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#39;module&#39;)
def bogus_vlan_net(request, api_client):
    &#34;&#34;&#34;bogus vlan network fixture (no dhcp) on mgmt network

    Args:
        request (FixtureRequest): https://docs.pytest.org/en/7.1.x/_modules/_pytest/fixtures.html#FixtureRequest # noqa
        api_client (HarvesterAPI): HarvesterAPI client

    Yields:
        dict: created bogus network attachment definition dictionary
    &#34;&#34;&#34;
    original_vlan_id = request.config.getoption(&#39;--vlan-id&#39;)

    existing_vm_net_code, existing_vm_net_data = api_client.networks.get()
    assert existing_vm_net_code == 200, &#39;we should be able to fetch vm networks from harvester&#39;
    existing_vm_net_list = existing_vm_net_data.get(&#39;items&#39;, [])
    vlans_to_exclude = set()
    vlans_to_exclude.add(1)
    for existing_vm_net in existing_vm_net_list:
        existing_vm_net_config = existing_vm_net.get(&#39;spec&#39;, {}).get(&#39;config&#39;, &#39;{}&#39;)
        assert existing_vm_net_config != &#39;{}&#39;, &#39;existing vm net should exist&#39;
        existing_vm_net_config_dict = json.loads(existing_vm_net_config)
        assert existing_vm_net_config_dict.get(
            &#39;vlan&#39;, 0) != 0, &#39;we should be able to get the vlan off the config&#39;
        existing_vm_net_vlan = existing_vm_net_config_dict.get(&#39;vlan&#39;)
        vlans_to_exclude.add(existing_vm_net_vlan)

    if original_vlan_id != -1:
        vlans_to_exclude.add(original_vlan_id)

    vlan_ids = set(range(2, 4095))  # 4094 is the last, 1 should always be excluded.
    code, data = api_client.networks.get()
    for net in data[&#39;items&#39;]:
        config = json.loads(net[&#39;spec&#39;].get(&#39;config&#39;, &#39;{}&#39;))
        if config.get(&#39;vlan&#39;):
            try:
                # try to remove the key, but VLAN may be used in both &#39;mgmt&#39;
                # and other cluster network(s) so it might have already been removed
                vlan_ids.remove(config[&#39;vlan&#39;])
            except KeyError:
                print(f&#34;key, {config[&#39;vlan&#39;]} was already removed by another cluster network&#34;)

    vlan_id = vlan_ids.pop()  # Remove and return an arbitrary set element.
    vm_network_name = f&#39;bogus-net-{vlan_id}&#39;
    code, data = api_client.networks.create(vm_network_name, vlan_id)
    assert 201 == code, (
        f&#34;Failed to create N.A.D. {vm_network_name} with error {code}, {data}&#34;
    )

    yield data

    api_client.networks.delete(vm_network_name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.image"><code class="name flex">
<span>def <span class="ident">image</span></span>(<span>api_client, image_opensuse, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def image(api_client, image_opensuse, unique_name, wait_timeout):
    unique_image_id = f&#39;image-{unique_name}&#39;
    code, data = api_client.images.create_by_url(
        unique_image_id, image_opensuse.url, display_name=f&#34;{unique_name}-{image_opensuse.name}&#34;
    )

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.images.get(unique_image_id)
        if 100 == data.get(&#39;status&#39;, {}).get(&#39;progress&#39;, 0):
            break
        sleep(3)
    else:
        raise AssertionError(
            &#34;Failed to create Image with error:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    yield dict(id=f&#34;{data[&#39;metadata&#39;][&#39;namespace&#39;]}/{unique_image_id}&#34;,
               user=image_opensuse.ssh_user)

    code, data = api_client.images.delete(unique_image_id)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.small_volume"><code class="name flex">
<span>def <span class="ident">small_volume</span></span>(<span>api_client, unique_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;class&#34;)
def small_volume(api_client, unique_name):
    vol_name, size = f&#34;sv-{unique_name}&#34;, 3
    vol_spec = api_client.volumes.Spec(size)
    code, data = api_client.volumes.create(vol_name, vol_spec)

    assert 201 == code, (code, data)

    yield vol_name, size

    code, data = api_client.volumes.delete(vol_name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.stopped_vm"><code class="name flex">
<span>def <span class="ident">stopped_vm</span></span>(<span>api_client, ssh_keypair, wait_timeout, image, unique_vm_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;class&#34;)
def stopped_vm(api_client, ssh_keypair, wait_timeout, image, unique_vm_name):
    unique_vm_name = f&#34;stopped-{datetime.now().strftime(&#39;%m%S%f&#39;)}-{unique_vm_name}&#34;
    cpu, mem = 1, 2
    pub_key, pri_key = ssh_keypair
    vm_spec = api_client.vms.Spec(cpu, mem)
    vm_spec.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])
    vm_spec.run_strategy = &#34;Halted&#34;

    userdata = yaml.safe_load(vm_spec.user_data)
    userdata[&#39;ssh_authorized_keys&#39;] = [pub_key]
    vm_spec.user_data = yaml.dump(userdata)

    code, data = api_client.vms.create(unique_vm_name, vm_spec)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;):
            break
        sleep(1)

    yield unique_vm_name, image[&#39;user&#39;]

    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)

    api_client.vms.delete(unique_vm_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 404 == code:
            break
        sleep(3)

    for vol in vm_spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        api_client.volumes.delete(vol_name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.test_create_stopped_vm"><code class="name flex">
<span>def <span class="ident">test_create_stopped_vm</span></span>(<span>api_client, stopped_vm, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/">https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/</a> # noqa</p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a VM with 1 CPU 2 Memory and runStrategy is <code>Halted</code></li>
<li>Save
Exepected Result:<ul>
<li>VM should created</li>
<li>VM should Stooped</li>
<li>VMI should not exist</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
def test_create_stopped_vm(api_client, stopped_vm, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/ # noqa

    Steps:
        1. Create a VM with 1 CPU 2 Memory and runStrategy is `Halted`
        2. Save
    Exepected Result:
        - VM should created
        - VM should Stooped
        - VMI should not exist
    &#34;&#34;&#34;
    unique_vm_name, _ = stopped_vm
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if (code == 200
                and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Create a Stopped VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    code, data = api_client.vms.get_status(unique_vm_name)
    assert 404 == code, (code, data)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.test_create_vm_no_available_resources"><code class="name flex">
<span>def <span class="ident">test_create_vm_no_available_resources</span></span>(<span>resource, api_client, image, wait_timeout, unique_vm_name, sleep_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a VM with outlandish resources for varying elements (purposefully negative test)</p>
<h2 id="prerequisite">Prerequisite</h2>
<p>Setting opensuse-image-url set to a valid URL for
an opensuse image.</p>
<p>Manual Test Doc(s):
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-cpu-not-in-cluster/">https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-cpu-not-in-cluster/</a> # noqa
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-memory-not-in-cluster/">https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-memory-not-in-cluster/</a> # noqa
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-both-cpu-and-memory-not-in-cluster/">https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-both-cpu-and-memory-not-in-cluster/</a> # noqa</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>request</code></strong> :&ensp;<code>FixtureRequest</code></dt>
<dd><a href="https://docs.pytest.org/en/7.1.x/_modules/_pytest/fixtures.html#FixtureRequest">https://docs.pytest.org/en/7.1.x/_modules/_pytest/fixtures.html#FixtureRequest</a> # noqa</dd>
<dt><strong><code>resource</code></strong> :&ensp;<code>dict</code></dt>
<dd>dict of name(s) &amp; value that can be deconstructed</dd>
<dt><strong><code>api_client</code></strong> :&ensp;<code>HarvesterAPI</code></dt>
<dd>HarvesterAPI client</dd>
<dt><strong><code>image</code></strong> :&ensp;<code>str</code></dt>
<dd>corresponding image from fixture</dd>
<dt><strong><code>wait_timeout</code></strong> :&ensp;<code>int</code></dt>
<dd>seconds for wait timeout from fixture</dd>
<dt><strong><code>unique_vm_name</code></strong> :&ensp;<code>str</code></dt>
<dd>string of unique vm name</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>when vm can not be created, all vms should be allowed to be created</dd>
</dl>
<p>Steps:
1. build vm object specs for outlandish resource(s) under test
2. request to build the vm, assert that succeeds
3. check for conditions of guest not running and vm being unschedulable
4. delete vm and volumes</p>
<p>Expected Result:
- building vm with outlandish resource requests to be successful
- asserting that the status condition of the vm that is built to not be running
- asserting that the status condition of the vm that is built to be unschedulable
- assert deleting vm and volumes to be successful</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.negative
@pytest.mark.parametrize(&#34;resource&#34;, [dict(cpu=MAX), dict(mem=MAX), dict(disk=MAX),
                                      dict(mem=MAX, cpu=MAX), dict(mem=MAX, cpu=MAX, disk=MAX)],
                         ids=[&#39;cpu&#39;, &#39;mem&#39;, &#39;disk&#39;, &#39;mem-and-cpu&#39;, &#39;mem-cpu-and-disk&#39;])
def test_create_vm_no_available_resources(resource, api_client, image,
                                          wait_timeout, unique_vm_name, sleep_timeout):
    &#34;&#34;&#34;Creates a VM with outlandish resources for varying elements (purposefully negative test)

    Prerequisite:
        Setting opensuse-image-url set to a valid URL for
        an opensuse image.

    Manual Test Doc(s):
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-cpu-not-in-cluster/ # noqa
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-memory-not-in-cluster/ # noqa
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-both-cpu-and-memory-not-in-cluster/ # noqa


    Args:
        request (FixtureRequest): https://docs.pytest.org/en/7.1.x/_modules/_pytest/fixtures.html#FixtureRequest # noqa
        resource (dict): dict of name(s) &amp; value that can be deconstructed
        api_client (HarvesterAPI): HarvesterAPI client
        image (str): corresponding image from fixture
        wait_timeout (int): seconds for wait timeout from fixture
        unique_vm_name (str): string of unique vm name

    Raises:
        AssertionError: when vm can not be created, all vms should be allowed to be created

    Steps:
    1. build vm object specs for outlandish resource(s) under test
    2. request to build the vm, assert that succeeds
    3. check for conditions of guest not running and vm being unschedulable
    4. delete vm and volumes

    Expected Result:
    - building vm with outlandish resource requests to be successful
    - asserting that the status condition of the vm that is built to not be running
    - asserting that the status condition of the vm that is built to be unschedulable
    - assert deleting vm and volumes to be successful
    &#34;&#34;&#34;
    unique_name_for_vm = f&#34;{&#39;&#39;.join(resource.keys())}-{unique_vm_name}&#34;
    overall_vm_obj = dict(cpu=1, mem=2, disk=10)
    overall_vm_obj.update(resource)

    vm = api_client.vms.Spec(overall_vm_obj[&#39;cpu&#39;], overall_vm_obj[&#39;mem&#39;])
    vm.add_image(&#34;disk-0&#34;, image[&#39;id&#39;], size=overall_vm_obj.get(&#39;disk&#39;))
    code, data = api_client.vms.create(unique_name_for_vm, vm)
    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name_for_vm)
        if 200 == code and len(data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [])) &gt; 1:
            checks = dict(GuestNotRunning=False, Unschedulable=False)
            for condition in data[&#39;status&#39;][&#39;conditions&#39;]:
                if condition.get(&#39;reason&#39;) in checks:
                    checks[condition[&#39;reason&#39;]] = True

            assert all(checks.values()), (
                &#34;The VM miss condition:\n&#34;
                &#34; and &#34;.join(k for k, v in checks.items() if not v)
            )
            code, data = api_client.vms.delete(unique_name_for_vm)
            assert 200 == code, (code, data)

            spec = api_client.vms.Spec.from_dict(data)
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to create VM({overall_vm_obj.get(&#39;cpu&#39;)} core, \n&#34;
            f&#34;{overall_vm_obj.get(&#39;mem&#39;)} RAM, \n&#34;
            f&#34;{overall_vm_obj.get(&#39;disk&#39;)} DISK) with errors:\n&#34;
            f&#34;Phase: {data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)}\t&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_name_for_vm)
        if 404 == code:
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({unique_name_for_vm}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    fails, check = [], dict()
    for vol in spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        check[vol_name] = api_client.volumes.delete(vol_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        l_check = dict()
        for vol_name, (code, data) in check.items():
            if 200 != code:
                fails.append(
                    (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
            else:
                code, data = api_client.volumes.get(vol_name)
                if 404 != code:
                    l_check[vol_name] = (code, data)
        check = l_check
        if not check:
            break
        sleep(sleep_timeout)
    else:
        for vol_name, (code, data) in check.items():
            fails.append(
                (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
    assert not fails, (
        f&#34;Failed to delete VM({unique_vm_name})&#39;s volumes with errors:\n&#34;
        &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
    )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.test_minimal_vm"><code class="name flex">
<span>def <span class="ident">test_minimal_vm</span></span>(<span>api_client, image, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/">https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/</a> # noqa</p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a VM with 1 CPU 2 Memory and other default values</li>
<li>Save
Exepected Result:<ul>
<li>VM should created</li>
<li>VM should Started</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.dependency(name=&#34;minimal_vm&#34;)
def test_minimal_vm(api_client, image, unique_vm_name, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/ # noqa

    Steps:
        1. Create a VM with 1 CPU 2 Memory and other default values
        2. Save
    Exepected Result:
        - VM should created
        - VM should Started
    &#34;&#34;&#34;
    cpu, mem = 1, 2
    vm = api_client.vms.Spec(cpu, mem)
    vm.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])

    code, data = api_client.vms.create(unique_vm_name, vm)

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code and &#34;Running&#34; == data.get(&#39;status&#39;, {}).get(&#39;phase&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to create Minimal VM({cpu} core, {mem} RAM) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.test_update_vm_machine_type"><code class="name flex">
<span>def <span class="ident">test_update_vm_machine_type</span></span>(<span>api_client, image, unique_vm_name, wait_timeout, machine_types, sleep_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a VM with machine type then update to another</p>
<h2 id="prerequisite">Prerequisite</h2>
<p>Setting opensuse-image-url set to a valid URL for
an opensuse image.</p>
<p>Manual Test Doc(s):
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-pc/">https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-pc/</a> # noqa
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-q35/">https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-q35/</a> # noqa</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>api_client</code></strong> :&ensp;<code>HarvesterAPI</code></dt>
<dd>HarvesterAPI client</dd>
<dt><strong><code>image</code></strong> :&ensp;<code>str</code></dt>
<dd>corresponding image from fixture</dd>
<dt><strong><code>wait_timeout</code></strong> :&ensp;<code>int</code></dt>
<dd>seconds for wait timeout from fixture</dd>
<dt><strong><code>unique_vm_name</code></strong> :&ensp;<code>str</code></dt>
<dd>fixture at module level based unique vm name</dd>
</dl>
<p>machine_types (tuple)(str): deconstructed to provide starting type and desired end type</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>failure to create, stop, update, or start</dd>
</dl>
<p>Steps:
1. build vm with starting machine type
2. power down vm with starting machine type
3. update vm from machine type starting to machine type ending
4. power up vm
5. delete vm and volumes</p>
<p>Expected Result:
- building a vm with machine type starting to be successful
- powering down the vm with machine type starting to be successful
- modifying the existing machine type starting and updating to ending to be successful
- powering up the modified vm to be successful and that now has the machine type ending
- deleting the vm to be successful</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.parametrize(&#34;machine_types&#34;, [(&#34;pc&#34;, &#34;q35&#34;), (&#34;q35&#34;, &#34;pc&#34;)],
                         ids=[&#39;pc_to_q35&#39;, &#39;q35_to_pc&#39;])
def test_update_vm_machine_type(api_client, image, unique_vm_name,
                                wait_timeout, machine_types, sleep_timeout):
    &#34;&#34;&#34;Create a VM with machine type then update to another

    Prerequisite:
        Setting opensuse-image-url set to a valid URL for
        an opensuse image.

    Manual Test Doc(s):
        - https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-pc/ # noqa
        - https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-q35/ # noqa

    Args:
        api_client (HarvesterAPI): HarvesterAPI client
        image (str): corresponding image from fixture
        wait_timeout (int): seconds for wait timeout from fixture
        unique_vm_name (str): fixture at module level based unique vm name
        machine_types (tuple)(str): deconstructed to provide starting type and desired end type

    Raises:
        AssertionError: failure to create, stop, update, or start

    Steps:
    1. build vm with starting machine type
    2. power down vm with starting machine type
    3. update vm from machine type starting to machine type ending
    4. power up vm
    5. delete vm and volumes

    Expected Result:
    - building a vm with machine type starting to be successful
    - powering down the vm with machine type starting to be successful
    - modifying the existing machine type starting and updating to ending to be successful
    - powering up the modified vm to be successful and that now has the machine type ending
    - deleting the vm to be successful
    &#34;&#34;&#34;
    cpu, mem = 1, 2
    starting_machine_type, ending_machine_type = machine_types
    vm = api_client.vms.Spec(cpu, mem)
    vm.machine_type = starting_machine_type

    vm.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])
    unique_name_for_vm = f&#34;{&#39;&#39;.join(starting_machine_type)}-{unique_vm_name}&#34;

    code, vm_create_data = api_client.vms.create(unique_name_for_vm, vm)

    assert 201 == code, (code, vm_create_data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name_for_vm)
        if 200 == code and &#34;Running&#34; == data.get(&#39;status&#39;, {}).get(&#39;phase&#39;):
            code, data = api_client.vms.stop(unique_name_for_vm)
            assert 204 == code, &#34;`Stop` return unexpected status code&#34;
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to create VM({cpu} core, {mem} RAM) with errors:\n&#34;
            f&#34;Phase: {data.get(&#39;status&#39;, {}).get(&#39;phase&#39;,&#39;&#39;)}\t&#34;
            f&#34;Status: {data.get(&#39;status&#39;, {})}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_name_for_vm)
        if 404 == code:
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to Stop VM({unique_name_for_vm}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    code, data = api_client.vms.get(unique_name_for_vm)
    assert &#34;Halted&#34; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
    assert &#34;Stopped&#34; == data[&#39;status&#39;][&#39;printableStatus&#39;]
    code, vm_to_modify = api_client.vms.get(unique_name_for_vm)
    assert code == 200
    spec = api_client.vms.Spec.from_dict(vm_to_modify)
    spec.machine_type = ending_machine_type
    code, data = api_client.vms.update(unique_name_for_vm, spec)
    result = api_client.vms.Spec.from_dict(data)
    if 200 == code and result.machine_type == ending_machine_type:
        code, data = api_client.vms.start(unique_name_for_vm)
        assert 204 == code, &#34;`Start return unexpected status code&#34;
    else:
        raise AssertionError(
            f&#34;Failed to Update VM({unique_name_for_vm}) with errors:\n&#34;
            f&#34;Phase: {data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)}\t&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_name_for_vm)
        strategy = data[&#39;spec&#39;][&#39;runStrategy&#39;]
        pstats = data[&#39;status&#39;][&#39;printableStatus&#39;]
        if &#34;Halted&#34; != strategy and &#34;Running&#34; == pstats:
            code, data = api_client.vms.delete(unique_name_for_vm)
            assert 200 == code, (code, data)

            spec = api_client.vms.Spec.from_dict(data)
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_name_for_vm}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_name_for_vm)
        if 404 == code:
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({unique_name_for_vm}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    fails, check = [], dict()
    for vol in spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        check[vol_name] = api_client.volumes.delete(vol_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        l_check = dict()
        for vol_name, (code, data) in check.items():
            if 200 != code:
                fails.append(
                    (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
            else:
                code, data = api_client.volumes.get(vol_name)
                if 404 != code:
                    l_check[vol_name] = (code, data)
        check = l_check
        if not check:
            break
    else:
        for vol_name, (code, data) in check.items():
            fails.append(
                (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
    assert not fails, (
        f&#34;Failed to delete VM({unique_name_for_vm})&#39;s volumes with errors:\n&#34;
        &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
    )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.test_vm_with_bogus_vlan"><code class="name flex">
<span>def <span class="ident">test_vm_with_bogus_vlan</span></span>(<span>api_client, image, unique_vm_name, wait_timeout, bogus_vlan_net, sleep_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>test building a VM with a VM (VLAN) Network has a bogus VLAN ID (no DHCP)</p>
<h2 id="prerequisite">Prerequisite</h2>
<p>Setting opensuse-image-url set to a valid URL for
an opensuse image.</p>
<p>Manual Test Doc(s):
- N/A</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>api_client</code></strong> :&ensp;<code>HarvesterAPI</code></dt>
<dd>HarvesterAPI client_</dd>
<dt><strong><code>image</code></strong> :&ensp;<code>str</code></dt>
<dd>corresponding image from fixture_</dd>
<dt><strong><code>unique_vm_name</code></strong> :&ensp;<code>str</code></dt>
<dd>fixture at module level based unique vm name</dd>
<dt><strong><code>wait_timeout</code></strong> :&ensp;<code>int</code></dt>
<dd>seconds for wait timeout from fixture</dd>
<dt><strong><code>bogus_vlan_net</code></strong> :&ensp;<code>dict</code></dt>
<dd>the data dict that contains info surrounding vm net</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>fails to create, delete, or delete volumes</dd>
</dl>
<p>Steps:
1. build vm with a single virtio network interface
that has a bogus vlan vm network (no dhcp)
2. delete vm and volumes</p>
<p>Expected Result:
- assert vlan vm network can be created successfully (fixture level)
- assert vm can be created successfully
- assert 'ipAddresses' not in the status of running vm's interfaces
- assert can delete vm and volumes</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.negative
@pytest.mark.virtualmachines
def test_vm_with_bogus_vlan(api_client, image, unique_vm_name,
                            wait_timeout, bogus_vlan_net, sleep_timeout):
    &#34;&#34;&#34;test building a VM with a VM (VLAN) Network has a bogus VLAN ID (no DHCP)

    Prerequisite:
        Setting opensuse-image-url set to a valid URL for
        an opensuse image.

    Manual Test Doc(s):
        - N/A

    Args:
        api_client (HarvesterAPI): HarvesterAPI client_
        image (str): corresponding image from fixture_
        unique_vm_name (str): fixture at module level based unique vm name
        wait_timeout (int): seconds for wait timeout from fixture
        bogus_vlan_net (dict): the data dict that contains info surrounding vm net

    Raises:
        AssertionError: fails to create, delete, or delete volumes

    Steps:
    1. build vm with a single virtio network interface
    that has a bogus vlan vm network (no dhcp)
    2. delete vm and volumes

    Expected Result:
    - assert vlan vm network can be created successfully (fixture level)
    - assert vm can be created successfully
    - assert &#39;ipAddresses&#39; not in the status of running vm&#39;s interfaces
    - assert can delete vm and volumes
    &#34;&#34;&#34;
    cpu, mem = 1, 2
    bvn = bogus_vlan_net
    vm = api_client.vms.Spec(cpu, mem)
    net_uid = f&#34;{bvn[&#39;metadata&#39;][&#39;namespace&#39;]}/{bvn[&#39;metadata&#39;][&#39;name&#39;]}&#34;
    vm = api_client.vms.Spec(cpu, mem)
    vm.add_network(&#39;no-dhcp&#39;, net_uid)
    vm.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])
    code, data = api_client.vms.create(unique_vm_name, vm)

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code and &#34;Running&#34; == data.get(&#39;status&#39;, {}).get(&#39;phase&#39;):
            code, data = api_client.vms.get_status(unique_vm_name)
            assert 200 == code, (code, data)
            assert data[&#39;status&#39;][&#39;interfaces&#39;][1] is not None
            assert &#39;infoSource&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][1]
            assert &#39;mac&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][1]
            assert data[&#39;status&#39;][&#39;interfaces&#39;][1][&#39;mac&#39;] is not None
            assert &#39;name&#39; in data[&#39;status&#39;][&#39;interfaces&#39;][1]
            # checking that ipAddress/es are not present due to
            # vlan that was used not having dhcp so no assignment
            # kubevirt v1 virtualmachineinstancenetworkinterface
            assert &#39;ipAddresses&#39; not in data[&#39;status&#39;][&#39;interfaces&#39;][1]
            assert &#39;ipAddress&#39; not in data[&#39;status&#39;][&#39;interfaces&#39;][1]
            code, data = api_client.vms.delete(unique_vm_name)
            assert 200 == code, (code, data)
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to create VM({cpu} core, {mem} RAM) with errors:\n&#34;
            f&#34;Phase: {data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)}\t&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    spec = api_client.vms.Spec.from_dict(data)

    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if 404 == code:
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    fails, check = [], dict()
    for vol in spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        check[vol_name] = api_client.volumes.delete(vol_name)

    while endtime &gt; datetime.now():
        l_check = dict()
        for vol_name, (code, data) in check.items():
            if 200 != code:
                fails.append(
                    (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
            else:
                code, data = api_client.volumes.get(vol_name)
                if 404 != code:
                    l_check[vol_name] = (code, data)
        check = l_check
        if not check:
            break
        sleep(sleep_timeout)
    else:
        for vol_name, (code, data) in check.items():
            fails.append(
                (vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))

    assert not fails, (
        f&#34;Failed to delete VM({unique_vm_name})&#39;s volumes with errors:\n&#34;
        &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
    )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.unique_vm_name"><code class="name flex">
<span>def <span class="ident">unique_vm_name</span></span>(<span>unique_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def unique_vm_name(unique_name):
    return f&#34;vm-{unique_name}&#34;</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume"><code class="flex name class">
<span>class <span class="ident">TestHotPlugVolume</span></span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/volumes/support-volume-hot-unplug/">https://harvester.github.io/tests/manual/volumes/support-volume-hot-unplug/</a></p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create and start VM</li>
<li>Create Data volume</li>
<li>Attach data volume</li>
<li>Detach data volume
Exepected Result:<ul>
<li>VM should started successfully</li>
<li>Data volume should attached and available in VM</li>
<li>Data volume should detached and unavailable in VM</li>
<li>VM should not be reboot or restart while attaching/detaching volume</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
class TestHotPlugVolume:
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/volumes/support-volume-hot-unplug/

    Steps:
        1. Create and start VM
        2. Create Data volume
        3. Attach data volume
        4. Detach data volume
    Exepected Result:
        - VM should started successfully
        - Data volume should attached and available in VM
        - Data volume should detached and unavailable in VM
        - VM should not be reboot or restart while attaching/detaching volume
    &#34;&#34;&#34;

    disk_name = &#34;disk-hot-plug&#34;

    @contextmanager
    def login_to_vm_from_host(
        self, host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
    ):
        with host_shell.login(host_ip, jumphost=True) as host_sh:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=host_sh.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as vm_sh:
                yield (vm_sh, host_sh)

    @pytest.mark.dependency(name=&#34;hot_plug_volume&#34;)
    def test_add(
        self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, small_volume, stopped_vm
    ):
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair

        # Start VM
        code, data = api_client.vms.start(unique_vm_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        # Log into VM to verify OS is ready
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with self.login_to_vm_from_host(
            host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
        ) as (sh, host_sh):
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )

        # attach volume
        vol_name, vol_size = small_volume
        code, data = api_client.vms.add_volume(unique_vm_name, self.disk_name, vol_name)

        assert 204 == code, (code, data)

        # Login to VM to verify volume hot pluged
        with self.login_to_vm_from_host(
            host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
        ) as (sh, host_sh):
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                scsi, err = sh.exec_command(
                    &#34;ls -d /sys/block/sd*/device/scsi_device/*&#34;
                    &#34; | awk -F &#39;[/]&#39; &#39;{print $4,$7}&#39;&#34;
                )
                if scsi:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;Hot pluged Volume {vol_name} unavailable after {wait_timeout}s\n&#34;
                    f&#34;STDOUT: {scsi}, STDERR: {err}&#34;
                )

            out, err = sh.exec_command(
                f&#34;lsblk -r | grep {scsi.split()[0]}&#34;
            )

        assert f&#34;{vol_size}G 0 disk&#34; in out, (
            f&#34;existing Volume {vol_size}G not found\n&#34;
            f&#34;lsblk output: {out}&#34;
        )

    @pytest.mark.dependency(depends=[&#34;hot_plug_volume&#34;])
    def test_remove(
        self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, small_volume, stopped_vm
    ):
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair

        # remove volume
        vol_name, vol_size = small_volume
        code, data = api_client.vms.remove_volume(unique_vm_name, self.disk_name)

        assert 204 == code, (code, data)

        # Log into VM to verify volume removed
        code, data = api_client.vms.get_status(unique_vm_name)
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with self.login_to_vm_from_host(
            host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
        ) as (sh, host_sh):
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                scsi, err = sh.exec_command(
                    &#34;ls -d1 /sys/block/* | grep &#39;sd&#39;&#34;
                )
                if not scsi:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;Hot pluged Volume {vol_name} still available after {wait_timeout}s\n&#34;
                    f&#34;STDOUT: {scsi}, STDERR: {err}&#34;
                )

            out, err = sh.exec_command(
                &#34;lsblk -r | grep &#39;sd&#39;&#34;
            )

        assert not scsi, &#34;SCSI device still available in `/sys/block/`&#34;
        assert not out, &#34;SCSI device still available in `lsblk -r`&#34;</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.disk_name"><code class="name">var <span class="ident">disk_name</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.login_to_vm_from_host"><code class="name flex">
<span>def <span class="ident">login_to_vm_from_host</span></span>(<span>self, host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def login_to_vm_from_host(
    self, host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
):
    with host_shell.login(host_ip, jumphost=True) as host_sh:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=host_sh.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

        with vm_sh as vm_sh:
            yield (vm_sh, host_sh)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.test_add"><code class="name flex">
<span>def <span class="ident">test_add</span></span>(<span>self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, small_volume, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;hot_plug_volume&#34;)
def test_add(
    self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, small_volume, stopped_vm
):
    unique_vm_name, ssh_user = stopped_vm
    pub_key, pri_key = ssh_keypair

    # Start VM
    code, data = api_client.vms.start(unique_vm_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    # Log into VM to verify OS is ready
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    with self.login_to_vm_from_host(
        host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
    ) as (sh, host_sh):
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            out, err = sh.exec_command(&#39;cloud-init status&#39;)
            if &#39;done&#39; in out:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                f&#34;, but cloud-init still in {out}&#34;
            )

    # attach volume
    vol_name, vol_size = small_volume
    code, data = api_client.vms.add_volume(unique_vm_name, self.disk_name, vol_name)

    assert 204 == code, (code, data)

    # Login to VM to verify volume hot pluged
    with self.login_to_vm_from_host(
        host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
    ) as (sh, host_sh):
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            scsi, err = sh.exec_command(
                &#34;ls -d /sys/block/sd*/device/scsi_device/*&#34;
                &#34; | awk -F &#39;[/]&#39; &#39;{print $4,$7}&#39;&#34;
            )
            if scsi:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Hot pluged Volume {vol_name} unavailable after {wait_timeout}s\n&#34;
                f&#34;STDOUT: {scsi}, STDERR: {err}&#34;
            )

        out, err = sh.exec_command(
            f&#34;lsblk -r | grep {scsi.split()[0]}&#34;
        )

    assert f&#34;{vol_size}G 0 disk&#34; in out, (
        f&#34;existing Volume {vol_size}G not found\n&#34;
        f&#34;lsblk output: {out}&#34;
    )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.test_remove"><code class="name flex">
<span>def <span class="ident">test_remove</span></span>(<span>self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, small_volume, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;hot_plug_volume&#34;])
def test_remove(
    self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, small_volume, stopped_vm
):
    unique_vm_name, ssh_user = stopped_vm
    pub_key, pri_key = ssh_keypair

    # remove volume
    vol_name, vol_size = small_volume
    code, data = api_client.vms.remove_volume(unique_vm_name, self.disk_name)

    assert 204 == code, (code, data)

    # Log into VM to verify volume removed
    code, data = api_client.vms.get_status(unique_vm_name)
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    with self.login_to_vm_from_host(
        host_shell, vm_shell, wait_timeout, host_ip, ssh_user, pri_key, vm_ip
    ) as (sh, host_sh):
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            scsi, err = sh.exec_command(
                &#34;ls -d1 /sys/block/* | grep &#39;sd&#39;&#34;
            )
            if not scsi:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Hot pluged Volume {vol_name} still available after {wait_timeout}s\n&#34;
                f&#34;STDOUT: {scsi}, STDERR: {err}&#34;
            )

        out, err = sh.exec_command(
            &#34;lsblk -r | grep &#39;sd&#39;&#34;
        )

    assert not scsi, &#34;SCSI device still available in `/sys/block/`&#34;
    assert not out, &#34;SCSI device still available in `lsblk -r`&#34;</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMClone"><code class="flex name class">
<span>class <span class="ident">TestVMClone</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
class TestVMClone:
    def test_clone_running_vm(self, api_client, ssh_keypair, wait_timeout,
                              host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/ # noqa
        - (new) https://github.com/harvester/tests/issues/361

        Steps:
            1. Create a VM with 1 CPU 2 Memory
            2. Start the VM and write some data
            3. Clone the VM into VM-cloned
            4. Verify VM-Cloned

        Exepected Result:
            - Cloned-VM should be available and starting
            - Cloned-VM should becomes `Running`
            - Written data should available in Cloned-VM
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.start(unique_vm_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into VM to make some data
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(f&#39;echo {unique_vm_name!r} &gt; ~/vmname&#39;)
                assert not err, (out, err)
                sh.exec_command(&#39;sync&#39;)

        # Clone VM into new VM
        cloned_name = f&#34;cloned-{unique_vm_name}&#34;
        code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
        assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

        # Check VM started
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(cloned_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into new VM to check VM is cloned as old one
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )

                out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
            assert unique_vm_name in out, (
                f&#34;cloud-init writefile failed\n&#34;
                f&#34;Executed stdout: {out}\n&#34;
                f&#34;Executed stderr: {err}&#34;
            )

        # Remove cloned VM and volumes
        code, data = api_client.vms.get(cloned_name)
        cloned_spec = api_client.vms.Spec.from_dict(data)
        api_client.vms.delete(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )
        for vol in cloned_spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)

    def test_clone_stopped_vm(self, api_client, ssh_keypair, wait_timeout,
                              host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/ # noqa
        - (new) https://github.com/harvester/tests/issues/361

        Steps:
            1. Create a VM with 1 CPU 2 Memory
            2. Start the VM and write some data
            3. Stop the VM
            4. Clone the VM into VM-cloned
            5. Verify VM-Cloned

        Exepected Result:
            - Cloned-VM should be available and stopped
            - Cloned-VM should able to start and becomes `Running`
            - Written data should available in Cloned-VM
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.start(unique_vm_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into VM to make some data
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(f&#39;echo &#34;stopped-{unique_vm_name}&#34; &gt; ~/vmname&#39;)
                assert not err, (out, err)
                sh.exec_command(&#39;sync&#39;)

        # Stop the VM
        code, data = api_client.vms.stop(unique_vm_name)
        assert 204 == code, &#34;`Stop` return unexpected status code&#34;
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        # Clone VM into new VM
        cloned_name = f&#34;cloned-{unique_vm_name}&#34;
        code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
        assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

        # Check cloned VM is available and stooped
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if (200 == code
               and &#34;Halted&#34; == data[&#39;spec&#39;].get(&#39;runStrategy&#39;)
               and &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Cloned VM {cloned_name} is not available and stopped&#34;
                f&#34;Status({code}): {data}&#34;
            )

        # Check cloned VM started
        api_client.vms.start(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(cloned_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        # Log into new VM to check VM is cloned as old one
        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )

                out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
            assert f&#34;stopped-{unique_vm_name}&#34; in out, (
                f&#34;cloud-init writefile failed\n&#34;
                f&#34;Executed stdout: {out}\n&#34;
                f&#34;Executed stderr: {err}&#34;
            )

        # Remove cloned VM and volumes
        code, data = api_client.vms.get(cloned_name)
        cloned_spec = api_client.vms.Spec.from_dict(data)
        api_client.vms.delete(cloned_name)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(cloned_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )
        for vol in cloned_spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_running_vm"><code class="name flex">
<span>def <span class="ident">test_clone_running_vm</span></span>(<span>self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- (legacy) <a href="https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/">https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/</a> # noqa
- (new) <a href="https://github.com/harvester/tests/issues/361">https://github.com/harvester/tests/issues/361</a></p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a VM with 1 CPU 2 Memory</li>
<li>Start the VM and write some data</li>
<li>Clone the VM into VM-cloned</li>
<li>Verify VM-Cloned</li>
</ol>
<p>Exepected Result:
- Cloned-VM should be available and starting
- Cloned-VM should becomes <code>Running</code>
- Written data should available in Cloned-VM</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_clone_running_vm(self, api_client, ssh_keypair, wait_timeout,
                          host_shell, vm_shell, stopped_vm):
    &#34;&#34;&#34;
    To cover test:
    - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/ # noqa
    - (new) https://github.com/harvester/tests/issues/361

    Steps:
        1. Create a VM with 1 CPU 2 Memory
        2. Start the VM and write some data
        3. Clone the VM into VM-cloned
        4. Verify VM-Cloned

    Exepected Result:
        - Cloned-VM should be available and starting
        - Cloned-VM should becomes `Running`
        - Written data should available in Cloned-VM
    &#34;&#34;&#34;
    unique_vm_name, ssh_user = stopped_vm
    pub_key, pri_key = ssh_keypair
    code, data = api_client.vms.start(unique_vm_name)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    # Log into VM to make some data
    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )
            out, err = sh.exec_command(f&#39;echo {unique_vm_name!r} &gt; ~/vmname&#39;)
            assert not err, (out, err)
            sh.exec_command(&#39;sync&#39;)

    # Clone VM into new VM
    cloned_name = f&#34;cloned-{unique_vm_name}&#34;
    code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
    assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

    # Check VM started
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(cloned_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    # Log into new VM to check VM is cloned as old one
    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )

            out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
        assert unique_vm_name in out, (
            f&#34;cloud-init writefile failed\n&#34;
            f&#34;Executed stdout: {out}\n&#34;
            f&#34;Executed stderr: {err}&#34;
        )

    # Remove cloned VM and volumes
    code, data = api_client.vms.get(cloned_name)
    cloned_spec = api_client.vms.Spec.from_dict(data)
    api_client.vms.delete(cloned_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(cloned_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    for vol in cloned_spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        api_client.volumes.delete(vol_name)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_stopped_vm"><code class="name flex">
<span>def <span class="ident">test_clone_stopped_vm</span></span>(<span>self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- (legacy) <a href="https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/">https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/</a> # noqa
- (new) <a href="https://github.com/harvester/tests/issues/361">https://github.com/harvester/tests/issues/361</a></p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a VM with 1 CPU 2 Memory</li>
<li>Start the VM and write some data</li>
<li>Stop the VM</li>
<li>Clone the VM into VM-cloned</li>
<li>Verify VM-Cloned</li>
</ol>
<p>Exepected Result:
- Cloned-VM should be available and stopped
- Cloned-VM should able to start and becomes <code>Running</code>
- Written data should available in Cloned-VM</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_clone_stopped_vm(self, api_client, ssh_keypair, wait_timeout,
                          host_shell, vm_shell, stopped_vm):
    &#34;&#34;&#34;
    To cover test:
    - (legacy) https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/ # noqa
    - (new) https://github.com/harvester/tests/issues/361

    Steps:
        1. Create a VM with 1 CPU 2 Memory
        2. Start the VM and write some data
        3. Stop the VM
        4. Clone the VM into VM-cloned
        5. Verify VM-Cloned

    Exepected Result:
        - Cloned-VM should be available and stopped
        - Cloned-VM should able to start and becomes `Running`
        - Written data should available in Cloned-VM
    &#34;&#34;&#34;
    unique_vm_name, ssh_user = stopped_vm
    pub_key, pri_key = ssh_keypair
    code, data = api_client.vms.start(unique_vm_name)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    # Log into VM to make some data
    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )
            out, err = sh.exec_command(f&#39;echo &#34;stopped-{unique_vm_name}&#34; &gt; ~/vmname&#39;)
            assert not err, (out, err)
            sh.exec_command(&#39;sync&#39;)

    # Stop the VM
    code, data = api_client.vms.stop(unique_vm_name)
    assert 204 == code, &#34;`Stop` return unexpected status code&#34;
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    # Clone VM into new VM
    cloned_name = f&#34;cloned-{unique_vm_name}&#34;
    code, _ = api_client.vms.clone(unique_vm_name, cloned_name)
    assert 204 == code, f&#34;Failed to clone VM {unique_vm_name} into new VM {cloned_name}&#34;

    # Check cloned VM is available and stooped
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(cloned_name)
        if (200 == code
           and &#34;Halted&#34; == data[&#39;spec&#39;].get(&#39;runStrategy&#39;)
           and &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Cloned VM {cloned_name} is not available and stopped&#34;
            f&#34;Status({code}): {data}&#34;
        )

    # Check cloned VM started
    api_client.vms.start(cloned_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(cloned_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({cloned_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    # Log into new VM to check VM is cloned as old one
    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {cloned_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )

            out, err = sh.exec_command(&#39;cat ~/vmname&#39;)
        assert f&#34;stopped-{unique_vm_name}&#34; in out, (
            f&#34;cloud-init writefile failed\n&#34;
            f&#34;Executed stdout: {out}\n&#34;
            f&#34;Executed stderr: {err}&#34;
        )

    # Remove cloned VM and volumes
    code, data = api_client.vms.get(cloned_name)
    cloned_spec = api_client.vms.Spec.from_dict(data)
    api_client.vms.delete(cloned_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(cloned_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({cloned_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
    for vol in cloned_spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        api_client.volumes.delete(vol_name)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations"><code class="flex name class">
<span>class <span class="ident">TestVMOperations</span></span>
</code></dt>
<dd>
<div class="desc"><p>To cover tests:
- <a href="https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/">https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/</a> # noqa</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
@pytest.mark.dependency(depends=[&#34;minimal_vm&#34;])
class TestVMOperations:
    &#34;&#34;&#34;
    To cover tests:
    - https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/ # noqa
    &#34;&#34;&#34;

    @pytest.mark.dependency(name=&#34;pause_vm&#34;, depends=[&#34;minimal_vm&#34;])
    def test_pause(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Pause the VM was created
        Exepected Result:
            - VM should change status into `Paused`
        &#39;&#39;&#39;
        code, data = api_client.vms.pause(unique_vm_name)
        assert 204 == code, &#34;`Pause` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if [c for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []) if &#34;Paused&#34; == c[&#39;type&#39;]]:
                conditions = data[&#39;status&#39;][&#39;conditions&#39;]
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to pause VM({unique_vm_name}) with errors:\n&#34;
                f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        assert &#34;Paused&#34; == conditions[-1].get(&#39;type&#39;), conditions
        assert &#34;PausedByUser&#34; == conditions[-1].get(&#39;reason&#39;), conditions

    @pytest.mark.dependency(depends=[&#34;pause_vm&#34;])
    def test_unpause(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Unpause the VM was paused
        Exepected Result:
            - VM&#39;s status should not be `Paused`
        &#39;&#39;&#39;
        code, data = api_client.vms.unpause(unique_vm_name)
        assert 204 == code, &#34;`Unpause` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            cond_types = set(c[&#39;type&#39;] for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []))
            if {&#34;AgentConnected&#34;} &amp; cond_types and not {&#34;Paused&#34;} &amp; cond_types:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to unpause VM({unique_vm_name}) with errors:\n&#34;
                f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    @pytest.mark.dependency(name=&#34;stop_vm&#34;, depends=[&#34;minimal_vm&#34;])
    def test_stop(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Stop the VM was created and not stopped
        Exepected Result:
            - VM&#39;s status should be changed to `Stopped`
            - VM&#39;s `RunStrategy` should be changed to `Halted`
        &#39;&#39;&#39;
        code, data = api_client.vms.stop(unique_vm_name)
        assert 204 == code, &#34;`Stop` return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        code, data = api_client.vms.get(unique_vm_name)
        assert &#34;Halted&#34; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
        assert &#34;Stopped&#34; == data[&#39;status&#39;][&#39;printableStatus&#39;]

    @pytest.mark.dependency(name=&#34;start_vm&#34;, depends=[&#34;stop_vm&#34;])
    def test_start(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Start the VM was created and stopped
        Exepected Result:
            - VM should change status into `Running`
        &#39;&#39;&#39;
        code, data = api_client.vms.start(unique_vm_name)
        assert 204 == code, &#34;`Start return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            strategy = data[&#39;spec&#39;][&#39;runStrategy&#39;]
            pstats = data[&#39;status&#39;][&#39;printableStatus&#39;]
            if &#34;Halted&#34; != strategy and &#34;Running&#34; == pstats:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_restart(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Restart the VM was created
        Exepected Result:
            - VM&#39;s ActivePods should be updated (which means the VM restarted)
            - VM&#39;s status should update to `Running`
            - VM&#39;s qemu-agent should be connected
        &#39;&#39;&#39;
        code, data = api_client.vms.get_status(unique_vm_name)
        assert 200 == code, (
            f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

        old_pods = set(data[&#39;status&#39;][&#39;activePods&#39;].items())

        code, data = api_client.vms.restart(unique_vm_name)
        assert 204 == code, &#34;`Restart return unexpected status code&#34;

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if old_pods.difference(data[&#39;status&#39;].get(&#39;activePods&#39;, old_pods).items()):
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to Restart VM({unique_vm_name}), activePods is not updated.\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Restart VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_softreboot(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Softreboot the VM was created
        Exepected Result:
            - VM&#39;s qemu-agent should disconnected (which means the VM rebooting)
            - VM&#39;s qemu-agent should re-connected (which means the VM boot into OS)
            - VM&#39;s status should be changed to `Running`
        &#39;&#39;&#39;
        code, data = api_client.vms.get_status(unique_vm_name)
        assert 200 == code, (
            f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )
        old_agent = data[&#39;status&#39;][&#39;conditions&#39;][-1]
        assert &#34;AgentConnected&#34; == old_agent[&#39;type&#39;], (code, data)

        api_client.vms.softreboot(unique_vm_name)
        # Wait until agent disconnected (leaving OS)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if &#34;AgentConnected&#34; not in data[&#39;status&#39;][&#39;conditions&#39;][-1][&#39;type&#39;]:
                break
            sleep(5)
        # then wait agent connected again (Entering OS)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            phase, conds = data[&#39;status&#39;][&#39;phase&#39;], data[&#39;status&#39;].get(&#39;conditions&#39;, [{}])
            if &#34;Running&#34; == phase and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Softreboot VM({unique_vm_name}) with errors:\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        old_t = datetime.strptime(old_agent[&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)
        new_t = datetime.strptime(conds[-1][&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)

        assert new_t &gt; old_t, (
            &#34;Agent&#39;s probe time is not updated.\t&#34;
            f&#34;Before softreboot: {old_t}, After softreboot: {new_t}\n&#34;
            f&#34;Last API Status({code}): {data}&#34;
        )

    def test_migrate(self, api_client, unique_vm_name, wait_timeout):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/ # noqa

        Steps:
            1. migrate the VM was created
        Exepected Result:
            - VM&#39;s host Node should be changed to another one
        &#34;&#34;&#34;
        code, host_data = api_client.hosts.get()
        assert 200 == code, (code, host_data)
        code, data = api_client.vms.get_status(unique_vm_name)
        cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
        assert cur_host, (
            f&#34;VMI exists but `nodeName` is empty.\n&#34;
            f&#34;{data}&#34;
        )

        new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;]
                        if cur_host != h[&#39;id&#39;] and not h[&#39;spec&#39;].get(&#39;taint&#39;))

        code, data = api_client.vms.migrate(unique_vm_name, new_host)
        assert 204 == code, (code, data)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            migrating = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
            if not migrating and new_host == data[&#39;status&#39;][&#39;nodeName&#39;]:
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to Migrate VM({unique_vm_name}) from {cur_host} to {new_host}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    def test_abort_migrate(self, api_client, unique_vm_name, wait_timeout):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/live-migration/abort-live-migration/

        Steps:
            1. Abort the VM was created and migrating
        Exepected Result:
            - VM should able to perform migrate
            - VM should stay in current host when migrating be aborted.
        &#34;&#34;&#34;
        code, host_data = api_client.hosts.get()
        assert 200 == code, (code, host_data)
        code, data = api_client.vms.get_status(unique_vm_name)
        cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
        assert cur_host, (
            f&#34;VMI exists but `nodeName` is empty.\n&#34;
            f&#34;{data}&#34;
        )

        new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;]
                        if cur_host != h[&#39;id&#39;] and not h[&#39;spec&#39;].get(&#39;taint&#39;))

        code, data = api_client.vms.migrate(unique_vm_name, new_host)
        assert 204 == code, (code, data)

        states = [&#34;Aborting migration&#34;, &#34;Migrating&#34;]
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            m_state = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
            if m_state == states[-1]:
                states.pop()
                if states:
                    code, err = api_client.vms.abort_migrate(unique_vm_name)
                    assert 204 == code, (code, err)
                else:
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to abort VM({unique_vm_name})&#39;s migration, stuck on {states[-1]}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        assert cur_host == data[&#39;status&#39;][&#39;nodeName&#39;], (
            f&#34;Failed to abort VM({unique_vm_name})&#39;s migration,&#34;
            f&#34;VM been moved to {data[&#39;status&#39;][&#39;nodeName&#39;]} is not the origin host {cur_host}\n&#34;
        )

    def test_delete(self, api_client, unique_vm_name, wait_timeout):
        &#39;&#39;&#39;
        Steps:
            1. Delete the VM was created
            2. Delete Volumes was belonged to the VM
        Exepected Result:
            - VM should able to be deleted and success
            - Volumes should able to be deleted and success
        &#39;&#39;&#39;

        code, data = api_client.vms.delete(unique_vm_name)
        assert 200 == code, (code, data)

        spec = api_client.vms.Spec.from_dict(data)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if 404 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Delete VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status({code}): {data}&#34;
            )

        fails, check = [], dict()
        for vol in spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            check[vol_name] = api_client.volumes.delete(vol_name)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            l_check = dict()
            for vol_name, (code, data) in check.items():
                if 200 != code:
                    fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
                else:
                    code, data = api_client.volumes.get(vol_name)
                    if 404 != code:
                        l_check[vol_name] = (code, data)
            check = l_check
            if not check:
                break
            sleep(5)
        else:
            for vol_name, (code, data) in check.items():
                fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))

        assert not fails, (
            f&#34;Failed to delete VM({unique_vm_name})&#39;s volumes with errors:\n&#34;
            &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
        )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_abort_migrate"><code class="name flex">
<span>def <span class="ident">test_abort_migrate</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/live-migration/abort-live-migration/">https://harvester.github.io/tests/manual/live-migration/abort-live-migration/</a></p>
<h2 id="steps">Steps</h2>
<ol>
<li>Abort the VM was created and migrating
Exepected Result:<ul>
<li>VM should able to perform migrate</li>
<li>VM should stay in current host when migrating be aborted.</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_abort_migrate(self, api_client, unique_vm_name, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/live-migration/abort-live-migration/

    Steps:
        1. Abort the VM was created and migrating
    Exepected Result:
        - VM should able to perform migrate
        - VM should stay in current host when migrating be aborted.
    &#34;&#34;&#34;
    code, host_data = api_client.hosts.get()
    assert 200 == code, (code, host_data)
    code, data = api_client.vms.get_status(unique_vm_name)
    cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
    assert cur_host, (
        f&#34;VMI exists but `nodeName` is empty.\n&#34;
        f&#34;{data}&#34;
    )

    new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;]
                    if cur_host != h[&#39;id&#39;] and not h[&#39;spec&#39;].get(&#39;taint&#39;))

    code, data = api_client.vms.migrate(unique_vm_name, new_host)
    assert 204 == code, (code, data)

    states = [&#34;Aborting migration&#34;, &#34;Migrating&#34;]
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        m_state = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
        if m_state == states[-1]:
            states.pop()
            if states:
                code, err = api_client.vms.abort_migrate(unique_vm_name)
                assert 204 == code, (code, err)
            else:
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to abort VM({unique_vm_name})&#39;s migration, stuck on {states[-1]}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    assert cur_host == data[&#39;status&#39;][&#39;nodeName&#39;], (
        f&#34;Failed to abort VM({unique_vm_name})&#39;s migration,&#34;
        f&#34;VM been moved to {data[&#39;status&#39;][&#39;nodeName&#39;]} is not the origin host {cur_host}\n&#34;
    )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_delete"><code class="name flex">
<span>def <span class="ident">test_delete</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Delete the VM was created</li>
<li>Delete Volumes was belonged to the VM
Exepected Result:<ul>
<li>VM should able to be deleted and success</li>
<li>Volumes should able to be deleted and success</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_delete(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Delete the VM was created
        2. Delete Volumes was belonged to the VM
    Exepected Result:
        - VM should able to be deleted and success
        - Volumes should able to be deleted and success
    &#39;&#39;&#39;

    code, data = api_client.vms.delete(unique_vm_name)
    assert 200 == code, (code, data)

    spec = api_client.vms.Spec.from_dict(data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Delete VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    fails, check = [], dict()
    for vol in spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        check[vol_name] = api_client.volumes.delete(vol_name)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        l_check = dict()
        for vol_name, (code, data) in check.items():
            if 200 != code:
                fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))
            else:
                code, data = api_client.volumes.get(vol_name)
                if 404 != code:
                    l_check[vol_name] = (code, data)
        check = l_check
        if not check:
            break
        sleep(5)
    else:
        for vol_name, (code, data) in check.items():
            fails.append((vol_name, f&#34;Failed to delete\nStatus({code}): {data}&#34;))

    assert not fails, (
        f&#34;Failed to delete VM({unique_vm_name})&#39;s volumes with errors:\n&#34;
        &#34;\n&#34;.join(f&#34;Volume({n}): {r}&#34; for n, r in fails)
    )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_migrate"><code class="name flex">
<span>def <span class="ident">test_migrate</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/">https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/</a> # noqa</p>
<h2 id="steps">Steps</h2>
<ol>
<li>migrate the VM was created
Exepected Result:<ul>
<li>VM's host Node should be changed to another one</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_migrate(self, api_client, unique_vm_name, wait_timeout):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/ # noqa

    Steps:
        1. migrate the VM was created
    Exepected Result:
        - VM&#39;s host Node should be changed to another one
    &#34;&#34;&#34;
    code, host_data = api_client.hosts.get()
    assert 200 == code, (code, host_data)
    code, data = api_client.vms.get_status(unique_vm_name)
    cur_host = data[&#39;status&#39;].get(&#39;nodeName&#39;)
    assert cur_host, (
        f&#34;VMI exists but `nodeName` is empty.\n&#34;
        f&#34;{data}&#34;
    )

    new_host = next(h[&#39;id&#39;] for h in host_data[&#39;data&#39;]
                    if cur_host != h[&#39;id&#39;] and not h[&#39;spec&#39;].get(&#39;taint&#39;))

    code, data = api_client.vms.migrate(unique_vm_name, new_host)
    assert 204 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        migrating = data[&#39;metadata&#39;][&#39;annotations&#39;].get(&#34;harvesterhci.io/migrationState&#34;)
        if not migrating and new_host == data[&#39;status&#39;][&#39;nodeName&#39;]:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to Migrate VM({unique_vm_name}) from {cur_host} to {new_host}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_pause"><code class="name flex">
<span>def <span class="ident">test_pause</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Pause the VM was created
Exepected Result:<ul>
<li>VM should change status into <code>Paused</code></li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;pause_vm&#34;, depends=[&#34;minimal_vm&#34;])
def test_pause(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Pause the VM was created
    Exepected Result:
        - VM should change status into `Paused`
    &#39;&#39;&#39;
    code, data = api_client.vms.pause(unique_vm_name)
    assert 204 == code, &#34;`Pause` return unexpected status code&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if [c for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []) if &#34;Paused&#34; == c[&#39;type&#39;]]:
            conditions = data[&#39;status&#39;][&#39;conditions&#39;]
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to pause VM({unique_vm_name}) with errors:\n&#34;
            f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    assert &#34;Paused&#34; == conditions[-1].get(&#39;type&#39;), conditions
    assert &#34;PausedByUser&#34; == conditions[-1].get(&#39;reason&#39;), conditions</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_restart"><code class="name flex">
<span>def <span class="ident">test_restart</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Restart the VM was created
Exepected Result:<ul>
<li>VM's ActivePods should be updated (which means the VM restarted)</li>
<li>VM's status should update to <code>Running</code></li>
<li>VM's qemu-agent should be connected</li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_restart(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Restart the VM was created
    Exepected Result:
        - VM&#39;s ActivePods should be updated (which means the VM restarted)
        - VM&#39;s status should update to `Running`
        - VM&#39;s qemu-agent should be connected
    &#39;&#39;&#39;
    code, data = api_client.vms.get_status(unique_vm_name)
    assert 200 == code, (
        f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
        f&#34;Status({code}): {data}&#34;
    )

    old_pods = set(data[&#39;status&#39;][&#39;activePods&#39;].items())

    code, data = api_client.vms.restart(unique_vm_name)
    assert 204 == code, &#34;`Restart return unexpected status code&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if old_pods.difference(data[&#39;status&#39;].get(&#39;activePods&#39;, old_pods).items()):
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to Restart VM({unique_vm_name}), activePods is not updated.\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
        conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
        if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Restart VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_softreboot"><code class="name flex">
<span>def <span class="ident">test_softreboot</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Softreboot the VM was created
Exepected Result:<ul>
<li>VM's qemu-agent should disconnected (which means the VM rebooting)</li>
<li>VM's qemu-agent should re-connected (which means the VM boot into OS)</li>
<li>VM's status should be changed to <code>Running</code></li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_softreboot(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Softreboot the VM was created
    Exepected Result:
        - VM&#39;s qemu-agent should disconnected (which means the VM rebooting)
        - VM&#39;s qemu-agent should re-connected (which means the VM boot into OS)
        - VM&#39;s status should be changed to `Running`
    &#39;&#39;&#39;
    code, data = api_client.vms.get_status(unique_vm_name)
    assert 200 == code, (
        f&#34;unable to get VM({unique_vm_name})&#39;s instance infos with errors:\n&#34;
        f&#34;Status({code}): {data}&#34;
    )
    old_agent = data[&#39;status&#39;][&#39;conditions&#39;][-1]
    assert &#34;AgentConnected&#34; == old_agent[&#39;type&#39;], (code, data)

    api_client.vms.softreboot(unique_vm_name)
    # Wait until agent disconnected (leaving OS)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if &#34;AgentConnected&#34; not in data[&#39;status&#39;][&#39;conditions&#39;][-1][&#39;type&#39;]:
            break
        sleep(5)
    # then wait agent connected again (Entering OS)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        phase, conds = data[&#39;status&#39;][&#39;phase&#39;], data[&#39;status&#39;].get(&#39;conditions&#39;, [{}])
        if &#34;Running&#34; == phase and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Softreboot VM({unique_vm_name}) with errors:\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    old_t = datetime.strptime(old_agent[&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)
    new_t = datetime.strptime(conds[-1][&#39;lastProbeTime&#39;], &#39;%Y-%m-%dT%H:%M:%SZ&#39;)

    assert new_t &gt; old_t, (
        &#34;Agent&#39;s probe time is not updated.\t&#34;
        f&#34;Before softreboot: {old_t}, After softreboot: {new_t}\n&#34;
        f&#34;Last API Status({code}): {data}&#34;
    )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_start"><code class="name flex">
<span>def <span class="ident">test_start</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Start the VM was created and stopped
Exepected Result:<ul>
<li>VM should change status into <code>Running</code></li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;start_vm&#34;, depends=[&#34;stop_vm&#34;])
def test_start(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Start the VM was created and stopped
    Exepected Result:
        - VM should change status into `Running`
    &#39;&#39;&#39;
    code, data = api_client.vms.start(unique_vm_name)
    assert 204 == code, &#34;`Start return unexpected status code&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        strategy = data[&#39;spec&#39;][&#39;runStrategy&#39;]
        pstats = data[&#39;status&#39;][&#39;printableStatus&#39;]
        if &#34;Halted&#34; != strategy and &#34;Running&#34; == pstats:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
        conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
        if &#34;Running&#34; == phase and conds and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_stop"><code class="name flex">
<span>def <span class="ident">test_stop</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Stop the VM was created and not stopped
Exepected Result:<ul>
<li>VM's status should be changed to <code>Stopped</code></li>
<li>VM's <code>RunStrategy</code> should be changed to <code>Halted</code></li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;stop_vm&#34;, depends=[&#34;minimal_vm&#34;])
def test_stop(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Stop the VM was created and not stopped
    Exepected Result:
        - VM&#39;s status should be changed to `Stopped`
        - VM&#39;s `RunStrategy` should be changed to `Halted`
    &#39;&#39;&#39;
    code, data = api_client.vms.stop(unique_vm_name)
    assert 204 == code, &#34;`Stop` return unexpected status code&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Stop VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    code, data = api_client.vms.get(unique_vm_name)
    assert &#34;Halted&#34; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
    assert &#34;Stopped&#34; == data[&#39;status&#39;][&#39;printableStatus&#39;]</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_unpause"><code class="name flex">
<span>def <span class="ident">test_unpause</span></span>(<span>self, api_client, unique_vm_name, wait_timeout)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="steps">Steps</h2>
<ol>
<li>Unpause the VM was paused
Exepected Result:<ul>
<li>VM's status should not be <code>Paused</code></li>
</ul>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;pause_vm&#34;])
def test_unpause(self, api_client, unique_vm_name, wait_timeout):
    &#39;&#39;&#39;
    Steps:
        1. Unpause the VM was paused
    Exepected Result:
        - VM&#39;s status should not be `Paused`
    &#39;&#39;&#39;
    code, data = api_client.vms.unpause(unique_vm_name)
    assert 204 == code, &#34;`Unpause` return unexpected status code&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        cond_types = set(c[&#39;type&#39;] for c in data[&#39;status&#39;].get(&#39;conditions&#39;, []))
        if {&#34;AgentConnected&#34;} &amp; cond_types and not {&#34;Paused&#34;} &amp; cond_types:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to unpause VM({unique_vm_name}) with errors:\n&#34;
            f&#34;VM Status: {data[&#39;status&#39;]}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes"><code class="flex name class">
<span>class <span class="ident">TestVMWithVolumes</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.virtualmachines
class TestVMWithVolumes:
    def test_create_with_two_volumes(self, api_client, ssh_keypair, wait_timeout,
                                     host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/ # noqa

        Steps:
            1. Create a VM with 1 CPU 2 Memory and 2 disk volumes
            2. Start the VM
            3. Verify the VM

        Exepected Result:
            - VM should able to start and becomes `Running`
            - 2 disk volumes should be available in the VM
            - Disk size in VM should be the same as its volume configured
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;RerunOnFailure&#34;
        volumes = [(&#39;disk-1&#39;, 1), (&#39;disk-2&#39;, 2)]
        for name, size in volumes:
            vm_spec.add_volume(name, size)

        # Start VM with 2 additional volumes
        code, data = api_client.vms.update(unique_vm_name, vm_spec)
        assert 200 == code, (code, data)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        # Log into VM to verify added volumes
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(&#34;lsblk -r&#34;)
                assert not err, (out, err)

        assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
            f&#34;Added Volumes amount is not correct.\n&#34;
            f&#34;lsblk output: {out}&#34;
        )
        fails = []
        for _, size in volumes:
            if not re.search(f&#34;vd.*{size}G 0 disk&#34;, out):
                fails.append(f&#34;Volume size {size}G not found&#34;)

        assert not fails, (
            f&#34;lsblk output: {out}\n&#34;
            &#34;\n&#34;.join(fails)
        )

        # Tear down: Stop VM and remove added volumes
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;Halted&#34;
        vol_names, vols, claims = [n for n, s in volumes], [], []
        for vd in vm_spec.volumes:
            if vd[&#39;disk&#39;][&#39;name&#39;] in vol_names:
                claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
            else:
                vols.append(vd)
        else:
            vm_spec.volumes = vols

        api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if (code == 200
                    and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                    and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)

        for vol_name in claims:
            api_client.volumes.delete(vol_name)

    def test_create_with_existing_volume(self, api_client, ssh_keypair, wait_timeout,
                                         host_shell, vm_shell, stopped_vm):
        &#34;&#34;&#34;
        To cover test:
        - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/ # noqa

        Steps:
            1. Create a data volume
            2. Create a VM with 1 CPU 2 Memory and the existing data volume
            3. Start the VM
            4. Verify the VM

        Exepected Result:
            - VM should able to start and becomes `Running`
            - Disk volume should be available in the VM
            - Disk size in VM should be the same as its volume configured
        &#34;&#34;&#34;
        unique_vm_name, ssh_user = stopped_vm
        pub_key, pri_key = ssh_keypair

        vol_name, size = &#39;disk-existing&#39;, 3
        vol_spec = api_client.volumes.Spec(size)
        code, data = api_client.volumes.create(f&#34;{unique_vm_name}-{vol_name}&#34;, vol_spec)

        assert 201 == code, (code, data)

        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;RerunOnFailure&#34;
        vm_spec.add_existing_volume(vol_name, f&#34;{unique_vm_name}-{vol_name}&#34;)

        # Start VM with added existing volume
        code, data = api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get_status(unique_vm_name)
            if 200 == code:
                phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
                conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
                if (&#34;Running&#34; == phase
                   and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
                   and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                    break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
                f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

        # Log into VM to verify added volumes
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

        with host_shell.login(host_ip, jumphost=True) as h:
            vm_sh = vm_shell(ssh_user, pkey=pri_key)
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    vm_sh.connect(vm_ip, jumphost=h.client)
                except ChannelException as e:
                    login_ex = e
                    sleep(3)
                else:
                    break
            else:
                raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

            with vm_sh as sh:
                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    out, err = sh.exec_command(&#39;cloud-init status&#39;)
                    if &#39;done&#39; in out:
                        break
                    sleep(3)
                else:
                    raise AssertionError(
                        f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                        f&#34;, but cloud-init still in {out}&#34;
                    )
                out, err = sh.exec_command(&#34;lsblk -r&#34;)
                assert not err, (out, err)

        assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
            f&#34;Added Volumes amount is not correct.\n&#34;
            f&#34;lsblk output: {out}&#34;
        )

        assert f&#34;{size}G 0 disk&#34; in out, (
            f&#34;existing Volume {size}G not found\n&#34;
            f&#34;lsblk output: {out}&#34;
        )

        # Tear down: Stop VM and remove added volumes
        code, data = api_client.vms.get(unique_vm_name)
        vm_spec = api_client.vms.Spec.from_dict(data)
        vm_spec.run_strategy = &#34;Halted&#34;
        vols, claims = [], []
        for vd in vm_spec.volumes:
            if vd[&#39;disk&#39;][&#39;name&#39;] == vol_name:
                claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
            else:
                vols.append(vd)
        else:
            vm_spec.volumes = vols

        api_client.vms.update(unique_vm_name, vm_spec)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(unique_vm_name)
            if (code == 200
                    and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                    and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
                break
            sleep(3)

        for claim in claims:
            api_client.volumes.delete(claim)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_existing_volume"><code class="name flex">
<span>def <span class="ident">test_create_with_existing_volume</span></span>(<span>self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/">https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/</a> # noqa</p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a data volume</li>
<li>Create a VM with 1 CPU 2 Memory and the existing data volume</li>
<li>Start the VM</li>
<li>Verify the VM</li>
</ol>
<p>Exepected Result:
- VM should able to start and becomes <code>Running</code>
- Disk volume should be available in the VM
- Disk size in VM should be the same as its volume configured</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_create_with_existing_volume(self, api_client, ssh_keypair, wait_timeout,
                                     host_shell, vm_shell, stopped_vm):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/ # noqa

    Steps:
        1. Create a data volume
        2. Create a VM with 1 CPU 2 Memory and the existing data volume
        3. Start the VM
        4. Verify the VM

    Exepected Result:
        - VM should able to start and becomes `Running`
        - Disk volume should be available in the VM
        - Disk size in VM should be the same as its volume configured
    &#34;&#34;&#34;
    unique_vm_name, ssh_user = stopped_vm
    pub_key, pri_key = ssh_keypair

    vol_name, size = &#39;disk-existing&#39;, 3
    vol_spec = api_client.volumes.Spec(size)
    code, data = api_client.volumes.create(f&#34;{unique_vm_name}-{vol_name}&#34;, vol_spec)

    assert 201 == code, (code, data)

    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)
    vm_spec.run_strategy = &#34;RerunOnFailure&#34;
    vm_spec.add_existing_volume(vol_name, f&#34;{unique_vm_name}-{vol_name}&#34;)

    # Start VM with added existing volume
    code, data = api_client.vms.update(unique_vm_name, vm_spec)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    # Log into VM to verify added volumes
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )
            out, err = sh.exec_command(&#34;lsblk -r&#34;)
            assert not err, (out, err)

    assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
        f&#34;Added Volumes amount is not correct.\n&#34;
        f&#34;lsblk output: {out}&#34;
    )

    assert f&#34;{size}G 0 disk&#34; in out, (
        f&#34;existing Volume {size}G not found\n&#34;
        f&#34;lsblk output: {out}&#34;
    )

    # Tear down: Stop VM and remove added volumes
    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)
    vm_spec.run_strategy = &#34;Halted&#34;
    vols, claims = [], []
    for vd in vm_spec.volumes:
        if vd[&#39;disk&#39;][&#39;name&#39;] == vol_name:
            claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
        else:
            vols.append(vd)
    else:
        vm_spec.volumes = vols

    api_client.vms.update(unique_vm_name, vm_spec)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if (code == 200
                and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
            break
        sleep(3)

    for claim in claims:
        api_client.volumes.delete(claim)</code></pre>
</details>
</dd>
<dt id="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_two_volumes"><code class="name flex">
<span>def <span class="ident">test_create_with_two_volumes</span></span>(<span>self, api_client, ssh_keypair, wait_timeout, host_shell, vm_shell, stopped_vm)</span>
</code></dt>
<dd>
<div class="desc"><p>To cover test:
- <a href="https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/">https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/</a> # noqa</p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a VM with 1 CPU 2 Memory and 2 disk volumes</li>
<li>Start the VM</li>
<li>Verify the VM</li>
</ol>
<p>Exepected Result:
- VM should able to start and becomes <code>Running</code>
- 2 disk volumes should be available in the VM
- Disk size in VM should be the same as its volume configured</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_create_with_two_volumes(self, api_client, ssh_keypair, wait_timeout,
                                 host_shell, vm_shell, stopped_vm):
    &#34;&#34;&#34;
    To cover test:
    - https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/ # noqa

    Steps:
        1. Create a VM with 1 CPU 2 Memory and 2 disk volumes
        2. Start the VM
        3. Verify the VM

    Exepected Result:
        - VM should able to start and becomes `Running`
        - 2 disk volumes should be available in the VM
        - Disk size in VM should be the same as its volume configured
    &#34;&#34;&#34;
    unique_vm_name, ssh_user = stopped_vm
    pub_key, pri_key = ssh_keypair
    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)
    vm_spec.run_strategy = &#34;RerunOnFailure&#34;
    volumes = [(&#39;disk-1&#39;, 1), (&#39;disk-2&#39;, 2)]
    for name, size in volumes:
        vm_spec.add_volume(name, size)

    # Start VM with 2 additional volumes
    code, data = api_client.vms.update(unique_vm_name, vm_spec)
    assert 200 == code, (code, data)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 200 == code:
            phase = data.get(&#39;status&#39;, {}).get(&#39;phase&#39;)
            conds = data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, [{}])
            if (&#34;Running&#34; == phase
               and &#34;AgentConnected&#34; == conds[-1].get(&#39;type&#39;)
               and data[&#39;status&#39;].get(&#39;interfaces&#39;)):
                break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    # Log into VM to verify added volumes
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(data[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)

    with host_shell.login(host_ip, jumphost=True) as h:
        vm_sh = vm_shell(ssh_user, pkey=pri_key)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                vm_sh.connect(vm_ip, jumphost=h.client)
            except ChannelException as e:
                login_ex = e
                sleep(3)
            else:
                break
        else:
            raise AssertionError(f&#34;Unable to login to VM {unique_vm_name}&#34;) from login_ex

        with vm_sh as sh:
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                out, err = sh.exec_command(&#39;cloud-init status&#39;)
                if &#39;done&#39; in out:
                    break
                sleep(3)
            else:
                raise AssertionError(
                    f&#34;VM {unique_vm_name} Started {wait_timeout} seconds&#34;
                    f&#34;, but cloud-init still in {out}&#34;
                )
            out, err = sh.exec_command(&#34;lsblk -r&#34;)
            assert not err, (out, err)

    assert 1 + len(vm_spec.volumes) == len(re.findall(&#39;disk&#39;, out)), (
        f&#34;Added Volumes amount is not correct.\n&#34;
        f&#34;lsblk output: {out}&#34;
    )
    fails = []
    for _, size in volumes:
        if not re.search(f&#34;vd.*{size}G 0 disk&#34;, out):
            fails.append(f&#34;Volume size {size}G not found&#34;)

    assert not fails, (
        f&#34;lsblk output: {out}\n&#34;
        &#34;\n&#34;.join(fails)
    )

    # Tear down: Stop VM and remove added volumes
    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)
    vm_spec.run_strategy = &#34;Halted&#34;
    vol_names, vols, claims = [n for n, s in volumes], [], []
    for vd in vm_spec.volumes:
        if vd[&#39;disk&#39;][&#39;name&#39;] in vol_names:
            claims.append(vd[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;])
        else:
            vols.append(vd)
    else:
        vm_spec.volumes = vols

    api_client.vms.update(unique_vm_name, vm_spec)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if (code == 200
                and &#39;Halted&#39; == data[&#39;spec&#39;][&#39;runStrategy&#39;]
                and &#39;Stopped&#39; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;)):
            break
        sleep(3)

    for vol_name in claims:
        api_client.volumes.delete(vol_name)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="harvester_e2e_tests.integration" href="index.html">harvester_e2e_tests.integration</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.bogus_vlan_net" href="#harvester_e2e_tests.integration.test_vm_functions.bogus_vlan_net">bogus_vlan_net</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.image" href="#harvester_e2e_tests.integration.test_vm_functions.image">image</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.small_volume" href="#harvester_e2e_tests.integration.test_vm_functions.small_volume">small_volume</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.stopped_vm" href="#harvester_e2e_tests.integration.test_vm_functions.stopped_vm">stopped_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.test_create_stopped_vm" href="#harvester_e2e_tests.integration.test_vm_functions.test_create_stopped_vm">test_create_stopped_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.test_create_vm_no_available_resources" href="#harvester_e2e_tests.integration.test_vm_functions.test_create_vm_no_available_resources">test_create_vm_no_available_resources</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.test_minimal_vm" href="#harvester_e2e_tests.integration.test_vm_functions.test_minimal_vm">test_minimal_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.test_update_vm_machine_type" href="#harvester_e2e_tests.integration.test_vm_functions.test_update_vm_machine_type">test_update_vm_machine_type</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.test_vm_with_bogus_vlan" href="#harvester_e2e_tests.integration.test_vm_functions.test_vm_with_bogus_vlan">test_vm_with_bogus_vlan</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.unique_vm_name" href="#harvester_e2e_tests.integration.test_vm_functions.unique_vm_name">unique_vm_name</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume" href="#harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume">TestHotPlugVolume</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.disk_name" href="#harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.disk_name">disk_name</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.login_to_vm_from_host" href="#harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.login_to_vm_from_host">login_to_vm_from_host</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.pytestmark" href="#harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.test_add" href="#harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.test_add">test_add</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.test_remove" href="#harvester_e2e_tests.integration.test_vm_functions.TestHotPlugVolume.test_remove">test_remove</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMClone" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMClone">TestVMClone</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.pytestmark" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMClone.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_running_vm" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_running_vm">test_clone_running_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_stopped_vm" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMClone.test_clone_stopped_vm">test_clone_stopped_vm</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations">TestVMOperations</a></code></h4>
<ul class="two-column">
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.pytestmark" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_abort_migrate" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_abort_migrate">test_abort_migrate</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_delete" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_delete">test_delete</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_migrate" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_migrate">test_migrate</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_pause" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_pause">test_pause</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_restart" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_restart">test_restart</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_softreboot" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_softreboot">test_softreboot</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_start" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_start">test_start</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_stop" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_stop">test_stop</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_unpause" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMOperations.test_unpause">test_unpause</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes">TestVMWithVolumes</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.pytestmark" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_existing_volume" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_existing_volume">test_create_with_existing_volume</a></code></li>
<li><code><a title="harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_two_volumes" href="#harvester_e2e_tests.integration.test_vm_functions.TestVMWithVolumes.test_create_with_two_volumes">test_create_with_two_volumes</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>