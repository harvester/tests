<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>harvester_e2e_tests.integrations.test_upgrade API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>harvester_e2e_tests.integrations.test_upgrade</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.cluster_network"><code class="name flex">
<span>def <span class="ident">cluster_network</span></span>(<span>vlan_nic, api_client, unique_name, network_checker, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#39;module&#39;)
def cluster_network(vlan_nic, api_client, unique_name, network_checker, wait_timeout):
    code, data = api_client.clusternetworks.get_config()
    assert 200 == code, (code, data)

    node_key = &#39;network.harvesterhci.io/matched-nodes&#39;
    cnet_nodes = dict()  # cluster_network: items
    for cfg in data[&#39;items&#39;]:
        if vlan_nic in cfg[&#39;spec&#39;][&#39;uplink&#39;][&#39;nics&#39;]:
            nodes = json.loads(cfg[&#39;metadata&#39;][&#39;annotations&#39;][node_key])
            cnet_nodes.setdefault(cfg[&#39;spec&#39;][&#39;clusterNetwork&#39;], []).extend(nodes)

    code, data = api_client.hosts.get()
    assert 200 == code, (code, data)
    all_nodes = set(n[&#39;id&#39;] for n in data[&#39;data&#39;])
    try:
        # vlad_nic configured on specific cluster network, reuse it
        yield next(cnet for cnet, nodes in cnet_nodes.items() if all_nodes == set(nodes))
        return None
    except StopIteration:
        configured_nodes = reduce(add, cnet_nodes.values(), [])
        if any(n in configured_nodes for n in all_nodes):
            raise AssertionError(
                &#34;Not all nodes&#39; VLAN NIC {vlan_nic} are available.\n&#34;
                f&#34;VLAN NIC configured nodes: {configured_nodes}\n&#34;
                f&#34;All nodes: {all_nodes}\n&#34;
            )

    # Create cluster network
    cnet = f&#34;cnet-{datetime.strptime(unique_name, &#39;%Hh%Mm%Ss%f-%m-%d&#39;).strftime(&#39;%H%M%S&#39;)}&#34;
    created = []
    code, data = api_client.clusternetworks.create(cnet)
    assert 201 == code, (code, data)
    while all_nodes:
        node = all_nodes.pop()
        code, data = api_client.clusternetworks.create_config(node, cnet, vlan_nic, hostname=node)
        assert 201 == code, (
            f&#34;Failed to create cluster config for {node}\n&#34;
            f&#34;Created: {created}\t Remaining: {all_nodes}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
        cnet_config_created, (code, data) = network_checker.wait_cnet_config_created(node)
        assert cnet_config_created, (code, data)
        created.append(node)

    yield cnet

    # Teardown
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        name = created.pop(0)
        code, data = api_client.clusternetworks.delete_config(name)
        if code != 404:
            created.append(name)
        if not created:
            break
        sleep(1)
    else:
        raise AssertionError(
            f&#34;Failed to remove network configs {created} after {wait_timeout}s\n&#34;
            f&#34;Last API Status({code}): {data}&#34;
        )

    code, data = api_client.clusternetworks.delete(cnet)
    assert 200 == code, (code, data)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.cluster_state"><code class="name flex">
<span>def <span class="ident">cluster_state</span></span>(<span>request, unique_name, api_client)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def cluster_state(request, unique_name, api_client):
    class ClusterState:
        vm1 = None
        vm2 = None
        vm3 = None
        pass

    state = ClusterState()

    if request.config.getoption(&#39;--upgrade-target-version&#39;):
        state.version_verify = True
        state.version = request.config.getoption(&#39;--upgrade-target-version&#39;)
    else:
        state.version_verify = False
        state.version = f&#34;version-{unique_name}&#34;

    return state</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.config_backup_target"><code class="name flex">
<span>def <span class="ident">config_backup_target</span></span>(<span>request, api_client, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;class&#34;)
def config_backup_target(request, api_client, wait_timeout):
    # multiple fixtures from `vm_backup_restore`
    conflict_retries = 5
    nfs_endpoint = request.config.getoption(&#39;--nfs-endpoint&#39;)
    assert nfs_endpoint, f&#34;NFS endpoint not configured: {nfs_endpoint}&#34;
    assert nfs_endpoint.startswith(&#34;nfs://&#34;), (
        f&#34;NFS endpoint should starts with `nfs://`, not {nfs_endpoint}&#34;
    )
    backup_type, config = (&#34;NFS&#34;, dict(endpoint=nfs_endpoint))

    code, data = api_client.settings.get(&#39;backup-target&#39;)
    origin_spec = api_client.settings.BackupTargetSpec.from_dict(data)

    spec = getattr(api_client.settings.BackupTargetSpec, backup_type)(**config)
    # ???: when switching S3 -&gt; NFS, update backup-target will easily hit resource conflict
    # so we would need retries to apply the change.
    for _ in range(conflict_retries):
        code, data = api_client.settings.update(&#39;backup-target&#39;, spec)
        if 409 == code and &#34;Conflict&#34; == data[&#39;reason&#39;]:
            sleep(3)
        else:
            break
    else:
        raise AssertionError(
            f&#34;Unable to update backup-target after {conflict_retries} retried.&#34;
            f&#34;API Status({code}): {data}&#34;
        )
    assert 200 == code, (
        f&#39;Failed to update backup target to {backup_type} with {config}\n&#39;
        f&#34;API Status({code}): {data}&#34;
    )

    yield spec

    # remove unbound LH backupVolumes
    code, data = api_client.lhbackupvolumes.get()
    assert 200 == code, &#34;Failed to list lhbackupvolumes&#34;

    check_names = []
    for volume_data in data[&#34;items&#34;]:
        volume_name = volume_data[&#34;metadata&#34;][&#34;name&#34;]
        backup_name = volume_data[&#34;status&#34;][&#34;lastBackupName&#34;]
        if not backup_name:
            api_client.lhbackupvolumes.delete(volume_name)
            check_names.append(volume_name)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        for name in check_names[:]:
            code, data = api_client.lhbackupvolumes.get(name)
            if 404 == code:
                check_names.remove(name)
        if not check_names:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to delete unbound lhbackupvolumes: {check_names}\n&#34;
            f&#34;Last API Status({code}): {data}&#34;
            )

    # restore to original backup-target and remove backups not belong to it
    code, data = api_client.settings.update(&#39;backup-target&#39;, origin_spec)
    code, data = api_client.backups.get()
    assert 200 == code, &#34;Failed to list backups&#34;

    check_names = []
    for backup in data[&#39;data&#39;]:
        endpoint = backup[&#39;status&#39;][&#39;backupTarget&#39;].get(&#39;endpoint&#39;)
        if endpoint != origin_spec.value.get(&#39;endpoint&#39;):
            api_client.backups.delete(backup[&#39;metadata&#39;][&#39;name&#39;])
            check_names.append(backup[&#39;metadata&#39;][&#39;name&#39;])

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        for name in check_names[:]:
            code, data = api_client.backups.get(name)
            if 404 == code:
                check_names.remove(name)
        if not check_names:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to delete backups: {check_names}\n&#34;
            f&#34;Last API Status({code}): {data}&#34;
            )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.config_storageclass"><code class="name flex">
<span>def <span class="ident">config_storageclass</span></span>(<span>request, api_client, unique_name, cluster_state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def config_storageclass(request, api_client, unique_name, cluster_state):
    replicas = request.config.getoption(&#39;--upgrade-sc-replicas&#39;) or 3

    code, default_sc = api_client.scs.get_default()
    assert 200 == code, (code, default_sc)

    sc_name = f&#34;new-sc-{replicas}-{unique_name}&#34;
    code, data = api_client.scs.create(sc_name, replicas)
    assert 201 == code, (code, data)

    code, data = api_client.scs.set_default(sc_name)
    assert 200 == code, (code, data)

    cluster_state.scs = (default_sc, data)
    yield default_sc, data

    code, data = api_client.scs.set_default(default_sc[&#39;metadata&#39;][&#39;name&#39;])
    assert 200 == code, (code, data)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.harvester_crds"><code class="name flex">
<span>def <span class="ident">harvester_crds</span></span>(<span>api_client)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def harvester_crds(api_client):
    crds = {
        &#34;addons.harvesterhci.io&#34;: False,
        &#34;blockdevices.harvesterhci.io&#34;: False,
        &#34;keypairs.harvesterhci.io&#34;: False,
        &#34;preferences.harvesterhci.io&#34;: False,
        &#34;settings.harvesterhci.io&#34;: False,
        &#34;supportbundles.harvesterhci.io&#34;: False,
        &#34;upgrades.harvesterhci.io&#34;: False,
        &#34;versions.harvesterhci.io&#34;: False,
        &#34;virtualmachinebackups.harvesterhci.io&#34;: False,
        &#34;virtualmachineimages.harvesterhci.io&#34;: False,
        &#34;virtualmachinerestores.harvesterhci.io&#34;: False,
        &#34;virtualmachinetemplates.harvesterhci.io&#34;: False,
        &#34;virtualmachinetemplateversions.harvesterhci.io&#34;: False,

        &#34;clusternetworks.network.harvesterhci.io&#34;: False,
        &#34;linkmonitors.network.harvesterhci.io&#34;: False,
        &#34;nodenetworks.network.harvesterhci.io&#34;: False,
        &#34;vlanconfigs.network.harvesterhci.io&#34;: False,
        &#34;vlanstatuses.network.harvesterhci.io&#34;: False,

        &#34;ksmtuneds.node.harvesterhci.io&#34;: False,
        &#34;loadbalancers.loadbalancer.harvesterhci.io&#34;: False,
    }

    if api_client.cluster_version.release &gt;= (1, 2, 0):
        # removed after `v1.2.0` (network-controller v0.3.3)
        # ref: https://github.com/harvester/network-controller-harvester/pull/85
        crds.pop(&#34;nodenetworks.network.harvesterhci.io&#34;)

    return crds</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.image"><code class="name flex">
<span>def <span class="ident">image</span></span>(<span>api_client, image_ubuntu, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def image(api_client, image_ubuntu, unique_name, wait_timeout):
    unique_image_id = f&#39;image-{unique_name}&#39;
    code, data = api_client.images.create_by_url(
        unique_image_id, image_ubuntu.url, display_name=f&#34;{unique_name}-{image_ubuntu.name}&#34;
    )

    assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.images.get(unique_image_id)
        if 100 == data.get(&#39;status&#39;, {}).get(&#39;progress&#39;, 0):
            break
        sleep(3)
    else:
        raise AssertionError(
            &#34;Failed to create Image with error:\n&#34;
            f&#34;Status({code}): {data}&#34;
        )

    yield dict(id=f&#34;{data[&#39;metadata&#39;][&#39;namespace&#39;]}/{unique_image_id}&#34;,
               user=image_ubuntu.ssh_user,
               first_nic=image_ubuntu.first_nic)

    code, data = api_client.images.delete(unique_image_id)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.interceptor"><code class="name flex">
<span>def <span class="ident">interceptor</span></span>(<span>api_client)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def interceptor(api_client):
    from inspect import getmembers, ismethod

    class Interceptor:

        def intercepts(self):
            meths = getmembers(self, predicate=ismethod)
            return [m for name, m in meths if name.startswith(&#34;intercept_&#34;)]

        def check(self, data):
            for func in self.intercepts():
                func(data)

    return Interceptor()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.logging_addon"><code class="name flex">
<span>def <span class="ident">logging_addon</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def logging_addon():
    return SimpleNamespace(
        namespace=&#34;cattle-logging-system&#34;,
        name=&#34;rancher-logging&#34;,
        enable_statuses=(&#34;deployed&#34;, &#34;AddonDeploySuccessful&#34;),
        enable_toggled=False
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.stopped_vm"><code class="name flex">
<span>def <span class="ident">stopped_vm</span></span>(<span>request, api_client, ssh_keypair, wait_timeout, unique_name, image)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def stopped_vm(request, api_client, ssh_keypair, wait_timeout, unique_name, image):
    unique_vm_name = f&#34;{request.node.name.lstrip(&#39;test_&#39;).replace(&#39;_&#39;, &#39;-&#39;)}-{unique_name}&#34;
    cpu, mem = 1, 2
    pub_key, pri_key = ssh_keypair
    vm_spec = api_client.vms.Spec(cpu, mem)
    vm_spec.add_image(&#34;disk-0&#34;, image[&#39;id&#39;])
    vm_spec.run_strategy = &#34;Halted&#34;

    userdata = yaml.safe_load(vm_spec.user_data)
    userdata[&#39;ssh_authorized_keys&#39;] = [pub_key]
    vm_spec.user_data = yaml.dump(userdata)

    code, data = api_client.vms.create(unique_vm_name, vm_spec)
    assert 201 == code, (code, data)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(unique_vm_name)
        if &#34;Stopped&#34; == data.get(&#39;status&#39;, {}).get(&#39;printableStatus&#39;):
            break
        sleep(1)

    yield unique_vm_name, image[&#39;user&#39;], pri_key

    code, data = api_client.vms.get(unique_vm_name)
    vm_spec = api_client.vms.Spec.from_dict(data)

    api_client.vms.delete(unique_vm_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get_status(unique_vm_name)
        if 404 == code:
            break
        sleep(3)

    for vol in vm_spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        api_client.volumes.delete(vol_name)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.upgrade_target"><code class="name flex">
<span>def <span class="ident">upgrade_target</span></span>(<span>request, unique_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def upgrade_target(request, unique_name):
    version = request.config.getoption(&#39;--upgrade-target-version&#39;).strip()
    version = version or f&#34;upgrade-{unique_name}&#34;
    iso_url = request.config.getoption(&#39;--upgrade-iso-url&#39;).strip()
    assert iso_url, &#34;Target ISO URL should not be empty&#34;
    checksum = request.config.getoption(&#34;--upgrade-iso-checksum&#34;).strip()
    assert checksum, &#34;Checksum for Target ISO should not be empty&#34;

    return version, iso_url, checksum</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.vm_network"><code class="name flex">
<span>def <span class="ident">vm_network</span></span>(<span>api_client, unique_name, wait_timeout, cluster_network, vlan_id, cluster_state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;module&#34;)
def vm_network(api_client, unique_name, wait_timeout, cluster_network, vlan_id, cluster_state):
    code, data = api_client.networks.create(
        unique_name, vlan_id, cluster_network=cluster_network
    )
    assert 201 == code, (code, data)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.networks.get(unique_name)
        annotations = data[&#39;metadata&#39;].get(&#39;annotations&#39;, {})
        if 200 == code and annotations.get(&#39;network.harvesterhci.io/route&#39;):
            route = json.loads(annotations[&#39;network.harvesterhci.io/route&#39;])
            if route[&#39;cidr&#39;]:
                break
        sleep(3)
    else:
        raise AssertionError(
            &#34;VM network created but route info not available\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

    cluster_state.network = data
    yield dict(name=unique_name, cidr=route[&#39;cidr&#39;], namespace=data[&#39;metadata&#39;][&#39;namespace&#39;])

    code, data = api_client.networks.delete(unique_name)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.networks.get(unique_name)
        if 404 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;Failed to remove VM network {unique_name} after {wait_timeout}s\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade"><code class="flex name class">
<span>class <span class="ident">TestAnyNodesUpgrade</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.upgrade
@pytest.mark.any_nodes
class TestAnyNodesUpgrade:
    @pytest.mark.dependency(name=&#34;preq_setup_logging&#34;)
    def test_preq_logging_pods(self, api_client, logging_addon, wait_timeout, sleep_timeout):
        # logging is an addon instead of built-in since v1.2.0
        if api_client.cluster_version.release &gt;= (1, 2, 0):
            addon = &#34;/&#34;.join([logging_addon.namespace, logging_addon.name])
            code, data = api_client.addons.get(addon)
            assert 200 == code, (code, data)

            if not data.get(&#39;status&#39;, {}).get(&#39;status&#39;) in logging_addon.enable_statuses:
                code, data = api_client.addons.enable(addon)
                assert 200 == code, (code, data)
                assert data.get(&#39;spec&#39;, {}).get(&#39;enabled&#39;, False), (code, data)
                logging_addon.enable_toggled = True

                endtime = datetime.now() + timedelta(seconds=wait_timeout)
                while endtime &gt; datetime.now():
                    code, data = api_client.addons.get(addon)
                    if data.get(&#39;status&#39;, {}).get(&#39;status&#39;) in logging_addon.enable_statuses:
                        break
                    sleep(sleep_timeout)
                else:
                    raise AssertionError(
                        f&#34;Failed to enable addon {addon} with {wait_timeout} timed out\n&#34;
                        f&#34;API Status({code}): {data}&#34;
                    )

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, pods = api_client.get_pods(namespace=logging_addon.namespace)
            assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, &#34;No logging pods found&#34;

            fails = []
            for pod in pods[&#39;data&#39;]:
                name, phase = pod[&#39;metadata&#39;][&#39;name&#39;], pod[&#34;status&#34;][&#34;phase&#34;]
                if phase not in (&#34;Running&#34;, &#34;Succeeded&#34;):
                    fails.append((name, phase))
            if not fails:
                break
            sleep(sleep_timeout)
        else:
            raise AssertionError(
                &#34;\n&#34;.join(f&#34;Pod({n})&#39;s phase({p}) is not expected.&#34; for n, p in fails))

    @pytest.mark.dependency(name=&#34;preq_setup_vmnetwork&#34;)
    def test_preq_setup_vmnetwork(self, vm_network):
        &#39;&#39;&#39; Be used to trigger the fixture to setup VM network &#39;&#39;&#39;

    @pytest.mark.dependency(name=&#34;preq_setup_storageclass&#34;)
    def test_preq_setup_storageclass(self, config_storageclass):
        &#34;&#34;&#34; Be used to trigger the fixture to setup storageclass&#34;&#34;&#34;

    @pytest.mark.dependency(name=&#34;preq_setup_vms&#34;)
    def test_preq_setup_vms(
        self, api_client, ssh_keypair, unique_name, vm_checker, vm_shell, vm_network, image,
        config_storageclass, config_backup_target, wait_timeout, cluster_state
    ):
        # create new storage class, make it default
        # create 3 VMs:
        # - having the new storage class
        # - the VM that have some data written, take backup
        # - the VM restored from the backup
        pub_key, pri_key = ssh_keypair
        old_sc, new_sc = config_storageclass
        unique_vm_name = f&#34;ug-vm-{unique_name}&#34;

        cpu, mem, size = 1, 2, 10
        vm_spec = api_client.vms.Spec(cpu, mem, mgmt_network=False)
        vm_spec.add_image(&#39;disk-0&#39;, image[&#39;id&#39;], size=size)
        vm_spec.add_network(&#39;nic-1&#39;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)
        userdata = yaml.safe_load(vm_spec.user_data)
        userdata[&#39;ssh_authorized_keys&#39;] = [pub_key]
        vm_spec.user_data = yaml.dump(userdata)
        # Ref. https://docs.harvesterhci.io/v1.7/vm/backup-restore#create-a-vm-backup
        vm_spec.network_data = yaml.dump({
            &#34;version&#34;: 2,
            &#34;ethernets&#34;: {
                image[&#34;first_nic&#34;]: {
                    &#34;dhcp4&#34;: True,
                    &#34;dhcp6&#34;: True,
                    &#34;dhcp-identifier&#34;: &#34;mac&#34;
                }
            }
        })

        code, data = api_client.vms.create(unique_vm_name, vm_spec)
        assert 201 == code, (code, data)
        vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(unique_vm_name, [&#34;nic-1&#34;])
        assert vm_got_ips, (
            f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;nic-1&#39;)
        # write data into VM
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                with vm_shell.login(vm_ip, image[&#39;user&#39;], pkey=pri_key) as sh:
                    cloud_inited, (out, err) = vm_checker.wait_cloudinit_done(sh)
                    assert cloud_inited and not err, (out, err)
                    out, err = sh.exec_command(
                        &#34;dd if=/dev/urandom of=./generate_file bs=1M count=1024; sync&#34;
                    )
                    assert not out, (out, err)
                    vm1_md5, err = sh.exec_command(
                        &#34;md5sum ./generate_file &gt; ./generate_file.md5; cat ./generate_file.md5&#34;
                    )
                    assert not err, (vm1_md5, err)
                    break
            except (SSHException, NoValidConnectionsError, TimeoutError):
                sleep(5)
        else:
            raise AssertionError(&#34;Timed out while writing data into VM&#34;)

        # Take backup then check it&#39;s ready
        code, data = api_client.vms.backup(unique_vm_name, unique_vm_name)
        assert 204 == code, (code, data)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, backup = api_client.backups.get(unique_vm_name)
            if 200 == code and backup.get(&#39;status&#39;, {}).get(&#39;readyToUse&#39;):
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#39;Timed-out waiting for the backup \&#39;{unique_vm_name}\&#39; to be ready.&#39;
            )
        # restore into new VM
        restored_vm_name = f&#34;r-{unique_vm_name}&#34;
        spec = api_client.backups.RestoreSpec.for_new(restored_vm_name)
        code, data = api_client.backups.restore(unique_vm_name, spec)
        assert 201 == code, (code, data)
        # Check restore VM is created
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(restored_vm_name)
            if 200 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;restored VM {restored_vm_name} is not created&#34;
            )
        vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(restored_vm_name, [&#34;nic-1&#34;])
        assert vm_got_ips, (
            f&#34;Failed to Start VM({restored_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )
        # Check data consistency
        r_vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                       if iface[&#39;name&#39;] == &#39;nic-1&#39;)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                with vm_shell.login(r_vm_ip, image[&#39;user&#39;], pkey=pri_key) as sh:
                    cloud_inited, (out, err) = vm_checker.wait_cloudinit_done(sh)
                    assert cloud_inited and not err, (out, err)
                    out, err = sh.exec_command(&#34;md5sum -c ./generate_file.md5&#34;)
                    assert not err, (out, err)
                    vm2_md5, err = sh.exec_command(&#34;cat ./generate_file.md5&#34;)
                    assert not err, (vm2_md5, err)
                    assert vm1_md5 == vm2_md5
                    out, err = sh.exec_command(
                        f&#34;ping -c1 {vm_ip} &gt; /dev/null &amp;&amp; echo -n success || echo -n fail&#34;
                    )
                    assert &#34;success&#34; == out and not err
                    break
            except (SSHException, NoValidConnectionsError, ConnectionResetError, TimeoutError):
                sleep(5)
        else:
            raise AssertionError(&#34;Unable to login to restored VM to check data consistency&#34;)

        # Create VM having additional volume with new storage class
        vm_spec.add_volume(&#34;vol-1&#34;, 5, storage_cls=new_sc[&#39;metadata&#39;][&#39;name&#39;])
        code, data = api_client.vms.create(f&#34;sc-{unique_vm_name}&#34;, vm_spec)
        assert 201 == code, (code, data)
        vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(f&#34;sc-{unique_vm_name}&#34;, [&#34;nic-1&#34;])
        assert vm_got_ips, (
            f&#34;Failed to Start VM(sc-{unique_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

        # store into cluster&#39;s state
        names = [unique_vm_name, f&#34;r-{unique_vm_name}&#34;, f&#34;sc-{unique_vm_name}&#34;]
        cluster_state.vms = dict(md5=vm1_md5, names=names, ssh_user=image[&#39;user&#39;], pkey=pri_key)

    @pytest.mark.dependency(name=&#34;any_nodes_upgrade&#34;)
    def test_perform_upgrade(
        self, api_client, unique_name, upgrade_target, upgrade_timeout, interceptor,
        upgrade_checker
    ):
        &#34;&#34;&#34;
        - perform upgrade
        - check all nodes upgraded
        &#34;&#34;&#34;
        # Check nodes counts
        code, data = api_client.hosts.get()
        assert code == 200, (code, data)
        nodes = len(data[&#39;data&#39;])

        # create Upgrade version and start
        skip_version_check = {&#34;harvesterhci.io/skip-version-check&#34;: True}  # for test purpose
        version, url, checksum = upgrade_target
        version = f&#34;{version}-{unique_name}&#34;
        code, data = api_client.versions.create(version, url, checksum)
        assert 201 == code, f&#34;Failed to create upgrade for {version}&#34;
        version_created, (code, data) = upgrade_checker.wait_version_created(version)
        assert version_created, (code, data)
        code, data = api_client.upgrades.create(version, annotations=skip_version_check)
        assert 201 == code, f&#34;Failed to start upgrade for {version}&#34;
        upgrade_name = data[&#39;metadata&#39;][&#39;name&#39;]

        # Check upgrade status
        # TODO: check every upgrade stages
        endtime = datetime.now() + timedelta(seconds=upgrade_timeout * nodes)
        while endtime &gt; datetime.now():
            code, data = api_client.upgrades.get(upgrade_name)
            if 200 != code:
                continue
            interceptor.check(data)
            conds = dict((c[&#39;type&#39;], c) for c in data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, []))
            state = data.get(&#39;metadata&#39;, {}).get(&#39;labels&#39;, {}).get(&#39;harvesterhci.io/upgradeState&#39;)
            if &#34;Succeeded&#34; == state and &#34;True&#34; == conds.get(&#39;Completed&#39;, {}).get(&#39;status&#39;):
                break
            if any(&#34;False&#34; == c[&#39;status&#39;] for c in conds.values()):
                raise AssertionError(f&#34;Upgrade failed with conditions: {conds.values()}&#34;)
            sleep(30)
        else:
            raise AssertionError(
                f&#34;Upgrade timed out with conditions: {conds.values()}\n&#34;
                f&#34;API Status({code}): {data}&#34;
            )

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_logging&#34;])
    def test_verify_logging_pods(self, api_client, logging_addon):
        &#34;&#34;&#34; Verify logging pods and logs
        Criteria: https://github.com/harvester/tests/issues/535
        &#34;&#34;&#34;
        # logging is an addon instead of built-in since v1.2.0
        if api_client.cluster_version.release &gt;= (1, 2, 0):
            addon = &#34;/&#34;.join([logging_addon.namespace, logging_addon.name])
            code, data = api_client.addons.get(addon)
            assert data.get(&#39;status&#39;, {}).get(&#39;status&#39;) in logging_addon.enable_statuses

        code, pods = api_client.get_pods(namespace=logging_addon.namespace)
        assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, &#34;No logging pods found&#34;

        fails = []
        for pod in pods[&#39;data&#39;]:
            phase = pod[&#34;status&#34;][&#34;phase&#34;]
            if phase not in (&#34;Running&#34;, &#34;Succeeded&#34;):
                fails.append((pod[&#39;metadata&#39;][&#39;name&#39;], phase))
        else:
            assert not fails, (
                &#34;\n&#34;.join(f&#34;Pod({n})&#39;s phase({p}) is not expected.&#34; for n, p in fails)
            )

        # teardown
        if logging_addon.enable_toggled:
            api_client.addons.disable(addon)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_audit_log(self, api_client, host_shell, wait_timeout):
        code, data = api_client.hosts.get()
        assert 200 == code, (code, data)
        label_main = &#34;node-role.kubernetes.io/control-plane&#34;
        masters = [n for n in data[&#39;data&#39;] if n[&#39;metadata&#39;][&#39;labels&#39;].get(label_main) == &#34;true&#34;]
        assert len(masters) &gt; 0, &#34;No master nodes found&#34;

        script = (&#34;sudo tail /var/lib/rancher/rke2/server/logs/audit.log | awk &#39;END{print}&#39; &#34;
                  &#34;| jq .requestReceivedTimestamp &#34;
                  &#34;| xargs -I {} date -d \&#34;{}\&#34; +%s&#34;)

        node_ips = [n[&#34;metadata&#34;][&#34;annotations&#34;][NODE_INTERNAL_IP_ANNOTATION] for n in masters]
        cmp = dict()
        done = set()
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            for ip in done.symmetric_difference(node_ips):
                try:
                    with host_shell.login(ip) as shell:
                        out, err = shell.exec_command(script)
                        timestamp = int(out)
                        if not err and ip not in cmp:
                            cmp[ip] = timestamp
                            continue
                        if not err and cmp[ip] &lt; timestamp:
                            done.add(ip)
                except (SSHException, NoValidConnectionsError, ConnectionResetError, TimeoutError):
                    continue

            if not done.symmetric_difference(node_ips):
                break
            sleep(5)
        else:
            raise AssertionError(
                &#34;\n&#34;.join(&#34;Node {ip} audit log is not updated.&#34; for ip in set(node_ips) ^ done)
            )

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_vmnetwork&#34;])
    def test_verify_network(self, api_client, cluster_state):
        &#34;&#34;&#34; Verify cluster and VLAN networks
        - cluster network `mgmt` should exists
        - Created VLAN should exists
        &#34;&#34;&#34;

        code, cnets = api_client.clusternetworks.get()
        assert code == 200, (
            &#34;Failed to get Networks: %d, %s&#34; % (code, cnets))

        assert len(cnets[&#34;items&#34;]) &gt; 0, (&#34;No Networks found&#34;)

        assert any(n[&#39;metadata&#39;][&#39;name&#39;] == &#34;mgmt&#34; for n in cnets[&#39;items&#39;]), (
            &#34;Cluster network mgmt not found&#34;)

        code, vnets = api_client.networks.get()
        assert code == 200, (f&#34;Failed to get VLANs: {code}, {vnets}&#34; % (code, vnets))
        assert len(vnets[&#34;items&#34;]) &gt; 0, (&#34;No VLANs found&#34;)

        used_vlan = cluster_state.network[&#39;metadata&#39;][&#39;name&#39;]
        assert any(used_vlan == n[&#39;metadata&#39;][&#39;name&#39;] for n in vnets[&#39;items&#39;]), (
            f&#34;VLAN {used_vlan} not found&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_vms&#34;])
    def test_verify_vms(self, api_client, cluster_state, vm_shell, vm_checker, wait_timeout):
        &#34;&#34;&#34; Verify VMs&#39; state and data
        Criteria:
        - VMs should keep in running state
        - data in VMs should not lost
        &#34;&#34;&#34;

        code, vmis = api_client.vms.get_status()
        assert code == 200 and len(vmis[&#39;data&#39;]), (code, vmis)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            fails, ips = list(), dict()
            for name in cluster_state.vms[&#39;names&#39;]:
                code, data = api_client.vms.get_status(name)
                try:
                    assert 200 == code
                    assert &#34;Running&#34; == data[&#39;status&#39;][&#39;phase&#39;]
                    assert data[&#39;status&#39;][&#39;nodeName&#39;]
                    ips[name] = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                                     if iface[&#39;name&#39;] == &#39;nic-1&#39;)
                except (AssertionError, TypeError, StopIteration, KeyError) as ex:
                    fails.append((name, (ex, code, data)))
            if not fails:
                break
        else:
            raise AssertionError(&#34;\n&#34;.join(
                f&#34;VM {name} is not in expected state.\nException: {ex}\nAPI Status({code}): {data}&#34;
                for (name, (ex, code, data)) in fails)
            )

        pri_key, ssh_user = cluster_state.vms[&#39;pkey&#39;], cluster_state.vms[&#39;ssh_user&#39;]
        for name in cluster_state.vms[&#39;names&#39;][:-1]:
            vm_ip = ips[name]
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                try:
                    with vm_shell.login(vm_ip, ssh_user, pkey=pri_key) as sh:
                        out, err = sh.exec_command(&#34;md5sum -c ./generate_file.md5&#34;)
                        assert not err, (out, err)
                        md5, err = sh.exec_command(&#34;cat ./generate_file.md5&#34;)
                        assert not err, (md5, err)
                        assert md5 == cluster_state.vms[&#39;md5&#39;]
                        break
                except (SSHException, NoValidConnectionsError, ConnectionResetError, TimeoutError):
                    sleep(5)
            else:
                fails.append(f&#34;Data in VM({name}, {vm_ip}) is inconsistent.&#34;)

        assert not fails, &#34;\n&#34;.join(fails)

        # Teardown: remove all VMs
        for name in cluster_state.vms[&#39;names&#39;]:
            code, data = api_client.vms.get(name)
            spec = api_client.vms.Spec.from_dict(data)
            _ = vm_checker.wait_deleted(name)
            for vol in spec.volumes:
                vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
                api_client.volumes.delete(vol_name)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_vms&#34;])
    def test_verify_restore_vm(
        self, api_client, cluster_state, vm_shell, vm_checker, wait_timeout
    ):
        &#34;&#34;&#34; Verify VM restored from the backup
        Criteria:
        - VM should able to start
        - data in VM should not lost
        &#34;&#34;&#34;

        backup_name = cluster_state.vms[&#39;names&#39;][0]
        restored_vm_name = f&#34;new-r-{backup_name}&#34;

        # Restore VM from backup and check networking is good
        restore_spec = api_client.backups.RestoreSpec.for_new(restored_vm_name)
        code, data = api_client.backups.restore(backup_name, restore_spec)
        assert code == 201, &#34;Unable to restore backup {backup_name} after upgrade&#34;
        # Check restore VM is created
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(restored_vm_name)
            if 200 == code:
                break
            sleep(3)
        else:
            raise AssertionError(
                f&#34;restored VM {restored_vm_name} is not created&#34;
            )
        vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(restored_vm_name, [&#34;nic-1&#34;])
        assert vm_got_ips, (
            f&#34;Failed to Start VM({restored_vm_name}) with errors:\n&#34;
            f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )

        # Check data in restored VM is consistent
        pri_key, ssh_user = cluster_state.vms[&#39;pkey&#39;], cluster_state.vms[&#39;ssh_user&#39;]
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;nic-1&#39;)
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                with vm_shell.login(vm_ip, ssh_user, pkey=pri_key) as sh:
                    cloud_inited, (out, err) = vm_checker.wait_cloudinit_done(sh)
                    assert cloud_inited and not err, (out, err)
                    out, err = sh.exec_command(&#34;md5sum -c ./generate_file.md5&#34;)
                    assert not err, (out, err)
                    md5, err = sh.exec_command(&#34;cat ./generate_file.md5&#34;)
                    assert not err, (md5, err)
                    assert md5 == cluster_state.vms[&#39;md5&#39;]
                    break
            except (SSHException, NoValidConnectionsError, ConnectionResetError, TimeoutError):
                sleep(5)
        else:
            raise AssertionError(&#34;Unable to login to restored VM to check data consistency&#34;)

        # teardown: remove the VM
        code, data = api_client.vms.get(restored_vm_name)
        spec = api_client.vms.Spec.from_dict(data)
        _ = vm_checker.wait_deleted(restored_vm_name)
        for vol in spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_storageclass&#34;])
    def test_verify_storage_class(self, api_client, cluster_state):
        &#34;&#34;&#34; Verify StorageClasses and defaults
        - `new_sc` should be settle as default
        - `longhorn` should exists
        &#34;&#34;&#34;
        code, scs = api_client.scs.get()
        assert code == 200, (&#34;Failed to get StorageClasses: %d, %s&#34; % (code, scs))
        assert len(scs[&#34;items&#34;]) &gt; 0, (&#34;No StorageClasses found&#34;)

        created_sc = cluster_state.scs[-1][&#39;metadata&#39;][&#39;name&#39;]
        names = {sc[&#39;metadata&#39;][&#39;name&#39;]: sc[&#39;metadata&#39;].get(&#39;annotations&#39;) for sc in scs[&#39;items&#39;]}
        assert &#34;longhorn&#34; in names
        assert created_sc in names
        assert &#34;storageclass.kubernetes.io/is-default-class&#34; in names[created_sc]
        assert &#34;true&#34; == names[created_sc][&#34;storageclass.kubernetes.io/is-default-class&#34;]

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_os_version(self, request, api_client, cluster_state, host_shell):
        # Verify /etc/os-release on all nodes
        script = &#34;cat /etc/os-release&#34;
        if not cluster_state.version_verify:
            pytest.skip(&#34;skip verify os version&#34;)

        # Get all nodes
        code, data = api_client.hosts.get()
        assert 200 == code, (code, data)
        for node in data[&#39;data&#39;]:
            node_ip = node[&#34;metadata&#34;][&#34;annotations&#34;][NODE_INTERNAL_IP_ANNOTATION]

            with host_shell.login(node_ip) as sh:
                lines, stderr = sh.exec_command(script, get_pty=True, splitlines=True)
                assert not stderr, (
                    f&#34;Failed to execute {script} on {node_ip}: {stderr}&#34;)

                # eg: PRETTY_NAME=&#34;Harvester v1.1.0&#34;
                assert cluster_state.version == re.findall(r&#34;Harvester (.+?)\&#34;&#34;, lines[3])[0], (
                    &#34;OS version is not correct&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_rke2_version(self, api_client, host_shell):
        # Verify node version on all nodes
        script = &#34;cat /etc/harvester-release.yaml&#34;

        label_main = &#34;node-role.kubernetes.io/control-plane&#34;
        code, data = api_client.hosts.get()
        assert 200 == code, (code, data)
        masters = [n for n in data[&#39;data&#39;] if n[&#39;metadata&#39;][&#39;labels&#39;].get(label_main) == &#34;true&#34;]

        # Verify rke2 version
        except_rke2_version = &#34;&#34;
        for node in masters:
            node_ip = node[&#34;metadata&#34;][&#34;annotations&#34;][NODE_INTERNAL_IP_ANNOTATION]

            # Get except rke2 version
            if except_rke2_version == &#34;&#34;:
                with host_shell.login(node_ip) as sh:
                    lines, stderr = sh.exec_command(script, get_pty=True, splitlines=True)
                    assert not stderr, (
                        f&#34;Failed to execute {script} on {node_ip}: {stderr}&#34;)

                    for line in lines:
                        if &#34;kubernetes&#34; in line:
                            except_rke2_version = re.findall(r&#34;kubernetes: (.*)&#34;, line.strip())[0]
                            break

                    assert except_rke2_version != &#34;&#34;, (&#34;Failed to get except rke2 version&#34;)

            assert node.get(&#39;status&#39;, {}).get(&#39;nodeInfo&#39;, {}).get(
                   &#34;kubeletVersion&#34;, &#34;&#34;) == except_rke2_version, (
                   &#34;rke2 version is not correct&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_deployed_components_version(self, api_client):
        &#34;&#34;&#34; Verify deployed kubevirt and longhorn version
        Criteria:
        - except version(get from apps.catalog.cattle.io/harvester) should be equal to the version
          of kubevirt and longhorn
        &#34;&#34;&#34;

        def check_image_version(old, new):
            def sanitized_ver(img: str):
                try:
                    # PEP 440 compliant: N(.N)*[-+]?[{a|alpha|b|beta|c|rc}N][.postN][.devN]
                    ver_str = img.split(&#39;:&#39;, 1)[-1]
                    ver_obj = parse_version(ver_str)
                    return ver_obj
                except InvalidVersion as e:
                    if &#34;-&#34; in ver_str:
                        # Conform to PEP 440 by replacing the first &#39;-&#39; with &#39;+&#39;
                        # e.g. 1.2.3-4.5.6 -&gt; 1.2.3+4.5.6, v1.10.1-hotfix-2 -&gt; v1.10.1+hotfix
                        ver_obj = parse_version(&#34;+&#34;.join(ver_str.split(&#34;-&#34;)[:2]))
                        logger.warning(f&#34;Convert {img} to {ver_obj} for comparison&#34;)
                        return ver_obj
                    raise e

            old_le_new = (sanitized_ver(old) &lt;= sanitized_ver(new))
            if not old_le_new:
                logger.error(f&#34;Old image {old} does not less or equal to the new {new}&#34;)

            return old_le_new

        kubevirt_version_existed = False
        engine_image_version_existed = False
        longhorn_manager_version_existed = False

        # Get expected image of kubevirt
        code, app = api_client.get_apps_deployments(name=&#34;virt-operator&#34;,
                                                    namespace=DEFAULT_HARVESTER_NAMESPACE)
        assert code == 200, (code, app)
        kubevirt_operator_image = app[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;]

        # Get except image of longhorn
        code, apps = api_client.get_apps_controllerrevisions(namespace=DEFAULT_LONGHORN_NAMESPACE)
        assert code == 200, (code, apps)

        longhorn_images = {
            &#34;engine-image&#34;: &#34;&#34;,
            &#34;longhorn-manager&#34;: &#34;&#34;
        }
        for lh_app in longhorn_images:
            for app in apps[&#39;data&#39;]:
                if app[&#34;id&#34;].startswith(f&#34;{DEFAULT_LONGHORN_NAMESPACE}/{lh_app}&#34;):
                    longhorn_images[lh_app] = (
                        app[&#34;data&#34;][&#34;spec&#34;][&#34;template&#34;][&#34;spec&#34;][&#34;containers&#34;][0][&#34;image&#34;])
                    break

        # Verify kubevirt version
        code, pods = api_client.get_pods(namespace=DEFAULT_HARVESTER_NAMESPACE)
        assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, (
            f&#34;Failed to get pods in namespace {DEFAULT_HARVESTER_NAMESPACE}&#34;)

        for pod in pods[&#39;data&#39;]:
            if &#34;virt-operator&#34; in pod[&#39;metadata&#39;][&#39;name&#39;]:
                kubevirt_version_existed = check_image_version(
                    kubevirt_operator_image, pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;]
                )

        # Verify longhorn version
        code, pods = api_client.get_pods(namespace=DEFAULT_LONGHORN_NAMESPACE)
        assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, (
            f&#34;Failed to get pods in namespace {DEFAULT_LONGHORN_NAMESPACE}&#34;)

        for pod in pods[&#39;data&#39;]:
            if &#34;longhorn-manager&#34; in pod[&#39;metadata&#39;][&#39;name&#39;]:
                longhorn_manager_version_existed = check_image_version(
                  longhorn_images[&#34;longhorn-manager&#34;], pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;]
                )
            elif &#34;engine-image&#34; in pod[&#39;metadata&#39;][&#39;name&#39;]:
                engine_image_version_existed = check_image_version(
                    longhorn_images[&#34;engine-image&#34;], pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;]
                )

        assert kubevirt_version_existed, &#34;kubevirt version is not correct&#34;
        assert engine_image_version_existed, &#34;longhorn engine image version is not correct&#34;
        assert longhorn_manager_version_existed, &#34;longhorn manager version is not correct&#34;

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_crds_existed(self, api_client, harvester_crds):
        &#34;&#34;&#34; Verify crds existed
        Criteria:
        - crds should be existed
        &#34;&#34;&#34;
        not_existed_crds = []
        exist_crds = True
        for crd in harvester_crds:
            code, _ = api_client.get_crds(name=crd)

            if code != 200:
                exist_crds = False
                not_existed_crds.append(crd)

        if not exist_crds:
            raise AssertionError(f&#34;CRDs {not_existed_crds} are not existed&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_verify_upgradelog(self, api_client):
        &#34;&#34;&#34; Verify upgradelog pod and volume existed when upgrade with &#34;Enable Logging&#34;
        &#34;&#34;&#34;
        # pod
        code, data = api_client.get_pods(namespace=&#39;harvester-system&#39;)
        assert code == 200 and data[&#39;data&#39;], (code, data)

        upgradelog_pods = [pod for pod in data[&#39;data&#39;] if &#39;upgradelog&#39; in pod[&#39;id&#39;]]
        assert upgradelog_pods, f&#34;No upgradelog pod found:\n{data[&#39;data&#39;]}&#34;
        for pod in upgradelog_pods:
            assert pod[&#34;status&#34;][&#34;phase&#34;] == &#34;Running&#34;, (code, upgradelog_pods)

        # volume
        code, data = api_client.volumes.get(namespace=&#39;harvester-system&#39;)
        assert code == 200 and data[&#39;data&#39;], (code, data)

        upgradelog_vols = [vol for vol in data[&#39;data&#39;] if &#39;upgradelog&#39; in vol[&#39;id&#39;]]
        assert upgradelog_vols, f&#34;No upgradelog volume found:\n{data[&#39;data&#39;]}&#34;
        for vol in upgradelog_vols:
            assert not vol[&#34;metadata&#34;][&#39;state&#39;][&#39;error&#39;], (code, upgradelog_vols)
            assert not vol[&#34;metadata&#34;][&#39;state&#39;][&#39;transitioning&#39;], (code, upgradelog_vols)
            assert vol[&#39;status&#39;][&#39;phase&#39;] == &#34;Bound&#34;, (code, upgradelog_vols)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_upgrade_vm_deleted(self, api_client, wait_timeout):
        # max to wait 300s for the upgrade related VMs to be deleted
        endtime = datetime.now() + timedelta(seconds=min(wait_timeout / 5, 300))
        while endtime &gt; datetime.now():
            code, data = api_client.vms.get(namespace=&#39;harvester-system&#39;)
            upgrade_vms = [vm for vm in data[&#39;data&#39;] if &#39;upgrade&#39; in vm[&#39;id&#39;]]
            if not upgrade_vms:
                break
        else:
            raise AssertionError(f&#34;Upgrade related VM still available:\n{upgrade_vms}&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_upgrade_volume_deleted(self, api_client, wait_timeout):
        # max to wait 300s for the upgrade related volumes to be deleted
        endtime = datetime.now() + timedelta(seconds=min(wait_timeout / 5, 300))
        while endtime &gt; datetime.now():
            code, data = api_client.volumes.get(namespace=&#39;harvester-system&#39;)
            upgrade_vols = [vol for vol in data[&#39;data&#39;]
                            if &#39;upgrade&#39; in vol[&#39;id&#39;] and &#39;log-archive&#39; not in vol[&#39;id&#39;]]
            if not upgrade_vols:
                break
        else:
            raise AssertionError(f&#34;Upgrade related volume(s) still available:\n{upgrade_vols}&#34;)

    @pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
    def test_upgrade_image_deleted(self, api_client, wait_timeout):
        # max to wait 300s for the upgrade related volumes to be deleted
        endtime = datetime.now() + timedelta(seconds=min(wait_timeout / 5, 300))
        while endtime &gt; datetime.now():
            code, data = api_client.images.get(namespace=&#39;harvester-system&#39;)
            upgrade_images = [image for image in data[&#39;items&#39;]
                              if &#39;upgrade&#39; in image[&#39;spec&#39;][&#39;displayName&#39;]]
            if not upgrade_images:
                break
        else:
            raise AssertionError(f&#34;Upgrade related image(s) still available:\n{upgrade_images}&#34;)</code></pre>
</details>
<div class="desc"></div>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_perform_upgrade"><code class="name flex">
<span>def <span class="ident">test_perform_upgrade</span></span>(<span>self,<br>api_client,<br>unique_name,<br>upgrade_target,<br>upgrade_timeout,<br>interceptor,<br>upgrade_checker)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;any_nodes_upgrade&#34;)
def test_perform_upgrade(
    self, api_client, unique_name, upgrade_target, upgrade_timeout, interceptor,
    upgrade_checker
):
    &#34;&#34;&#34;
    - perform upgrade
    - check all nodes upgraded
    &#34;&#34;&#34;
    # Check nodes counts
    code, data = api_client.hosts.get()
    assert code == 200, (code, data)
    nodes = len(data[&#39;data&#39;])

    # create Upgrade version and start
    skip_version_check = {&#34;harvesterhci.io/skip-version-check&#34;: True}  # for test purpose
    version, url, checksum = upgrade_target
    version = f&#34;{version}-{unique_name}&#34;
    code, data = api_client.versions.create(version, url, checksum)
    assert 201 == code, f&#34;Failed to create upgrade for {version}&#34;
    version_created, (code, data) = upgrade_checker.wait_version_created(version)
    assert version_created, (code, data)
    code, data = api_client.upgrades.create(version, annotations=skip_version_check)
    assert 201 == code, f&#34;Failed to start upgrade for {version}&#34;
    upgrade_name = data[&#39;metadata&#39;][&#39;name&#39;]

    # Check upgrade status
    # TODO: check every upgrade stages
    endtime = datetime.now() + timedelta(seconds=upgrade_timeout * nodes)
    while endtime &gt; datetime.now():
        code, data = api_client.upgrades.get(upgrade_name)
        if 200 != code:
            continue
        interceptor.check(data)
        conds = dict((c[&#39;type&#39;], c) for c in data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, []))
        state = data.get(&#39;metadata&#39;, {}).get(&#39;labels&#39;, {}).get(&#39;harvesterhci.io/upgradeState&#39;)
        if &#34;Succeeded&#34; == state and &#34;True&#34; == conds.get(&#39;Completed&#39;, {}).get(&#39;status&#39;):
            break
        if any(&#34;False&#34; == c[&#39;status&#39;] for c in conds.values()):
            raise AssertionError(f&#34;Upgrade failed with conditions: {conds.values()}&#34;)
        sleep(30)
    else:
        raise AssertionError(
            f&#34;Upgrade timed out with conditions: {conds.values()}\n&#34;
            f&#34;API Status({code}): {data}&#34;
        )</code></pre>
</details>
<div class="desc"><ul>
<li>perform upgrade</li>
<li>check all nodes upgraded</li>
</ul></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_logging_pods"><code class="name flex">
<span>def <span class="ident">test_preq_logging_pods</span></span>(<span>self, api_client, logging_addon, wait_timeout, sleep_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;preq_setup_logging&#34;)
def test_preq_logging_pods(self, api_client, logging_addon, wait_timeout, sleep_timeout):
    # logging is an addon instead of built-in since v1.2.0
    if api_client.cluster_version.release &gt;= (1, 2, 0):
        addon = &#34;/&#34;.join([logging_addon.namespace, logging_addon.name])
        code, data = api_client.addons.get(addon)
        assert 200 == code, (code, data)

        if not data.get(&#39;status&#39;, {}).get(&#39;status&#39;) in logging_addon.enable_statuses:
            code, data = api_client.addons.enable(addon)
            assert 200 == code, (code, data)
            assert data.get(&#39;spec&#39;, {}).get(&#39;enabled&#39;, False), (code, data)
            logging_addon.enable_toggled = True

            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                code, data = api_client.addons.get(addon)
                if data.get(&#39;status&#39;, {}).get(&#39;status&#39;) in logging_addon.enable_statuses:
                    break
                sleep(sleep_timeout)
            else:
                raise AssertionError(
                    f&#34;Failed to enable addon {addon} with {wait_timeout} timed out\n&#34;
                    f&#34;API Status({code}): {data}&#34;
                )

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, pods = api_client.get_pods(namespace=logging_addon.namespace)
        assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, &#34;No logging pods found&#34;

        fails = []
        for pod in pods[&#39;data&#39;]:
            name, phase = pod[&#39;metadata&#39;][&#39;name&#39;], pod[&#34;status&#34;][&#34;phase&#34;]
            if phase not in (&#34;Running&#34;, &#34;Succeeded&#34;):
                fails.append((name, phase))
        if not fails:
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            &#34;\n&#34;.join(f&#34;Pod({n})&#39;s phase({p}) is not expected.&#34; for n, p in fails))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_storageclass"><code class="name flex">
<span>def <span class="ident">test_preq_setup_storageclass</span></span>(<span>self, config_storageclass)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;preq_setup_storageclass&#34;)
def test_preq_setup_storageclass(self, config_storageclass):
    &#34;&#34;&#34; Be used to trigger the fixture to setup storageclass&#34;&#34;&#34;</code></pre>
</details>
<div class="desc"><p>Be used to trigger the fixture to setup storageclass</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vmnetwork"><code class="name flex">
<span>def <span class="ident">test_preq_setup_vmnetwork</span></span>(<span>self, vm_network)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;preq_setup_vmnetwork&#34;)
def test_preq_setup_vmnetwork(self, vm_network):
    &#39;&#39;&#39; Be used to trigger the fixture to setup VM network &#39;&#39;&#39;</code></pre>
</details>
<div class="desc"><p>Be used to trigger the fixture to setup VM network</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vms"><code class="name flex">
<span>def <span class="ident">test_preq_setup_vms</span></span>(<span>self,<br>api_client,<br>ssh_keypair,<br>unique_name,<br>vm_checker,<br>vm_shell,<br>vm_network,<br>image,<br>config_storageclass,<br>config_backup_target,<br>wait_timeout,<br>cluster_state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;preq_setup_vms&#34;)
def test_preq_setup_vms(
    self, api_client, ssh_keypair, unique_name, vm_checker, vm_shell, vm_network, image,
    config_storageclass, config_backup_target, wait_timeout, cluster_state
):
    # create new storage class, make it default
    # create 3 VMs:
    # - having the new storage class
    # - the VM that have some data written, take backup
    # - the VM restored from the backup
    pub_key, pri_key = ssh_keypair
    old_sc, new_sc = config_storageclass
    unique_vm_name = f&#34;ug-vm-{unique_name}&#34;

    cpu, mem, size = 1, 2, 10
    vm_spec = api_client.vms.Spec(cpu, mem, mgmt_network=False)
    vm_spec.add_image(&#39;disk-0&#39;, image[&#39;id&#39;], size=size)
    vm_spec.add_network(&#39;nic-1&#39;, f&#34;{vm_network[&#39;namespace&#39;]}/{vm_network[&#39;name&#39;]}&#34;)
    userdata = yaml.safe_load(vm_spec.user_data)
    userdata[&#39;ssh_authorized_keys&#39;] = [pub_key]
    vm_spec.user_data = yaml.dump(userdata)
    # Ref. https://docs.harvesterhci.io/v1.7/vm/backup-restore#create-a-vm-backup
    vm_spec.network_data = yaml.dump({
        &#34;version&#34;: 2,
        &#34;ethernets&#34;: {
            image[&#34;first_nic&#34;]: {
                &#34;dhcp4&#34;: True,
                &#34;dhcp6&#34;: True,
                &#34;dhcp-identifier&#34;: &#34;mac&#34;
            }
        }
    })

    code, data = api_client.vms.create(unique_vm_name, vm_spec)
    assert 201 == code, (code, data)
    vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(unique_vm_name, [&#34;nic-1&#34;])
    assert vm_got_ips, (
        f&#34;Failed to Start VM({unique_vm_name}) with errors:\n&#34;
        f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
        f&#34;API Status({code}): {data}&#34;
    )
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;nic-1&#39;)
    # write data into VM
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        try:
            with vm_shell.login(vm_ip, image[&#39;user&#39;], pkey=pri_key) as sh:
                cloud_inited, (out, err) = vm_checker.wait_cloudinit_done(sh)
                assert cloud_inited and not err, (out, err)
                out, err = sh.exec_command(
                    &#34;dd if=/dev/urandom of=./generate_file bs=1M count=1024; sync&#34;
                )
                assert not out, (out, err)
                vm1_md5, err = sh.exec_command(
                    &#34;md5sum ./generate_file &gt; ./generate_file.md5; cat ./generate_file.md5&#34;
                )
                assert not err, (vm1_md5, err)
                break
        except (SSHException, NoValidConnectionsError, TimeoutError):
            sleep(5)
    else:
        raise AssertionError(&#34;Timed out while writing data into VM&#34;)

    # Take backup then check it&#39;s ready
    code, data = api_client.vms.backup(unique_vm_name, unique_vm_name)
    assert 204 == code, (code, data)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, backup = api_client.backups.get(unique_vm_name)
        if 200 == code and backup.get(&#39;status&#39;, {}).get(&#39;readyToUse&#39;):
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#39;Timed-out waiting for the backup \&#39;{unique_vm_name}\&#39; to be ready.&#39;
        )
    # restore into new VM
    restored_vm_name = f&#34;r-{unique_vm_name}&#34;
    spec = api_client.backups.RestoreSpec.for_new(restored_vm_name)
    code, data = api_client.backups.restore(unique_vm_name, spec)
    assert 201 == code, (code, data)
    # Check restore VM is created
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(restored_vm_name)
        if 200 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;restored VM {restored_vm_name} is not created&#34;
        )
    vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(restored_vm_name, [&#34;nic-1&#34;])
    assert vm_got_ips, (
        f&#34;Failed to Start VM({restored_vm_name}) with errors:\n&#34;
        f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
        f&#34;API Status({code}): {data}&#34;
    )
    # Check data consistency
    r_vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                   if iface[&#39;name&#39;] == &#39;nic-1&#39;)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        try:
            with vm_shell.login(r_vm_ip, image[&#39;user&#39;], pkey=pri_key) as sh:
                cloud_inited, (out, err) = vm_checker.wait_cloudinit_done(sh)
                assert cloud_inited and not err, (out, err)
                out, err = sh.exec_command(&#34;md5sum -c ./generate_file.md5&#34;)
                assert not err, (out, err)
                vm2_md5, err = sh.exec_command(&#34;cat ./generate_file.md5&#34;)
                assert not err, (vm2_md5, err)
                assert vm1_md5 == vm2_md5
                out, err = sh.exec_command(
                    f&#34;ping -c1 {vm_ip} &gt; /dev/null &amp;&amp; echo -n success || echo -n fail&#34;
                )
                assert &#34;success&#34; == out and not err
                break
        except (SSHException, NoValidConnectionsError, ConnectionResetError, TimeoutError):
            sleep(5)
    else:
        raise AssertionError(&#34;Unable to login to restored VM to check data consistency&#34;)

    # Create VM having additional volume with new storage class
    vm_spec.add_volume(&#34;vol-1&#34;, 5, storage_cls=new_sc[&#39;metadata&#39;][&#39;name&#39;])
    code, data = api_client.vms.create(f&#34;sc-{unique_vm_name}&#34;, vm_spec)
    assert 201 == code, (code, data)
    vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(f&#34;sc-{unique_vm_name}&#34;, [&#34;nic-1&#34;])
    assert vm_got_ips, (
        f&#34;Failed to Start VM(sc-{unique_vm_name}) with errors:\n&#34;
        f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
        f&#34;API Status({code}): {data}&#34;
    )

    # store into cluster&#39;s state
    names = [unique_vm_name, f&#34;r-{unique_vm_name}&#34;, f&#34;sc-{unique_vm_name}&#34;]
    cluster_state.vms = dict(md5=vm1_md5, names=names, ssh_user=image[&#39;user&#39;], pkey=pri_key)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_image_deleted"><code class="name flex">
<span>def <span class="ident">test_upgrade_image_deleted</span></span>(<span>self, api_client, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
def test_upgrade_image_deleted(self, api_client, wait_timeout):
    # max to wait 300s for the upgrade related volumes to be deleted
    endtime = datetime.now() + timedelta(seconds=min(wait_timeout / 5, 300))
    while endtime &gt; datetime.now():
        code, data = api_client.images.get(namespace=&#39;harvester-system&#39;)
        upgrade_images = [image for image in data[&#39;items&#39;]
                          if &#39;upgrade&#39; in image[&#39;spec&#39;][&#39;displayName&#39;]]
        if not upgrade_images:
            break
    else:
        raise AssertionError(f&#34;Upgrade related image(s) still available:\n{upgrade_images}&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_vm_deleted"><code class="name flex">
<span>def <span class="ident">test_upgrade_vm_deleted</span></span>(<span>self, api_client, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
def test_upgrade_vm_deleted(self, api_client, wait_timeout):
    # max to wait 300s for the upgrade related VMs to be deleted
    endtime = datetime.now() + timedelta(seconds=min(wait_timeout / 5, 300))
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(namespace=&#39;harvester-system&#39;)
        upgrade_vms = [vm for vm in data[&#39;data&#39;] if &#39;upgrade&#39; in vm[&#39;id&#39;]]
        if not upgrade_vms:
            break
    else:
        raise AssertionError(f&#34;Upgrade related VM still available:\n{upgrade_vms}&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_volume_deleted"><code class="name flex">
<span>def <span class="ident">test_upgrade_volume_deleted</span></span>(<span>self, api_client, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
def test_upgrade_volume_deleted(self, api_client, wait_timeout):
    # max to wait 300s for the upgrade related volumes to be deleted
    endtime = datetime.now() + timedelta(seconds=min(wait_timeout / 5, 300))
    while endtime &gt; datetime.now():
        code, data = api_client.volumes.get(namespace=&#39;harvester-system&#39;)
        upgrade_vols = [vol for vol in data[&#39;data&#39;]
                        if &#39;upgrade&#39; in vol[&#39;id&#39;] and &#39;log-archive&#39; not in vol[&#39;id&#39;]]
        if not upgrade_vols:
            break
    else:
        raise AssertionError(f&#34;Upgrade related volume(s) still available:\n{upgrade_vols}&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_audit_log"><code class="name flex">
<span>def <span class="ident">test_verify_audit_log</span></span>(<span>self, api_client, host_shell, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
def test_verify_audit_log(self, api_client, host_shell, wait_timeout):
    code, data = api_client.hosts.get()
    assert 200 == code, (code, data)
    label_main = &#34;node-role.kubernetes.io/control-plane&#34;
    masters = [n for n in data[&#39;data&#39;] if n[&#39;metadata&#39;][&#39;labels&#39;].get(label_main) == &#34;true&#34;]
    assert len(masters) &gt; 0, &#34;No master nodes found&#34;

    script = (&#34;sudo tail /var/lib/rancher/rke2/server/logs/audit.log | awk &#39;END{print}&#39; &#34;
              &#34;| jq .requestReceivedTimestamp &#34;
              &#34;| xargs -I {} date -d \&#34;{}\&#34; +%s&#34;)

    node_ips = [n[&#34;metadata&#34;][&#34;annotations&#34;][NODE_INTERNAL_IP_ANNOTATION] for n in masters]
    cmp = dict()
    done = set()
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        for ip in done.symmetric_difference(node_ips):
            try:
                with host_shell.login(ip) as shell:
                    out, err = shell.exec_command(script)
                    timestamp = int(out)
                    if not err and ip not in cmp:
                        cmp[ip] = timestamp
                        continue
                    if not err and cmp[ip] &lt; timestamp:
                        done.add(ip)
            except (SSHException, NoValidConnectionsError, ConnectionResetError, TimeoutError):
                continue

        if not done.symmetric_difference(node_ips):
            break
        sleep(5)
    else:
        raise AssertionError(
            &#34;\n&#34;.join(&#34;Node {ip} audit log is not updated.&#34; for ip in set(node_ips) ^ done)
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_crds_existed"><code class="name flex">
<span>def <span class="ident">test_verify_crds_existed</span></span>(<span>self, api_client, harvester_crds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
def test_verify_crds_existed(self, api_client, harvester_crds):
    &#34;&#34;&#34; Verify crds existed
    Criteria:
    - crds should be existed
    &#34;&#34;&#34;
    not_existed_crds = []
    exist_crds = True
    for crd in harvester_crds:
        code, _ = api_client.get_crds(name=crd)

        if code != 200:
            exist_crds = False
            not_existed_crds.append(crd)

    if not exist_crds:
        raise AssertionError(f&#34;CRDs {not_existed_crds} are not existed&#34;)</code></pre>
</details>
<div class="desc"><p>Verify crds existed
Criteria:
- crds should be existed</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_deployed_components_version"><code class="name flex">
<span>def <span class="ident">test_verify_deployed_components_version</span></span>(<span>self, api_client)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
def test_verify_deployed_components_version(self, api_client):
    &#34;&#34;&#34; Verify deployed kubevirt and longhorn version
    Criteria:
    - except version(get from apps.catalog.cattle.io/harvester) should be equal to the version
      of kubevirt and longhorn
    &#34;&#34;&#34;

    def check_image_version(old, new):
        def sanitized_ver(img: str):
            try:
                # PEP 440 compliant: N(.N)*[-+]?[{a|alpha|b|beta|c|rc}N][.postN][.devN]
                ver_str = img.split(&#39;:&#39;, 1)[-1]
                ver_obj = parse_version(ver_str)
                return ver_obj
            except InvalidVersion as e:
                if &#34;-&#34; in ver_str:
                    # Conform to PEP 440 by replacing the first &#39;-&#39; with &#39;+&#39;
                    # e.g. 1.2.3-4.5.6 -&gt; 1.2.3+4.5.6, v1.10.1-hotfix-2 -&gt; v1.10.1+hotfix
                    ver_obj = parse_version(&#34;+&#34;.join(ver_str.split(&#34;-&#34;)[:2]))
                    logger.warning(f&#34;Convert {img} to {ver_obj} for comparison&#34;)
                    return ver_obj
                raise e

        old_le_new = (sanitized_ver(old) &lt;= sanitized_ver(new))
        if not old_le_new:
            logger.error(f&#34;Old image {old} does not less or equal to the new {new}&#34;)

        return old_le_new

    kubevirt_version_existed = False
    engine_image_version_existed = False
    longhorn_manager_version_existed = False

    # Get expected image of kubevirt
    code, app = api_client.get_apps_deployments(name=&#34;virt-operator&#34;,
                                                namespace=DEFAULT_HARVESTER_NAMESPACE)
    assert code == 200, (code, app)
    kubevirt_operator_image = app[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;]

    # Get except image of longhorn
    code, apps = api_client.get_apps_controllerrevisions(namespace=DEFAULT_LONGHORN_NAMESPACE)
    assert code == 200, (code, apps)

    longhorn_images = {
        &#34;engine-image&#34;: &#34;&#34;,
        &#34;longhorn-manager&#34;: &#34;&#34;
    }
    for lh_app in longhorn_images:
        for app in apps[&#39;data&#39;]:
            if app[&#34;id&#34;].startswith(f&#34;{DEFAULT_LONGHORN_NAMESPACE}/{lh_app}&#34;):
                longhorn_images[lh_app] = (
                    app[&#34;data&#34;][&#34;spec&#34;][&#34;template&#34;][&#34;spec&#34;][&#34;containers&#34;][0][&#34;image&#34;])
                break

    # Verify kubevirt version
    code, pods = api_client.get_pods(namespace=DEFAULT_HARVESTER_NAMESPACE)
    assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, (
        f&#34;Failed to get pods in namespace {DEFAULT_HARVESTER_NAMESPACE}&#34;)

    for pod in pods[&#39;data&#39;]:
        if &#34;virt-operator&#34; in pod[&#39;metadata&#39;][&#39;name&#39;]:
            kubevirt_version_existed = check_image_version(
                kubevirt_operator_image, pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;]
            )

    # Verify longhorn version
    code, pods = api_client.get_pods(namespace=DEFAULT_LONGHORN_NAMESPACE)
    assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, (
        f&#34;Failed to get pods in namespace {DEFAULT_LONGHORN_NAMESPACE}&#34;)

    for pod in pods[&#39;data&#39;]:
        if &#34;longhorn-manager&#34; in pod[&#39;metadata&#39;][&#39;name&#39;]:
            longhorn_manager_version_existed = check_image_version(
              longhorn_images[&#34;longhorn-manager&#34;], pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;]
            )
        elif &#34;engine-image&#34; in pod[&#39;metadata&#39;][&#39;name&#39;]:
            engine_image_version_existed = check_image_version(
                longhorn_images[&#34;engine-image&#34;], pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;image&#39;]
            )

    assert kubevirt_version_existed, &#34;kubevirt version is not correct&#34;
    assert engine_image_version_existed, &#34;longhorn engine image version is not correct&#34;
    assert longhorn_manager_version_existed, &#34;longhorn manager version is not correct&#34;</code></pre>
</details>
<div class="desc"><p>Verify deployed kubevirt and longhorn version
Criteria:
- except version(get from apps.catalog.cattle.io/harvester) should be equal to the version
of kubevirt and longhorn</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_logging_pods"><code class="name flex">
<span>def <span class="ident">test_verify_logging_pods</span></span>(<span>self, api_client, logging_addon)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_logging&#34;])
def test_verify_logging_pods(self, api_client, logging_addon):
    &#34;&#34;&#34; Verify logging pods and logs
    Criteria: https://github.com/harvester/tests/issues/535
    &#34;&#34;&#34;
    # logging is an addon instead of built-in since v1.2.0
    if api_client.cluster_version.release &gt;= (1, 2, 0):
        addon = &#34;/&#34;.join([logging_addon.namespace, logging_addon.name])
        code, data = api_client.addons.get(addon)
        assert data.get(&#39;status&#39;, {}).get(&#39;status&#39;) in logging_addon.enable_statuses

    code, pods = api_client.get_pods(namespace=logging_addon.namespace)
    assert code == 200 and len(pods[&#39;data&#39;]) &gt; 0, &#34;No logging pods found&#34;

    fails = []
    for pod in pods[&#39;data&#39;]:
        phase = pod[&#34;status&#34;][&#34;phase&#34;]
        if phase not in (&#34;Running&#34;, &#34;Succeeded&#34;):
            fails.append((pod[&#39;metadata&#39;][&#39;name&#39;], phase))
    else:
        assert not fails, (
            &#34;\n&#34;.join(f&#34;Pod({n})&#39;s phase({p}) is not expected.&#34; for n, p in fails)
        )

    # teardown
    if logging_addon.enable_toggled:
        api_client.addons.disable(addon)</code></pre>
</details>
<div class="desc"><p>Verify logging pods and logs
Criteria: <a href="https://github.com/harvester/tests/issues/535">https://github.com/harvester/tests/issues/535</a></p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_network"><code class="name flex">
<span>def <span class="ident">test_verify_network</span></span>(<span>self, api_client, cluster_state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_vmnetwork&#34;])
def test_verify_network(self, api_client, cluster_state):
    &#34;&#34;&#34; Verify cluster and VLAN networks
    - cluster network `mgmt` should exists
    - Created VLAN should exists
    &#34;&#34;&#34;

    code, cnets = api_client.clusternetworks.get()
    assert code == 200, (
        &#34;Failed to get Networks: %d, %s&#34; % (code, cnets))

    assert len(cnets[&#34;items&#34;]) &gt; 0, (&#34;No Networks found&#34;)

    assert any(n[&#39;metadata&#39;][&#39;name&#39;] == &#34;mgmt&#34; for n in cnets[&#39;items&#39;]), (
        &#34;Cluster network mgmt not found&#34;)

    code, vnets = api_client.networks.get()
    assert code == 200, (f&#34;Failed to get VLANs: {code}, {vnets}&#34; % (code, vnets))
    assert len(vnets[&#34;items&#34;]) &gt; 0, (&#34;No VLANs found&#34;)

    used_vlan = cluster_state.network[&#39;metadata&#39;][&#39;name&#39;]
    assert any(used_vlan == n[&#39;metadata&#39;][&#39;name&#39;] for n in vnets[&#39;items&#39;]), (
        f&#34;VLAN {used_vlan} not found&#34;)</code></pre>
</details>
<div class="desc"><p>Verify cluster and VLAN networks
- cluster network <code>mgmt</code> should exists
- Created VLAN should exists</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_os_version"><code class="name flex">
<span>def <span class="ident">test_verify_os_version</span></span>(<span>self, request, api_client, cluster_state, host_shell)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
def test_verify_os_version(self, request, api_client, cluster_state, host_shell):
    # Verify /etc/os-release on all nodes
    script = &#34;cat /etc/os-release&#34;
    if not cluster_state.version_verify:
        pytest.skip(&#34;skip verify os version&#34;)

    # Get all nodes
    code, data = api_client.hosts.get()
    assert 200 == code, (code, data)
    for node in data[&#39;data&#39;]:
        node_ip = node[&#34;metadata&#34;][&#34;annotations&#34;][NODE_INTERNAL_IP_ANNOTATION]

        with host_shell.login(node_ip) as sh:
            lines, stderr = sh.exec_command(script, get_pty=True, splitlines=True)
            assert not stderr, (
                f&#34;Failed to execute {script} on {node_ip}: {stderr}&#34;)

            # eg: PRETTY_NAME=&#34;Harvester v1.1.0&#34;
            assert cluster_state.version == re.findall(r&#34;Harvester (.+?)\&#34;&#34;, lines[3])[0], (
                &#34;OS version is not correct&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_restore_vm"><code class="name flex">
<span>def <span class="ident">test_verify_restore_vm</span></span>(<span>self, api_client, cluster_state, vm_shell, vm_checker, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_vms&#34;])
def test_verify_restore_vm(
    self, api_client, cluster_state, vm_shell, vm_checker, wait_timeout
):
    &#34;&#34;&#34; Verify VM restored from the backup
    Criteria:
    - VM should able to start
    - data in VM should not lost
    &#34;&#34;&#34;

    backup_name = cluster_state.vms[&#39;names&#39;][0]
    restored_vm_name = f&#34;new-r-{backup_name}&#34;

    # Restore VM from backup and check networking is good
    restore_spec = api_client.backups.RestoreSpec.for_new(restored_vm_name)
    code, data = api_client.backups.restore(backup_name, restore_spec)
    assert code == 201, &#34;Unable to restore backup {backup_name} after upgrade&#34;
    # Check restore VM is created
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.vms.get(restored_vm_name)
        if 200 == code:
            break
        sleep(3)
    else:
        raise AssertionError(
            f&#34;restored VM {restored_vm_name} is not created&#34;
        )
    vm_got_ips, (code, data) = vm_checker.wait_ip_addresses(restored_vm_name, [&#34;nic-1&#34;])
    assert vm_got_ips, (
        f&#34;Failed to Start VM({restored_vm_name}) with errors:\n&#34;
        f&#34;Status: {data.get(&#39;status&#39;)}\n&#34;
        f&#34;API Status({code}): {data}&#34;
    )

    # Check data in restored VM is consistent
    pri_key, ssh_user = cluster_state.vms[&#39;pkey&#39;], cluster_state.vms[&#39;ssh_user&#39;]
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;nic-1&#39;)
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        try:
            with vm_shell.login(vm_ip, ssh_user, pkey=pri_key) as sh:
                cloud_inited, (out, err) = vm_checker.wait_cloudinit_done(sh)
                assert cloud_inited and not err, (out, err)
                out, err = sh.exec_command(&#34;md5sum -c ./generate_file.md5&#34;)
                assert not err, (out, err)
                md5, err = sh.exec_command(&#34;cat ./generate_file.md5&#34;)
                assert not err, (md5, err)
                assert md5 == cluster_state.vms[&#39;md5&#39;]
                break
        except (SSHException, NoValidConnectionsError, ConnectionResetError, TimeoutError):
            sleep(5)
    else:
        raise AssertionError(&#34;Unable to login to restored VM to check data consistency&#34;)

    # teardown: remove the VM
    code, data = api_client.vms.get(restored_vm_name)
    spec = api_client.vms.Spec.from_dict(data)
    _ = vm_checker.wait_deleted(restored_vm_name)
    for vol in spec.volumes:
        vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
        api_client.volumes.delete(vol_name)</code></pre>
</details>
<div class="desc"><p>Verify VM restored from the backup
Criteria:
- VM should able to start
- data in VM should not lost</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_rke2_version"><code class="name flex">
<span>def <span class="ident">test_verify_rke2_version</span></span>(<span>self, api_client, host_shell)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
def test_verify_rke2_version(self, api_client, host_shell):
    # Verify node version on all nodes
    script = &#34;cat /etc/harvester-release.yaml&#34;

    label_main = &#34;node-role.kubernetes.io/control-plane&#34;
    code, data = api_client.hosts.get()
    assert 200 == code, (code, data)
    masters = [n for n in data[&#39;data&#39;] if n[&#39;metadata&#39;][&#39;labels&#39;].get(label_main) == &#34;true&#34;]

    # Verify rke2 version
    except_rke2_version = &#34;&#34;
    for node in masters:
        node_ip = node[&#34;metadata&#34;][&#34;annotations&#34;][NODE_INTERNAL_IP_ANNOTATION]

        # Get except rke2 version
        if except_rke2_version == &#34;&#34;:
            with host_shell.login(node_ip) as sh:
                lines, stderr = sh.exec_command(script, get_pty=True, splitlines=True)
                assert not stderr, (
                    f&#34;Failed to execute {script} on {node_ip}: {stderr}&#34;)

                for line in lines:
                    if &#34;kubernetes&#34; in line:
                        except_rke2_version = re.findall(r&#34;kubernetes: (.*)&#34;, line.strip())[0]
                        break

                assert except_rke2_version != &#34;&#34;, (&#34;Failed to get except rke2 version&#34;)

        assert node.get(&#39;status&#39;, {}).get(&#39;nodeInfo&#39;, {}).get(
               &#34;kubeletVersion&#34;, &#34;&#34;) == except_rke2_version, (
               &#34;rke2 version is not correct&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_storage_class"><code class="name flex">
<span>def <span class="ident">test_verify_storage_class</span></span>(<span>self, api_client, cluster_state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_storageclass&#34;])
def test_verify_storage_class(self, api_client, cluster_state):
    &#34;&#34;&#34; Verify StorageClasses and defaults
    - `new_sc` should be settle as default
    - `longhorn` should exists
    &#34;&#34;&#34;
    code, scs = api_client.scs.get()
    assert code == 200, (&#34;Failed to get StorageClasses: %d, %s&#34; % (code, scs))
    assert len(scs[&#34;items&#34;]) &gt; 0, (&#34;No StorageClasses found&#34;)

    created_sc = cluster_state.scs[-1][&#39;metadata&#39;][&#39;name&#39;]
    names = {sc[&#39;metadata&#39;][&#39;name&#39;]: sc[&#39;metadata&#39;].get(&#39;annotations&#39;) for sc in scs[&#39;items&#39;]}
    assert &#34;longhorn&#34; in names
    assert created_sc in names
    assert &#34;storageclass.kubernetes.io/is-default-class&#34; in names[created_sc]
    assert &#34;true&#34; == names[created_sc][&#34;storageclass.kubernetes.io/is-default-class&#34;]</code></pre>
</details>
<div class="desc"><p>Verify StorageClasses and defaults
- <code>new_sc</code> should be settle as default
- <code>longhorn</code> should exists</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_upgradelog"><code class="name flex">
<span>def <span class="ident">test_verify_upgradelog</span></span>(<span>self, api_client)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;])
def test_verify_upgradelog(self, api_client):
    &#34;&#34;&#34; Verify upgradelog pod and volume existed when upgrade with &#34;Enable Logging&#34;
    &#34;&#34;&#34;
    # pod
    code, data = api_client.get_pods(namespace=&#39;harvester-system&#39;)
    assert code == 200 and data[&#39;data&#39;], (code, data)

    upgradelog_pods = [pod for pod in data[&#39;data&#39;] if &#39;upgradelog&#39; in pod[&#39;id&#39;]]
    assert upgradelog_pods, f&#34;No upgradelog pod found:\n{data[&#39;data&#39;]}&#34;
    for pod in upgradelog_pods:
        assert pod[&#34;status&#34;][&#34;phase&#34;] == &#34;Running&#34;, (code, upgradelog_pods)

    # volume
    code, data = api_client.volumes.get(namespace=&#39;harvester-system&#39;)
    assert code == 200 and data[&#39;data&#39;], (code, data)

    upgradelog_vols = [vol for vol in data[&#39;data&#39;] if &#39;upgradelog&#39; in vol[&#39;id&#39;]]
    assert upgradelog_vols, f&#34;No upgradelog volume found:\n{data[&#39;data&#39;]}&#34;
    for vol in upgradelog_vols:
        assert not vol[&#34;metadata&#34;][&#39;state&#39;][&#39;error&#39;], (code, upgradelog_vols)
        assert not vol[&#34;metadata&#34;][&#39;state&#39;][&#39;transitioning&#39;], (code, upgradelog_vols)
        assert vol[&#39;status&#39;][&#39;phase&#39;] == &#34;Bound&#34;, (code, upgradelog_vols)</code></pre>
</details>
<div class="desc"><p>Verify upgradelog pod and volume existed when upgrade with "Enable Logging"</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_vms"><code class="name flex">
<span>def <span class="ident">test_verify_vms</span></span>(<span>self, api_client, cluster_state, vm_shell, vm_checker, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;any_nodes_upgrade&#34;, &#34;preq_setup_vms&#34;])
def test_verify_vms(self, api_client, cluster_state, vm_shell, vm_checker, wait_timeout):
    &#34;&#34;&#34; Verify VMs&#39; state and data
    Criteria:
    - VMs should keep in running state
    - data in VMs should not lost
    &#34;&#34;&#34;

    code, vmis = api_client.vms.get_status()
    assert code == 200 and len(vmis[&#39;data&#39;]), (code, vmis)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        fails, ips = list(), dict()
        for name in cluster_state.vms[&#39;names&#39;]:
            code, data = api_client.vms.get_status(name)
            try:
                assert 200 == code
                assert &#34;Running&#34; == data[&#39;status&#39;][&#39;phase&#39;]
                assert data[&#39;status&#39;][&#39;nodeName&#39;]
                ips[name] = next(iface[&#39;ipAddress&#39;] for iface in data[&#39;status&#39;][&#39;interfaces&#39;]
                                 if iface[&#39;name&#39;] == &#39;nic-1&#39;)
            except (AssertionError, TypeError, StopIteration, KeyError) as ex:
                fails.append((name, (ex, code, data)))
        if not fails:
            break
    else:
        raise AssertionError(&#34;\n&#34;.join(
            f&#34;VM {name} is not in expected state.\nException: {ex}\nAPI Status({code}): {data}&#34;
            for (name, (ex, code, data)) in fails)
        )

    pri_key, ssh_user = cluster_state.vms[&#39;pkey&#39;], cluster_state.vms[&#39;ssh_user&#39;]
    for name in cluster_state.vms[&#39;names&#39;][:-1]:
        vm_ip = ips[name]
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            try:
                with vm_shell.login(vm_ip, ssh_user, pkey=pri_key) as sh:
                    out, err = sh.exec_command(&#34;md5sum -c ./generate_file.md5&#34;)
                    assert not err, (out, err)
                    md5, err = sh.exec_command(&#34;cat ./generate_file.md5&#34;)
                    assert not err, (md5, err)
                    assert md5 == cluster_state.vms[&#39;md5&#39;]
                    break
            except (SSHException, NoValidConnectionsError, ConnectionResetError, TimeoutError):
                sleep(5)
        else:
            fails.append(f&#34;Data in VM({name}, {vm_ip}) is inconsistent.&#34;)

    assert not fails, &#34;\n&#34;.join(fails)

    # Teardown: remove all VMs
    for name in cluster_state.vms[&#39;names&#39;]:
        code, data = api_client.vms.get(name)
        spec = api_client.vms.Spec.from_dict(data)
        _ = vm_checker.wait_deleted(name)
        for vol in spec.volumes:
            vol_name = vol[&#39;volume&#39;][&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
            api_client.volumes.delete(vol_name)</code></pre>
</details>
<div class="desc"><p>Verify VMs' state and data
Criteria:
- VMs should keep in running state
- data in VMs should not lost</p></div>
</dd>
</dl>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade"><code class="flex name class">
<span>class <span class="ident">TestInvalidUpgrade</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.upgrade
@pytest.mark.negative
@pytest.mark.any_nodes
class TestInvalidUpgrade:
    @pytest.mark.skip_if_version(
            &#34;&lt; v1.5.0&#34;,
            reason=&#34;https://github.com/harvester/harvester/issues/7654 fix after `v1.5.0`&#34;)
    def test_iso_url(self, api_client, unique_name, upgrade_checker):
        &#34;&#34;&#34;
        Steps:
        1. Create an invalid manifest.
        2. Try to upgrade with the invalid manifest.
        3. Upgrade should not start and fail.
        &#34;&#34;&#34;
        version, url = unique_name, &#34;https://invalid_iso_url&#34;
        checksum = sha512(b&#39;not_a_valid_checksum&#39;).hexdigest()

        code, data = api_client.versions.get(version)
        if code != 200:
            code, data = api_client.versions.create(version, url, checksum)
            assert code == 201, f&#34;Failed to create invalid version: {data}&#34;
            version_created, (code, data) = upgrade_checker.wait_version_created(version)
            assert version_created, (code, data)

        code, data = api_client.upgrades.create(version)
        assert code == 201, f&#34;Failed to create invalid upgrade: {data}&#34;
        upgrade_name = data[&#39;metadata&#39;][&#39;name&#39;]

        upgrade_fail_by_invalid_iso_url, (code, data) = \
            upgrade_checker.wait_upgrade_fail_by_invalid_iso_url(upgrade_name)
        assert upgrade_fail_by_invalid_iso_url, (code, data)

        # teardown
        api_client.upgrades.delete(upgrade_name)
        api_client.versions.delete(version)

    @pytest.mark.skip_if_version(
            &#34;&lt; v1.5.0&#34;,
            reason=&#34;https://github.com/harvester/harvester/issues/7654 fix after `v1.5.0`&#34;)
    @pytest.mark.parametrize(&#34;resort&#34;, [
        pytest.param(
            slice(None, None, -1),
            marks=pytest.mark.skip(reason=&#34;https://github.com/harvester/harvester/issues/9990&#34;)),
        slice(None, None, 2)
        ], ids=(&#34;mismatched&#34;, &#34;invalid&#34;))
    def test_checksum(self, api_client, unique_name, upgrade_target, resort, upgrade_checker):
        version, url, checksum = upgrade_target
        version = f&#34;{version}-{unique_name}&#34;

        if resort.step == 2:
            # ref: https://github.com/harvester/harvester/issues/5480
            code, data = api_client.versions.create(version, url, checksum[resort])
            try:
                assert 400 == code, (code, data)
            finally:
                return api_client.versions.delete(version)

        code, data = api_client.versions.create(version, url, checksum[resort])
        assert 201 == code, f&#34;Failed to create upgrade for {version}&#34;
        version_created, (code, data) = upgrade_checker.wait_version_created(version)
        assert version_created, (code, data)

        code, data = api_client.upgrades.create(version)
        assert 201 == code, f&#34;Failed to start upgrade for {version}&#34;
        upgrade_name = data[&#39;metadata&#39;][&#39;name&#39;]

        upgrade_fail_by_invalid_checksum, (code, data) = \
            upgrade_checker.wait_upgrade_fail_by_invalid_checksum(upgrade_name)
        assert upgrade_fail_by_invalid_checksum, (code, data)

        # teardown
        api_client.upgrades.delete(upgrade_name)
        api_client.versions.delete(version)

    @pytest.mark.skip(reason=&#34;https://github.com/harvester/harvester/issues/5494&#34;)
    def test_version_compatibility(
        self, api_client, unique_name, upgrade_target, upgrade_timeout
    ):
        version, url, checksum = upgrade_target
        version = f&#34;{version}-{unique_name}&#34;

        code, data = api_client.versions.create(version, url, checksum)
        assert 201 == code, f&#34;Failed to create upgrade for {version}&#34;
        code, data = api_client.upgrades.create(version)
        assert 201 == code, f&#34;Failed to start upgrade for {version}&#34;

        endtime = datetime.now() + timedelta(seconds=upgrade_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.upgrades.get(data[&#39;metadata&#39;][&#39;name&#39;])
            conds = dict((c[&#39;type&#39;], c) for c in data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, []))
            verified = []  # TODO
            if all(verified):
                break
        else:
            raise AssertionError(f&#34;Upgrade NOT failed in expected conditions: {conds}&#34;)

        # teardown
        api_client.upgrades.delete(data[&#39;metadata&#39;][&#39;name&#39;])
        api_client.versions.delete(version)

    def test_degraded_volume(
        self, api_client, unique_name, vm_shell_from_host, upgrade_target, stopped_vm,
        vm_checker, volume_checker, upgrade_checker
    ):
        &#34;&#34;&#34;
        Criteria: create upgrade should fails if there are any degraded volumes
        Steps:
        1. Create a VM using a volume with 3 replicas.
        2. Delete one replica of the volume. Let the volume stay in
           degraded state.
        3. Immediately upgrade Harvester.
        4. Upgrade should fail.
        &#34;&#34;&#34;
        # https://github.com/harvester/harvester/issues/6425
        code, data = api_client.hosts.get()
        assert 200 == code, (code, data)
        if (cluster_size := len(data[&#39;data&#39;])) &lt; 3:
            pytest.skip(
                f&#34;Degraded volumes only checked on 3+ nodes cluster, skip on {cluster_size}.&#34;
            )

        vm_name, ssh_user, pri_key = stopped_vm
        vm_started, (code, vmi) = vm_checker.wait_started(vm_name)
        assert vm_started, (code, vmi)

        # Write date into VM
        vm_ip = next(iface[&#39;ipAddress&#39;] for iface in vmi[&#39;status&#39;][&#39;interfaces&#39;]
                     if iface[&#39;name&#39;] == &#39;default&#39;)
        code, data = api_client.hosts.get(vmi[&#39;status&#39;][&#39;nodeName&#39;])
        host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                       if addr[&#39;type&#39;] == &#39;InternalIP&#39;)
        with vm_shell_from_host(host_ip, vm_ip, ssh_user, pkey=pri_key) as sh:
            stdout, stderr = sh.exec_command(
                &#34;dd if=/dev/urandom of=./generate_file bs=1M count=1024; sync&#34;
            )
            assert not stdout, (stdout, stderr)

        # Get pv name of the volume
        claim_name = vmi[&#34;spec&#34;][&#34;volumes&#34;][0][&#34;persistentVolumeClaim&#34;][&#34;claimName&#34;]
        code, data = api_client.volumes.get(name=claim_name)
        assert code == 200, f&#34;Failed to get volume {claim_name}: {data}&#34;
        pv_name = data[&#34;spec&#34;][&#34;volumeName&#34;]

        # Make the volume becomes degraded
        code, data = api_client.lhreplicas.get()
        assert code == 200 and data[&#39;items&#39;], f&#34;Failed to get longhorn replicas ({code}): {data}&#34;
        replica = next(r for r in data[&#34;items&#34;] if pv_name == r[&#39;spec&#39;][&#39;volumeName&#39;])
        api_client.lhreplicas.delete(name=replica[&#39;metadata&#39;][&#39;name&#39;])
        lhvolume_degraded = volume_checker.wait_lhvolume_degraded(pv_name)
        assert lhvolume_degraded, (code, data)

        # create upgrade and verify it is not allowed
        version, url, checksum = upgrade_target
        version = f&#34;{version}-{unique_name}&#34;
        code, data = api_client.versions.create(version, url, checksum)
        assert code == 201, f&#34;Failed to create version {version}: {data}&#34;
        version_created, (code, data) = upgrade_checker.wait_version_created(version)
        assert version_created, (code, data)
        code, data = api_client.upgrades.create(version)
        assert code == 400, f&#34;Failed to verify degraded volume: {code}, {data}&#34;

        # Teardown invalid upgrade
        api_client.versions.delete(version)</code></pre>
</details>
<div class="desc"></div>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_checksum"><code class="name flex">
<span>def <span class="ident">test_checksum</span></span>(<span>self, api_client, unique_name, upgrade_target, resort, upgrade_checker)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip_if_version(
        &#34;&lt; v1.5.0&#34;,
        reason=&#34;https://github.com/harvester/harvester/issues/7654 fix after `v1.5.0`&#34;)
@pytest.mark.parametrize(&#34;resort&#34;, [
    pytest.param(
        slice(None, None, -1),
        marks=pytest.mark.skip(reason=&#34;https://github.com/harvester/harvester/issues/9990&#34;)),
    slice(None, None, 2)
    ], ids=(&#34;mismatched&#34;, &#34;invalid&#34;))
def test_checksum(self, api_client, unique_name, upgrade_target, resort, upgrade_checker):
    version, url, checksum = upgrade_target
    version = f&#34;{version}-{unique_name}&#34;

    if resort.step == 2:
        # ref: https://github.com/harvester/harvester/issues/5480
        code, data = api_client.versions.create(version, url, checksum[resort])
        try:
            assert 400 == code, (code, data)
        finally:
            return api_client.versions.delete(version)

    code, data = api_client.versions.create(version, url, checksum[resort])
    assert 201 == code, f&#34;Failed to create upgrade for {version}&#34;
    version_created, (code, data) = upgrade_checker.wait_version_created(version)
    assert version_created, (code, data)

    code, data = api_client.upgrades.create(version)
    assert 201 == code, f&#34;Failed to start upgrade for {version}&#34;
    upgrade_name = data[&#39;metadata&#39;][&#39;name&#39;]

    upgrade_fail_by_invalid_checksum, (code, data) = \
        upgrade_checker.wait_upgrade_fail_by_invalid_checksum(upgrade_name)
    assert upgrade_fail_by_invalid_checksum, (code, data)

    # teardown
    api_client.upgrades.delete(upgrade_name)
    api_client.versions.delete(version)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_degraded_volume"><code class="name flex">
<span>def <span class="ident">test_degraded_volume</span></span>(<span>self,<br>api_client,<br>unique_name,<br>vm_shell_from_host,<br>upgrade_target,<br>stopped_vm,<br>vm_checker,<br>volume_checker,<br>upgrade_checker)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_degraded_volume(
    self, api_client, unique_name, vm_shell_from_host, upgrade_target, stopped_vm,
    vm_checker, volume_checker, upgrade_checker
):
    &#34;&#34;&#34;
    Criteria: create upgrade should fails if there are any degraded volumes
    Steps:
    1. Create a VM using a volume with 3 replicas.
    2. Delete one replica of the volume. Let the volume stay in
       degraded state.
    3. Immediately upgrade Harvester.
    4. Upgrade should fail.
    &#34;&#34;&#34;
    # https://github.com/harvester/harvester/issues/6425
    code, data = api_client.hosts.get()
    assert 200 == code, (code, data)
    if (cluster_size := len(data[&#39;data&#39;])) &lt; 3:
        pytest.skip(
            f&#34;Degraded volumes only checked on 3+ nodes cluster, skip on {cluster_size}.&#34;
        )

    vm_name, ssh_user, pri_key = stopped_vm
    vm_started, (code, vmi) = vm_checker.wait_started(vm_name)
    assert vm_started, (code, vmi)

    # Write date into VM
    vm_ip = next(iface[&#39;ipAddress&#39;] for iface in vmi[&#39;status&#39;][&#39;interfaces&#39;]
                 if iface[&#39;name&#39;] == &#39;default&#39;)
    code, data = api_client.hosts.get(vmi[&#39;status&#39;][&#39;nodeName&#39;])
    host_ip = next(addr[&#39;address&#39;] for addr in data[&#39;status&#39;][&#39;addresses&#39;]
                   if addr[&#39;type&#39;] == &#39;InternalIP&#39;)
    with vm_shell_from_host(host_ip, vm_ip, ssh_user, pkey=pri_key) as sh:
        stdout, stderr = sh.exec_command(
            &#34;dd if=/dev/urandom of=./generate_file bs=1M count=1024; sync&#34;
        )
        assert not stdout, (stdout, stderr)

    # Get pv name of the volume
    claim_name = vmi[&#34;spec&#34;][&#34;volumes&#34;][0][&#34;persistentVolumeClaim&#34;][&#34;claimName&#34;]
    code, data = api_client.volumes.get(name=claim_name)
    assert code == 200, f&#34;Failed to get volume {claim_name}: {data}&#34;
    pv_name = data[&#34;spec&#34;][&#34;volumeName&#34;]

    # Make the volume becomes degraded
    code, data = api_client.lhreplicas.get()
    assert code == 200 and data[&#39;items&#39;], f&#34;Failed to get longhorn replicas ({code}): {data}&#34;
    replica = next(r for r in data[&#34;items&#34;] if pv_name == r[&#39;spec&#39;][&#39;volumeName&#39;])
    api_client.lhreplicas.delete(name=replica[&#39;metadata&#39;][&#39;name&#39;])
    lhvolume_degraded = volume_checker.wait_lhvolume_degraded(pv_name)
    assert lhvolume_degraded, (code, data)

    # create upgrade and verify it is not allowed
    version, url, checksum = upgrade_target
    version = f&#34;{version}-{unique_name}&#34;
    code, data = api_client.versions.create(version, url, checksum)
    assert code == 201, f&#34;Failed to create version {version}: {data}&#34;
    version_created, (code, data) = upgrade_checker.wait_version_created(version)
    assert version_created, (code, data)
    code, data = api_client.upgrades.create(version)
    assert code == 400, f&#34;Failed to verify degraded volume: {code}, {data}&#34;

    # Teardown invalid upgrade
    api_client.versions.delete(version)</code></pre>
</details>
<div class="desc"><p>Criteria: create upgrade should fails if there are any degraded volumes
Steps:
1. Create a VM using a volume with 3 replicas.
2. Delete one replica of the volume. Let the volume stay in
degraded state.
3. Immediately upgrade Harvester.
4. Upgrade should fail.</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_iso_url"><code class="name flex">
<span>def <span class="ident">test_iso_url</span></span>(<span>self, api_client, unique_name, upgrade_checker)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip_if_version(
        &#34;&lt; v1.5.0&#34;,
        reason=&#34;https://github.com/harvester/harvester/issues/7654 fix after `v1.5.0`&#34;)
def test_iso_url(self, api_client, unique_name, upgrade_checker):
    &#34;&#34;&#34;
    Steps:
    1. Create an invalid manifest.
    2. Try to upgrade with the invalid manifest.
    3. Upgrade should not start and fail.
    &#34;&#34;&#34;
    version, url = unique_name, &#34;https://invalid_iso_url&#34;
    checksum = sha512(b&#39;not_a_valid_checksum&#39;).hexdigest()

    code, data = api_client.versions.get(version)
    if code != 200:
        code, data = api_client.versions.create(version, url, checksum)
        assert code == 201, f&#34;Failed to create invalid version: {data}&#34;
        version_created, (code, data) = upgrade_checker.wait_version_created(version)
        assert version_created, (code, data)

    code, data = api_client.upgrades.create(version)
    assert code == 201, f&#34;Failed to create invalid upgrade: {data}&#34;
    upgrade_name = data[&#39;metadata&#39;][&#39;name&#39;]

    upgrade_fail_by_invalid_iso_url, (code, data) = \
        upgrade_checker.wait_upgrade_fail_by_invalid_iso_url(upgrade_name)
    assert upgrade_fail_by_invalid_iso_url, (code, data)

    # teardown
    api_client.upgrades.delete(upgrade_name)
    api_client.versions.delete(version)</code></pre>
</details>
<div class="desc"><p>Steps:
1. Create an invalid manifest.
2. Try to upgrade with the invalid manifest.
3. Upgrade should not start and fail.</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_version_compatibility"><code class="name flex">
<span>def <span class="ident">test_version_compatibility</span></span>(<span>self, api_client, unique_name, upgrade_target, upgrade_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(reason=&#34;https://github.com/harvester/harvester/issues/5494&#34;)
def test_version_compatibility(
    self, api_client, unique_name, upgrade_target, upgrade_timeout
):
    version, url, checksum = upgrade_target
    version = f&#34;{version}-{unique_name}&#34;

    code, data = api_client.versions.create(version, url, checksum)
    assert 201 == code, f&#34;Failed to create upgrade for {version}&#34;
    code, data = api_client.upgrades.create(version)
    assert 201 == code, f&#34;Failed to start upgrade for {version}&#34;

    endtime = datetime.now() + timedelta(seconds=upgrade_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.upgrades.get(data[&#39;metadata&#39;][&#39;name&#39;])
        conds = dict((c[&#39;type&#39;], c) for c in data.get(&#39;status&#39;, {}).get(&#39;conditions&#39;, []))
        verified = []  # TODO
        if all(verified):
            break
    else:
        raise AssertionError(f&#34;Upgrade NOT failed in expected conditions: {conds}&#34;)

    # teardown
    api_client.upgrades.delete(data[&#39;metadata&#39;][&#39;name&#39;])
    api_client.versions.delete(version)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="harvester_e2e_tests.integrations" href="index.html">harvester_e2e_tests.integrations</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.cluster_network" href="#harvester_e2e_tests.integrations.test_upgrade.cluster_network">cluster_network</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.cluster_state" href="#harvester_e2e_tests.integrations.test_upgrade.cluster_state">cluster_state</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.config_backup_target" href="#harvester_e2e_tests.integrations.test_upgrade.config_backup_target">config_backup_target</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.config_storageclass" href="#harvester_e2e_tests.integrations.test_upgrade.config_storageclass">config_storageclass</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.harvester_crds" href="#harvester_e2e_tests.integrations.test_upgrade.harvester_crds">harvester_crds</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.image" href="#harvester_e2e_tests.integrations.test_upgrade.image">image</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.interceptor" href="#harvester_e2e_tests.integrations.test_upgrade.interceptor">interceptor</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.logging_addon" href="#harvester_e2e_tests.integrations.test_upgrade.logging_addon">logging_addon</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.stopped_vm" href="#harvester_e2e_tests.integrations.test_upgrade.stopped_vm">stopped_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.upgrade_target" href="#harvester_e2e_tests.integrations.test_upgrade.upgrade_target">upgrade_target</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.vm_network" href="#harvester_e2e_tests.integrations.test_upgrade.vm_network">vm_network</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade">TestAnyNodesUpgrade</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.pytestmark" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_perform_upgrade" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_perform_upgrade">test_perform_upgrade</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_logging_pods" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_logging_pods">test_preq_logging_pods</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_storageclass" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_storageclass">test_preq_setup_storageclass</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vmnetwork" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vmnetwork">test_preq_setup_vmnetwork</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vms" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_preq_setup_vms">test_preq_setup_vms</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_image_deleted" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_image_deleted">test_upgrade_image_deleted</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_vm_deleted" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_vm_deleted">test_upgrade_vm_deleted</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_volume_deleted" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_upgrade_volume_deleted">test_upgrade_volume_deleted</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_audit_log" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_audit_log">test_verify_audit_log</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_crds_existed" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_crds_existed">test_verify_crds_existed</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_deployed_components_version" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_deployed_components_version">test_verify_deployed_components_version</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_logging_pods" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_logging_pods">test_verify_logging_pods</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_network" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_network">test_verify_network</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_os_version" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_os_version">test_verify_os_version</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_restore_vm" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_restore_vm">test_verify_restore_vm</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_rke2_version" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_rke2_version">test_verify_rke2_version</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_storage_class" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_storage_class">test_verify_storage_class</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_upgradelog" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_upgradelog">test_verify_upgradelog</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_vms" href="#harvester_e2e_tests.integrations.test_upgrade.TestAnyNodesUpgrade.test_verify_vms">test_verify_vms</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade">TestInvalidUpgrade</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.pytestmark" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_checksum" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_checksum">test_checksum</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_degraded_volume" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_degraded_volume">test_degraded_volume</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_iso_url" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_iso_url">test_iso_url</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_version_compatibility" href="#harvester_e2e_tests.integrations.test_upgrade.TestInvalidUpgrade.test_version_compatibility">test_version_compatibility</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
