<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>harvester_e2e_tests.integrations.test_1_images API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>harvester_e2e_tests.integrations.test_1_images</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="harvester_e2e_tests.integrations.test_1_images.cluster_network"><code class="name flex">
<span>def <span class="ident">cluster_network</span></span>(<span>api_client, vlan_nic)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;class&#34;)
def cluster_network(api_client, vlan_nic):
    # We should change this at some point. It fails if the total cnet name is over 12 chars
    cnet = f&#34;cnet-{vlan_nic.lower()}&#34;[:12]  # ???: RFC1123 and length limits
    code, data = api_client.clusternetworks.get(cnet)
    if code != 200:
        code, data = api_client.clusternetworks.create(cnet)
        assert 201 == code, (code, data)

    code, data = api_client.clusternetworks.get_config(cnet)
    if code != 200:
        code, data = api_client.clusternetworks.create_config(cnet, cnet, vlan_nic)
        assert 201 == code, (code, data)

    yield cnet

    # Teardown
    code, data = api_client.clusternetworks.delete_config(cnet)
    assert 200 == code, (code, data)
    code, data = api_client.clusternetworks.delete(cnet)
    assert 200 == code, (code, data)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.create_image_url"><code class="name flex">
<span>def <span class="ident">create_image_url</span></span>(<span>api_client, name, image_url, image_checksum, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_image_url(api_client, name, image_url, image_checksum, wait_timeout):
    code, data = api_client.images.create_by_url(name, image_url, image_checksum)

    assert 201 == code, (code, data)
    image_spec = data.get(&#34;spec&#34;)

    assert name == image_spec.get(&#34;displayName&#34;)
    assert &#34;download&#34; == image_spec.get(&#34;sourceType&#34;)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.images.get(name)
        image_status = data.get(&#34;status&#34;, {})

        assert 200 == code, (code, data)
        if image_status.get(&#34;progress&#34;) == 100:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to download image {name} with {wait_timeout} timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.delete_image"><code class="name flex">
<span>def <span class="ident">delete_image</span></span>(<span>api_client, image_name, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_image(api_client, image_name, wait_timeout):
    code, data = api_client.images.delete(image_name)

    assert 200 == code, f&#34;Failed to delete image with error: {code}, {data}&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    while endtime &gt; datetime.now():
        code, data = api_client.images.get(image_name)
        if code == 404:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to delete image {image_name} with {wait_timeout} timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.delete_volume"><code class="name flex">
<span>def <span class="ident">delete_volume</span></span>(<span>api_client, volume_name, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_volume(api_client, volume_name, wait_timeout):
    code, data = api_client.volumes.delete(volume_name)

    assert 200 == code, f&#34;Failed to delete volume with error: {code}, {data}&#34;

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.volumes.get(volume_name)
        if code == 404:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to delete volume {volume_name} with {wait_timeout} timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.export_storage_class"><code class="name flex">
<span>def <span class="ident">export_storage_class</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;session&#34;)
def export_storage_class():
    storage_class = &#34;harvester-longhorn&#34;
    return storage_class</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.fake_invalid_image_file"><code class="name flex">
<span>def <span class="ident">fake_invalid_image_file</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;session&#34;)
def fake_invalid_image_file():
    with NamedTemporaryFile(&#34;wb&#34;) as f:
        f.seek(5)  # less than 10MB
        f.write(b&#34;\0&#34;)
        f.seek(0)
        yield Path(f.name)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.get_image"><code class="name flex">
<span>def <span class="ident">get_image</span></span>(<span>api_client, image_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_image(api_client, image_name):
    code, data = api_client.images.get()

    assert len(data[&#34;items&#34;]) &gt; 0, (code, data)

    code, data = api_client.images.get(image_name)
    assert 200 == code, (code, data)
    assert image_name == data[&#34;metadata&#34;][&#34;name&#34;]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.image_info"><code class="name flex">
<span>def <span class="ident">image_info</span></span>(<span>request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(params=[&#34;image_opensuse&#34;, &#34;image_ubuntu&#34;])
def image_info(request):
    return request.getfixturevalue(request.param)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.storage_network"><code class="name flex">
<span>def <span class="ident">storage_network</span></span>(<span>api_client, cluster_network, vlan_id, vlan_cidr, setting_checker)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;class&#34;)
def storage_network(api_client, cluster_network, vlan_id, vlan_cidr, setting_checker):
    &#39;&#39;&#39; Ref. https://docs.harvesterhci.io/v1.3/advanced/storagenetwork/#configuration-example
    &#39;&#39;&#39;
    enable_spec = api_client.settings.StorageNetworkSpec.enable_with(
        vlan_id, cluster_network, vlan_cidr
    )
    code, data = api_client.settings.update(&#39;storage-network&#39;, enable_spec)
    assert 200 == code, (code, data)
    snet_enabled, (code, data) = setting_checker.wait_storage_net_enabled_on_harvester()
    assert snet_enabled, (code, data)
    snet_enabled, (code, data) = setting_checker.wait_storage_net_enabled_on_longhorn(vlan_cidr)
    assert snet_enabled, (code, data)

    yield

    # Teardown
    disable_spec = api_client.settings.StorageNetworkSpec.disable()
    code, data = api_client.settings.update(&#39;storage-network&#39;, disable_spec)
    assert 200 == code, (code, data)
    snet_disabled, (code, data) = setting_checker.wait_storage_net_disabled_on_harvester()
    assert snet_disabled, (code, data)
    snet_disabled, (code, data) = setting_checker.wait_storage_net_disabled_on_longhorn()
    assert snet_disabled, (code, data)</code></pre>
</details>
<div class="desc"><p>Ref. <a href="https://docs.harvesterhci.io/v1.3/advanced/storagenetwork/#configuration-example">https://docs.harvesterhci.io/v1.3/advanced/storagenetwork/#configuration-example</a></p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.vlan_cidr"><code class="name flex">
<span>def <span class="ident">vlan_cidr</span></span>(<span>api_client, cluster_network, vlan_id, wait_timeout, sleep_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture(scope=&#34;class&#34;)
def vlan_cidr(api_client, cluster_network, vlan_id, wait_timeout, sleep_timeout):
    vnet = f&#39;{cluster_network}-vlan{vlan_id}&#39;
    code, data = api_client.networks.get(vnet)
    if code != 200:
        code, data = api_client.networks.create(vnet, vlan_id, cluster_network=cluster_network)
        assert 201 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.networks.get(vnet)
        annotations = data[&#39;metadata&#39;].get(&#39;annotations&#39;, {})
        if 200 == code and annotations.get(&#39;network.harvesterhci.io/route&#39;):
            route = json.loads(annotations[&#39;network.harvesterhci.io/route&#39;])
            if route[&#39;cidr&#39;]:
                break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Fail to get route info of VM network {vnet} with error: {code}, {data}&#34;
        )

    yield route[&#39;cidr&#39;]

    # Teardown
    code, data = api_client.networks.delete(vnet)
    assert 200 == code, (code, data)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestBackendImages"><code class="flex name class">
<span>class <span class="ident">TestBackendImages</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.images
class TestBackendImages:
    @pytest.mark.smoke
    @pytest.mark.dependency(name=&#34;create_image_from_volume&#34;)
    def test_create_image_from_volume(
        self, api_client, unique_name, export_storage_class, wait_timeout
    ):
        &#34;&#34;&#34;
        Test create image from volume

        Steps:
            1. Create a volume &#34;test-volume&#34; in Volumes page
            2. Export the volume to image &#34;export-image&#34;
            3. Check the image &#34;export-image&#34; exists
            4. Cleanup image &#34;export-image&#34; on Images page
            5. Cleanup volume &#34;test-volume&#34; on Volumes page
        &#34;&#34;&#34;

        volume_name = f&#34;volume-{unique_name}&#34;
        image_name = f&#34;image-{unique_name}&#34;

        spec = api_client.volumes.Spec(1)
        code, data = api_client.volumes.create(volume_name, spec)

        assert 201 == code, (code, data)

        # Check volume ready
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.volumes.get(volume_name)
            if data[&#34;status&#34;][&#34;phase&#34;] == &#34;Bound&#34;:
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to delete volume {volume_name} bound in {wait_timeout} timed out\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        api_client.volumes.export(volume_name, image_name, export_storage_class)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)

        image_id = &#34;&#34;
        while endtime &gt; datetime.now():
            code, data = api_client.images.get()
            assert 200 == code, (code, data)

            for image in data[&#34;items&#34;]:
                if image[&#34;spec&#34;][&#34;displayName&#34;] == image_name:
                    if 100 == image.get(&#34;status&#34;, {}).get(&#34;progress&#34;, 0):
                        image_id = image[&#34;metadata&#34;][&#34;name&#34;]
                    break
            else:
                raise AssertionError(f&#34;Failed to find image {image_name}&#34;)

            if image_id != &#34;&#34;:
                break

            sleep(3)  # snooze

        delete_volume(api_client, volume_name, wait_timeout)
        delete_image(api_client, image_id, wait_timeout)

    @pytest.mark.smoke
    @pytest.mark.dependency(name=&#34;create_image_url&#34;)
    def test_create_image_url(self, image_info, unique_name, api_client, wait_timeout):
        &#34;&#34;&#34;
        Test create raw and iso type image from url

        Steps:
        1. Open image page and select default URL
        2. Input qcow2 image file download URL, wait for download complete
        3. Check the qcow2 image exists
        4. Input iso image file download URL, wait for download complete
        5. Check the iso image exists
        &#34;&#34;&#34;
        image_name = f&#34;{image_info.name}-{unique_name}&#34;
        image_url = image_info.url
        create_image_url(api_client, image_name, image_url,
                         image_info.image_checksum, wait_timeout)

    @pytest.mark.sanity
    @pytest.mark.skip_version_if(&#34;&gt; v1.2.0&#34;, &#34;&lt;= v1.4.0&#34;, reason=&#34;Issue#4293 fix after `v1.4.0`&#34;)
    @pytest.mark.dependency(name=&#34;delete_image_recreate&#34;, depends=[&#34;create_image_url&#34;])
    def test_delete_image_recreate(
        self,
        api_client,
        image_info,
        unique_name,
        fake_image_file,
        wait_timeout,
    ):
        &#34;&#34;&#34;
        Test create raw and iso type image from file

        Steps:
        1. Check the image created by URL exists
        2. Delete the newly created image
        3. Create an iso file type image from URL
        4. Check the iso image exists
        5. Upload an qcow2 file type image
        5. Delete the newly uploaded file
        6. Upload a new qcow2 file type image
        &#34;&#34;&#34;
        image_name = f&#34;{image_info.name}-{unique_name}&#34;
        image_url = image_info.url
        image_checksum = image_info.image_checksum

        get_image(api_client, image_name)
        delete_image(api_client, image_name, wait_timeout)

        create_image_url(api_client, image_name, image_url, image_checksum, wait_timeout)
        get_image(api_client, image_name)

        resp = api_client.images.create_by_file(unique_name, fake_image_file)

        assert (
            200 == resp.status_code
        ), f&#34;Failed to upload fake image with error:{resp.status_code}, {resp.content}&#34;

        get_image(api_client, unique_name)
        delete_image(api_client, unique_name, wait_timeout)

        resp = api_client.images.create_by_file(unique_name, fake_image_file)

        assert (
            200 == resp.status_code
        ), f&#34;Failed to upload fake image with error:{resp.status_code}, {resp.content}&#34;

        get_image(api_client, unique_name)
        delete_image(api_client, unique_name, wait_timeout)

    @pytest.mark.sanity
    @pytest.mark.negative
    def test_create_invalid_file(
        self, api_client, gen_unique_name, fake_invalid_image_file, wait_timeout
    ):
        &#34;&#34;&#34;
        Test create upload image from invalid file type

        Steps:
        1. Prepare an invalid file that is not in a multiple of 512 bytes
        2. Try to upload invalid image file which to images page
        2. Check should get an error
        &#34;&#34;&#34;
        unique_name = gen_unique_name()
        resp = api_client.images.create_by_file(unique_name, fake_invalid_image_file)

        assert (
            500 == resp.status_code
        ), f&#34;File size correct, it&#39;s a multiple of 512 bytes:{resp.status_code}, {resp.content}&#34;
        delete_image(api_client, unique_name, wait_timeout)

    @pytest.mark.sanity
    @pytest.mark.negative
    @pytest.mark.dependency(name=&#34;edit_image_in_use&#34;, depends=[&#34;create_image_url&#34;])
    def test_edit_image_in_use(self, api_client, unique_name, image_info, wait_timeout):
        &#34;&#34;&#34;
        Test can edit image which already in use

        Steps:
        1. Check the image created from URL exists
        2. Create a volume from existing image
        3. Update the image labels and description
        4. Check can change the image content
        &#34;&#34;&#34;

        image_name = f&#34;{image_info.name}-{unique_name}&#34;
        volume_name = f&#34;volume-{image_info.name}-{unique_name}&#34;

        code, data = api_client.images.get(name=image_name)
        assert 200 == code, (code, data)

        image_size_gb = data[&#34;status&#34;][&#34;virtualSize&#34;] // 1024**3 + 1
        image_id = f&#34;{data[&#39;metadata&#39;][&#39;namespace&#39;]}/{image_name}&#34;

        # Create volume from image_id
        spec = api_client.volumes.Spec(image_size_gb)
        code, data = api_client.volumes.create(volume_name, spec, image_id=image_id)
        assert 201 == code, (code, data)

        # Check volume ready
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.volumes.get(volume_name)
            if data[&#34;status&#34;][&#34;phase&#34;] == &#34;Bound&#34;:
                break
            sleep(5)
        else:
            raise AssertionError(
                f&#34;Failed to delete volume {unique_name} bound in {wait_timeout} timed out\n&#34;
                f&#34;Still got {code} with {data}&#34;
            )

        # Update image content
        updates = {
            &#34;labels&#34;: {&#34;usage-label&#34;: &#34;yes&#34;},
            &#34;annotations&#34;: {&#34;field.cattle.io/description&#34;: &#34;edit image in use&#34;},
        }

        # Update image by input
        code, data = api_client.images.update(image_name, dict(metadata=updates))
        assert 200 == code, f&#34;Failed to update image with error: {code}, {data}&#34;

        unexpected = list()
        for field, pairs in updates.items():
            for k, val in pairs.items():
                if data[&#34;metadata&#34;][field].get(k) != val:
                    unexpected.append((field, k, val, data[&#34;metadata&#34;][field].get(k)))

        assert not unexpected, &#34;\n&#34;.join(
            f&#34;Update {f} failed, set key {k} as {v} but got {n}&#34;
            for f, k, v, n in unexpected
        )

        delete_volume(api_client, volume_name, wait_timeout)
        delete_image(api_client, image_name, wait_timeout)</code></pre>
</details>
<div class="desc"></div>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_create_image_from_volume"><code class="name flex">
<span>def <span class="ident">test_create_image_from_volume</span></span>(<span>self, api_client, unique_name, export_storage_class, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.smoke
@pytest.mark.dependency(name=&#34;create_image_from_volume&#34;)
def test_create_image_from_volume(
    self, api_client, unique_name, export_storage_class, wait_timeout
):
    &#34;&#34;&#34;
    Test create image from volume

    Steps:
        1. Create a volume &#34;test-volume&#34; in Volumes page
        2. Export the volume to image &#34;export-image&#34;
        3. Check the image &#34;export-image&#34; exists
        4. Cleanup image &#34;export-image&#34; on Images page
        5. Cleanup volume &#34;test-volume&#34; on Volumes page
    &#34;&#34;&#34;

    volume_name = f&#34;volume-{unique_name}&#34;
    image_name = f&#34;image-{unique_name}&#34;

    spec = api_client.volumes.Spec(1)
    code, data = api_client.volumes.create(volume_name, spec)

    assert 201 == code, (code, data)

    # Check volume ready
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.volumes.get(volume_name)
        if data[&#34;status&#34;][&#34;phase&#34;] == &#34;Bound&#34;:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to delete volume {volume_name} bound in {wait_timeout} timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )

    api_client.volumes.export(volume_name, image_name, export_storage_class)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)

    image_id = &#34;&#34;
    while endtime &gt; datetime.now():
        code, data = api_client.images.get()
        assert 200 == code, (code, data)

        for image in data[&#34;items&#34;]:
            if image[&#34;spec&#34;][&#34;displayName&#34;] == image_name:
                if 100 == image.get(&#34;status&#34;, {}).get(&#34;progress&#34;, 0):
                    image_id = image[&#34;metadata&#34;][&#34;name&#34;]
                break
        else:
            raise AssertionError(f&#34;Failed to find image {image_name}&#34;)

        if image_id != &#34;&#34;:
            break

        sleep(3)  # snooze

    delete_volume(api_client, volume_name, wait_timeout)
    delete_image(api_client, image_id, wait_timeout)</code></pre>
</details>
<div class="desc"><p>Test create image from volume</p>
<h2 id="steps">Steps</h2>
<ol>
<li>Create a volume "test-volume" in Volumes page</li>
<li>Export the volume to image "export-image"</li>
<li>Check the image "export-image" exists</li>
<li>Cleanup image "export-image" on Images page</li>
<li>Cleanup volume "test-volume" on Volumes page</li>
</ol></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_create_image_url"><code class="name flex">
<span>def <span class="ident">test_create_image_url</span></span>(<span>self, image_info, unique_name, api_client, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.smoke
@pytest.mark.dependency(name=&#34;create_image_url&#34;)
def test_create_image_url(self, image_info, unique_name, api_client, wait_timeout):
    &#34;&#34;&#34;
    Test create raw and iso type image from url

    Steps:
    1. Open image page and select default URL
    2. Input qcow2 image file download URL, wait for download complete
    3. Check the qcow2 image exists
    4. Input iso image file download URL, wait for download complete
    5. Check the iso image exists
    &#34;&#34;&#34;
    image_name = f&#34;{image_info.name}-{unique_name}&#34;
    image_url = image_info.url
    create_image_url(api_client, image_name, image_url,
                     image_info.image_checksum, wait_timeout)</code></pre>
</details>
<div class="desc"><p>Test create raw and iso type image from url</p>
<p>Steps:
1. Open image page and select default URL
2. Input qcow2 image file download URL, wait for download complete
3. Check the qcow2 image exists
4. Input iso image file download URL, wait for download complete
5. Check the iso image exists</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_create_invalid_file"><code class="name flex">
<span>def <span class="ident">test_create_invalid_file</span></span>(<span>self, api_client, gen_unique_name, fake_invalid_image_file, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.sanity
@pytest.mark.negative
def test_create_invalid_file(
    self, api_client, gen_unique_name, fake_invalid_image_file, wait_timeout
):
    &#34;&#34;&#34;
    Test create upload image from invalid file type

    Steps:
    1. Prepare an invalid file that is not in a multiple of 512 bytes
    2. Try to upload invalid image file which to images page
    2. Check should get an error
    &#34;&#34;&#34;
    unique_name = gen_unique_name()
    resp = api_client.images.create_by_file(unique_name, fake_invalid_image_file)

    assert (
        500 == resp.status_code
    ), f&#34;File size correct, it&#39;s a multiple of 512 bytes:{resp.status_code}, {resp.content}&#34;
    delete_image(api_client, unique_name, wait_timeout)</code></pre>
</details>
<div class="desc"><p>Test create upload image from invalid file type</p>
<p>Steps:
1. Prepare an invalid file that is not in a multiple of 512 bytes
2. Try to upload invalid image file which to images page
2. Check should get an error</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_delete_image_recreate"><code class="name flex">
<span>def <span class="ident">test_delete_image_recreate</span></span>(<span>self, api_client, image_info, unique_name, fake_image_file, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.sanity
@pytest.mark.skip_version_if(&#34;&gt; v1.2.0&#34;, &#34;&lt;= v1.4.0&#34;, reason=&#34;Issue#4293 fix after `v1.4.0`&#34;)
@pytest.mark.dependency(name=&#34;delete_image_recreate&#34;, depends=[&#34;create_image_url&#34;])
def test_delete_image_recreate(
    self,
    api_client,
    image_info,
    unique_name,
    fake_image_file,
    wait_timeout,
):
    &#34;&#34;&#34;
    Test create raw and iso type image from file

    Steps:
    1. Check the image created by URL exists
    2. Delete the newly created image
    3. Create an iso file type image from URL
    4. Check the iso image exists
    5. Upload an qcow2 file type image
    5. Delete the newly uploaded file
    6. Upload a new qcow2 file type image
    &#34;&#34;&#34;
    image_name = f&#34;{image_info.name}-{unique_name}&#34;
    image_url = image_info.url
    image_checksum = image_info.image_checksum

    get_image(api_client, image_name)
    delete_image(api_client, image_name, wait_timeout)

    create_image_url(api_client, image_name, image_url, image_checksum, wait_timeout)
    get_image(api_client, image_name)

    resp = api_client.images.create_by_file(unique_name, fake_image_file)

    assert (
        200 == resp.status_code
    ), f&#34;Failed to upload fake image with error:{resp.status_code}, {resp.content}&#34;

    get_image(api_client, unique_name)
    delete_image(api_client, unique_name, wait_timeout)

    resp = api_client.images.create_by_file(unique_name, fake_image_file)

    assert (
        200 == resp.status_code
    ), f&#34;Failed to upload fake image with error:{resp.status_code}, {resp.content}&#34;

    get_image(api_client, unique_name)
    delete_image(api_client, unique_name, wait_timeout)</code></pre>
</details>
<div class="desc"><p>Test create raw and iso type image from file</p>
<p>Steps:
1. Check the image created by URL exists
2. Delete the newly created image
3. Create an iso file type image from URL
4. Check the iso image exists
5. Upload an qcow2 file type image
5. Delete the newly uploaded file
6. Upload a new qcow2 file type image</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_edit_image_in_use"><code class="name flex">
<span>def <span class="ident">test_edit_image_in_use</span></span>(<span>self, api_client, unique_name, image_info, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.sanity
@pytest.mark.negative
@pytest.mark.dependency(name=&#34;edit_image_in_use&#34;, depends=[&#34;create_image_url&#34;])
def test_edit_image_in_use(self, api_client, unique_name, image_info, wait_timeout):
    &#34;&#34;&#34;
    Test can edit image which already in use

    Steps:
    1. Check the image created from URL exists
    2. Create a volume from existing image
    3. Update the image labels and description
    4. Check can change the image content
    &#34;&#34;&#34;

    image_name = f&#34;{image_info.name}-{unique_name}&#34;
    volume_name = f&#34;volume-{image_info.name}-{unique_name}&#34;

    code, data = api_client.images.get(name=image_name)
    assert 200 == code, (code, data)

    image_size_gb = data[&#34;status&#34;][&#34;virtualSize&#34;] // 1024**3 + 1
    image_id = f&#34;{data[&#39;metadata&#39;][&#39;namespace&#39;]}/{image_name}&#34;

    # Create volume from image_id
    spec = api_client.volumes.Spec(image_size_gb)
    code, data = api_client.volumes.create(volume_name, spec, image_id=image_id)
    assert 201 == code, (code, data)

    # Check volume ready
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.volumes.get(volume_name)
        if data[&#34;status&#34;][&#34;phase&#34;] == &#34;Bound&#34;:
            break
        sleep(5)
    else:
        raise AssertionError(
            f&#34;Failed to delete volume {unique_name} bound in {wait_timeout} timed out\n&#34;
            f&#34;Still got {code} with {data}&#34;
        )

    # Update image content
    updates = {
        &#34;labels&#34;: {&#34;usage-label&#34;: &#34;yes&#34;},
        &#34;annotations&#34;: {&#34;field.cattle.io/description&#34;: &#34;edit image in use&#34;},
    }

    # Update image by input
    code, data = api_client.images.update(image_name, dict(metadata=updates))
    assert 200 == code, f&#34;Failed to update image with error: {code}, {data}&#34;

    unexpected = list()
    for field, pairs in updates.items():
        for k, val in pairs.items():
            if data[&#34;metadata&#34;][field].get(k) != val:
                unexpected.append((field, k, val, data[&#34;metadata&#34;][field].get(k)))

    assert not unexpected, &#34;\n&#34;.join(
        f&#34;Update {f} failed, set key {k} as {v} but got {n}&#34;
        for f, k, v, n in unexpected
    )

    delete_volume(api_client, volume_name, wait_timeout)
    delete_image(api_client, image_name, wait_timeout)</code></pre>
</details>
<div class="desc"><p>Test can edit image which already in use</p>
<p>Steps:
1. Check the image created from URL exists
2. Create a volume from existing image
3. Update the image labels and description
4. Check can change the image content</p></div>
</dd>
</dl>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements"><code class="flex name class">
<span>class <span class="ident">TestImageEnhancements</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TestImageEnhancements:

    @pytest.mark.p2
    @pytest.mark.images
    @pytest.mark.performance
    @pytest.mark.parametrize(&#34;image_size&#34;, [&#34;10Mi&#34;, &#34;50Mi&#34;, &#34;200Mi&#34;])
    def test_image_processing_performance(self, api_client, unique_name, image_size, wait_timeout):
        &#34;&#34;&#34;
        Test Harvester image processing performance (excluding network upload time)
        Steps:
        1. Upload image files of different sizes
        2. Measure processing time from upload completion to ready state
        3. Verify Harvester processing meets performance expectations
        &#34;&#34;&#34;

        # Processing thresholds (seconds) - time from upload complete to ready
        processing_thresholds = {&#34;10Mi&#34;: 10, &#34;50Mi&#34;: 20, &#34;200Mi&#34;: 40}
        size_bytes = int(image_size[:-2]) * 1024 * 1024

        if size_bytes % 512 != 0:
            size_bytes = ((size_bytes // 512) + 1) * 512

        with NamedTemporaryFile(&#34;wb&#34;, suffix=&#34;.raw&#34;) as f:
            f.seek(size_bytes - 1)
            f.write(b&#34;\x00&#34;)
            f.seek(0)

            image_name = f&#34;proc-perf-{unique_name}&#34;

            # Step 1: Upload (don&#39;t measure this time - it&#39;s network dependent)
            resp = api_client.images.create_by_file(image_name, Path(f.name))
            assert resp.ok, f&#34;Failed to upload {image_size} image: {resp.status_code}, {resp.text}&#34;

            # Step 2: Wait for upload to be acknowledged by Harvester
            initial_timeout = datetime.now() + timedelta(seconds=30)
            while initial_timeout &gt; datetime.now():
                code, data = api_client.images.get(image_name)
                if code == 200:
                    status = data.get(&#34;status&#34;, {})
                    if &#34;progress&#34; in status:
                        # Upload acknowledged, start measuring processing time
                        processing_start_time = time()
                        break
                sleep(1)
            else:
                raise AssertionError(f&#34;Image {image_name} not acknowledged by Harvester&#34;)

            # Step 3: Measure time from processing start to completion
            endtime = datetime.now() + timedelta(seconds=wait_timeout)
            while endtime &gt; datetime.now():
                code, data = api_client.images.get(image_name)
                if code == 200 and data.get(&#34;status&#34;, {}).get(&#34;progress&#34;) == 100:
                    processing_end_time = time()
                    break
                sleep(2)
            else:
                raise AssertionError(f&#34;Image processing did not complete within {wait_timeout}s&#34;)

            # Step 4: Validate Harvester processing performance
            processing_time = processing_end_time - processing_start_time
            threshold = processing_thresholds[image_size]
            assert processing_time &lt; threshold, f&#34;Harvester processing took&#34; \
                                                f&#34; {processing_time:.2f}s, expected &lt; {threshold}s&#34;

            print(f&#34;Harvester Processing Performance:&#34;
                  f&#34; {image_size} processed in {processing_time:.2f}s&#34;)

            delete_image(api_client, image_name, wait_timeout)

    @pytest.mark.p1
    @pytest.mark.images
    def test_image_checksum_validation(self, api_client, image_info, unique_name, wait_timeout):
        &#34;&#34;&#34;
        Test image checksum validation during URL-based creation
        Steps:
        1. Create image with correct checksum - should succeed
        2. Create image with incorrect checksum - should fail
        3. Create image with no checksum - should succeed
        4. Verify checksum validation behavior
        &#34;&#34;&#34;
        # Extract OS name from image_info for naming
        os_name = image_info.name.lower()  # e.g., &#34;opensuse&#34; or &#34;ubuntu&#34;

        # Test with correct checksum
        correct_name = f&#34;correct-checksum-{os_name}-{unique_name}&#34;
        create_image_url(api_client, correct_name, image_info.url,
                         image_info.image_checksum, wait_timeout)

        # Test with incorrect checksum
        incorrect_name = f&#34;incorrect-checksum-{os_name}-{unique_name}&#34;
        fake_checksum = hashlib.sha512(b&#39;fake_checksum&#39;).hexdigest()

        code, data = api_client.images.create_by_url(incorrect_name, image_info.url, fake_checksum)
        assert 201 == code, &#34;Image creation should initially succeed&#34;

        # Should eventually fail due to checksum mismatch
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        checksum_error = False
        while endtime &gt; datetime.now():
            code, data = api_client.images.get(incorrect_name)
            if code == 200:
                status = data.get(&#34;status&#34;, {})
                if &#34;checksum&#34; in str(status).lower() and status.get(&#34;failed&#34;, 0) &gt; 0:
                    checksum_error = True
                    break
            sleep(5)

        assert checksum_error, &#34;Image should fail due to checksum mismatch&#34;

        # Test with no checksum
        no_checksum_name = f&#34;no-checksum-{os_name}-{unique_name}&#34;
        create_image_url(api_client, no_checksum_name, image_info.url, None, wait_timeout)

        # Cleanup
        delete_image(api_client, correct_name, wait_timeout)

        # Cleanup incorrect checksum image (may not exist if creation failed)
        try:
            delete_image(api_client, incorrect_name, wait_timeout)
        except AssertionError as e:
            print(f&#34;Cleanup failed/skipped for incorrect checksum image {incorrect_name}: {e}&#34;)

        # Cleanup no-checksum image (only if it was created)
        try:
            delete_image(api_client, no_checksum_name, wait_timeout)
        except AssertionError as e:
            print(f&#34;Cleanup failed/skipped for no-checksum image {no_checksum_name}: {e}&#34;)

    @pytest.mark.p1
    @pytest.mark.images
    def test_image_concurrent_operations(self, api_client, fake_image_file,
                                         gen_unique_name, wait_timeout):
        &#34;&#34;&#34;
        Test comprehensive concurrent image operations
        Steps:
        1. Test concurrent image uploads (multiple different images)
        2. Test concurrent operations on same image (get, update, status checks)
        3. Verify system handles all concurrency scenarios gracefully
        4. Ensure data integrity and proper resource management
        &#34;&#34;&#34;

        # Part 1: Concurrent uploads of different images
        concurrent_upload_count = 3
        upload_image_names = [f&#34;upload-concurrent-{i}-{gen_unique_name()}&#34;
                              for i in range(concurrent_upload_count)]
        successful_uploads = []
        upload_errors = []

        def upload_single_image(image_name):
            resp = api_client.images.create_by_file(image_name, fake_image_file)
            if resp.ok:
                return {&#34;name&#34;: image_name, &#34;success&#34;: True}
            else:
                return {&#34;name&#34;: image_name, &#34;success&#34;: False,
                        &#34;error&#34;: f&#34;{resp.status_code}: {resp.text}&#34;}

        # Execute concurrent uploads
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_upload_count) as executor:  # NOQA
            upload_futures = [executor.submit(upload_single_image, name)
                              for name in upload_image_names]

            for future in concurrent.futures.as_completed(upload_futures):
                result = future.result()
                if result[&#34;success&#34;]:
                    successful_uploads.append(result[&#34;name&#34;])
                else:
                    upload_errors.append(result)

        assert len(upload_errors) == 0, f&#34;Concurrent upload errors: {upload_errors}&#34;
        assert len(successful_uploads) == concurrent_upload_count, \
            f&#34;Expected {concurrent_upload_count} uploads, got {len(successful_uploads)}&#34;

        # Verify all uploaded images are accessible
        for image_name in successful_uploads:
            code, data = api_client.images.get(image_name)
            assert code == 200, f&#34;Image {image_name} not accessible: {code}&#34;

        # Part 2: Concurrent operations on the same image
        # Use the first uploaded image for operations testing
        target_image_name = successful_uploads[0]
        concurrent_results = []

        def concurrent_operation(operation_type, operation_id):
            if operation_type == &#34;get&#34;:
                code, data = api_client.images.get(target_image_name)
                return {&#34;id&#34;: operation_id, &#34;op&#34;: &#34;get&#34;, &#34;success&#34;: code == 200, &#34;code&#34;: code}

            elif operation_type == &#34;update&#34;:
                update_data = {&#34;labels&#34;: {f&#34;concurrent-op-{operation_id}&#34;: f&#34;timestamp-{time.time()}&#34;}}  # NOQA
                code, data = api_client.images.update(target_image_name,
                                                      dict(metadata=update_data))
                return {&#34;id&#34;: operation_id, &#34;op&#34;: &#34;update&#34;,
                        &#34;success&#34;: code in [200, 409], &#34;code&#34;: code}

            elif operation_type == &#34;status_check&#34;:
                code, data = api_client.images.get(target_image_name)
                if code == 200:
                    progress = data.get(&#34;status&#34;, {}).get(&#34;progress&#34;, 0)
                    return {&#34;id&#34;: operation_id, &#34;op&#34;: &#34;status_check&#34;,
                            &#34;success&#34;: True, &#34;code&#34;: code, &#34;progress&#34;: progress}
                return {&#34;id&#34;: operation_id, &#34;op&#34;: &#34;status_check&#34;, &#34;success&#34;: False, &#34;code&#34;: code}

        # Launch concurrent operations on the same image
        threads = []
        operations = [
            (&#34;get&#34;, 1), (&#34;update&#34;, 2), (&#34;status_check&#34;, 3),
            (&#34;get&#34;, 4), (&#34;update&#34;, 5), (&#34;get&#34;, 6), (&#34;status_check&#34;, 7)
        ]

        for op_type, op_id in operations:
            thread = threading.Thread(
                target=lambda ot=op_type, oid=op_id: concurrent_results.append(concurrent_operation(ot, oid))  # NOQA
            )
            threads.append(thread)
            thread.start()

        # Wait for all operations to complete
        for thread in threads:
            thread.join(timeout=15)
            assert not thread.is_alive(), &#34;Thread didn&#39;t complete in time&#34;

        # Analyze concurrent operation results
        successful_ops = [r for r in concurrent_results if r and r.get(&#34;success&#34;)]
        assert len(successful_ops) &gt; 0, &#34;No concurrent operations succeeded on same image&#34;

        # Target image should still be responsive after concurrent operations
        final_code, final_data = api_client.images.get(target_image_name)
        assert final_code == 200, f&#34;Target image not accessible after &#34; \
                                  f&#34;concurrent operations: {final_code}&#34;

        # Part 3: Concurrent cleanup of all images
        cleanup_successful = 0

        def cleanup_single_image(image_name):
            delete_image(api_client, image_name, wait_timeout)
            return image_name

        with concurrent.futures.ThreadPoolExecutor(max_workers=len(successful_uploads)) as executor:  # NOQA
            cleanup_futures = [executor.submit(cleanup_single_image, name)
                               for name in successful_uploads]

            for _ in concurrent.futures.as_completed(cleanup_futures):
                cleanup_successful += 1

        assert cleanup_successful == len(successful_uploads), \
            f&#34;Cleanup failed: {cleanup_successful}/{len(successful_uploads)}&#34;

    @pytest.mark.p1
    @pytest.mark.images
    def test_image_metadata_operations(self, api_client, fake_image_file,
                                       unique_name, wait_timeout):
        &#34;&#34;&#34;
        Test comprehensive image metadata operations
        Steps:
        1. Create image and verify initial metadata
        2. Update labels, annotations, and description
        3. Test metadata persistence across operations
        4. Verify metadata validation and limits
        &#34;&#34;&#34;
        image_name = f&#34;metadata-test-{unique_name}&#34;

        # Create image
        resp = api_client.images.create_by_file(image_name, fake_image_file)
        assert resp.ok, f&#34;Failed to create image: {resp.status_code}, {resp.text}&#34;

        # Wait for initial processing
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.images.get(image_name)
            if code == 200 and data.get(&#34;status&#34;, {}).get(&#34;progress&#34;) == 100:
                break
            sleep(2)

        # Test comprehensive metadata updates
        metadata_updates = {
            &#34;labels&#34;: {
                &#34;environment&#34;: &#34;test&#34;,
                &#34;team&#34;: &#34;qa&#34;,
                &#34;version&#34;: &#34;1.0.0&#34;,
                &#34;critical&#34;: &#34;false&#34;
            },
            &#34;annotations&#34;: {
                &#34;field.cattle.io/description&#34;: &#34;Test image for metadata operations&#34;,
                &#34;harvesterhci.io/imageId&#34;: image_name,
                &#34;custom.annotation/usage&#34;: &#34;automation-testing&#34;,
                &#34;created.by&#34;: &#34;harvester-e2e-tests&#34;
            }
        }

        code, data = api_client.images.update(image_name, dict(metadata=metadata_updates))
        assert 200 == code, f&#34;Failed to update metadata: {code}, {data}&#34;

        # Verify metadata was applied correctly
        code, data = api_client.images.get(image_name)
        assert 200 == code, (code, data)

        metadata = data[&#34;metadata&#34;]
        for field, expected_pairs in metadata_updates.items():
            for key, expected_value in expected_pairs.items():
                actual_value = metadata.get(field, {}).get(key)
                assert actual_value == expected_value, f&#34;Metadata {field}.{key}: expected&#34;\
                                                       f&#34; {expected_value}, got {actual_value}&#34;

        # Test incremental metadata updates
        incremental_updates = {
            &#34;labels&#34;: {&#34;priority&#34;: &#34;high&#34;},
            &#34;annotations&#34;: {&#34;last.modified&#34;: &#34;2025-09-30&#34;}
        }

        code, data = api_client.images.update(image_name, dict(metadata=incremental_updates))
        assert 200 == code, f&#34;Failed incremental update: {code}, {data}&#34;

        # Verify both old and new metadata exist
        code, data = api_client.images.get(image_name)
        assert 200 == code, (code, data)

        metadata = data[&#34;metadata&#34;]
        assert metadata.get(&#34;labels&#34;, {}).get(&#34;environment&#34;) == &#34;test&#34;  # Old label
        assert metadata.get(&#34;labels&#34;, {}).get(&#34;priority&#34;) == &#34;high&#34;  # New label

        delete_image(api_client, image_name, wait_timeout)

    @pytest.mark.p2
    @pytest.mark.images
    def test_image_complete_lifecycle(self, api_client, image_info, unique_name, wait_timeout):
        &#34;&#34;&#34;
        Test complete image lifecycle from creation to deletion
        Steps:
        1. Create image from URL with metadata
        2. Update image properties multiple times
        3. Create volume from image
        4. Export volume back to new image
        5. Download and verify image content
        6. Perform cleanup and verify deletion
        &#34;&#34;&#34;
        original_image = f&#34;lifecycle-original-{unique_name}&#34;
        exported_image = f&#34;lifecycle-exported-{unique_name}&#34;
        test_volume = f&#34;lifecycle-volume-{unique_name}&#34;

        # Step 1: Create original image with metadata
        create_image_url(api_client, original_image, image_info.url,
                         image_info.image_checksum, wait_timeout)

        initial_metadata = {
            &#34;labels&#34;: {&#34;lifecycle&#34;: &#34;test&#34;, &#34;stage&#34;: &#34;original&#34;},
            &#34;annotations&#34;: {&#34;description&#34;: &#34;Lifecycle test original image&#34;}
        }

        code, data = api_client.images.update(original_image, dict(metadata=initial_metadata))
        assert 200 == code, f&#34;Failed to set initial metadata: {code}, {data}&#34;

        # Step 2: Multiple metadata updates
        for i in range(3):
            update_metadata = {
                &#34;labels&#34;: {&#34;update.count&#34;: str(i + 1)},
                &#34;annotations&#34;: {f&#34;update.{i}&#34;: f&#34;value-{i}&#34;}
            }
            code, data = api_client.images.update(original_image, dict(metadata=update_metadata))
            assert 200 == code, f&#34;Failed update {i}: {code}, {data}&#34;

        # Step 3: Create volume from image
        code, data = api_client.images.get(original_image)
        assert 200 == code, (code, data)

        image_size_gb = data[&#34;status&#34;][&#34;virtualSize&#34;] // 1024**3 + 1
        image_id = f&#34;{data[&#39;metadata&#39;][&#39;namespace&#39;]}/{original_image}&#34;

        spec = api_client.volumes.Spec(image_size_gb)
        code, data = api_client.volumes.create(test_volume, spec, image_id=image_id)
        assert 201 == code, (code, data)

        # Wait for volume to be bound
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.volumes.get(test_volume)
            if data[&#34;status&#34;][&#34;phase&#34;] == &#34;Bound&#34;:
                break
            sleep(5)
        else:
            raise AssertionError(f&#34;Volume binding timeout: {code}, {data}&#34;)

        # Step 4: Export volume back to image
        code, data = api_client.volumes.export(test_volume, exported_image, &#34;harvester-longhorn&#34;)
        assert 200 == code, (code, data)

        # Wait for export completion
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        exported_image_id = &#34;&#34;
        while endtime &gt; datetime.now():
            code, data = api_client.images.get()
            assert 200 == code, (code, data)

            for image in data[&#34;items&#34;]:
                if image[&#34;spec&#34;][&#34;displayName&#34;] == exported_image:
                    if image.get(&#34;status&#34;, {}).get(&#34;progress&#34;) == 100:
                        exported_image_id = image[&#34;metadata&#34;][&#34;name&#34;]
                        break

            if exported_image_id:
                break
            sleep(5)

        assert exported_image_id, f&#34;Failed to find exported image {exported_image}&#34;

        # Step 5: Verify exported image properties
        code, data = api_client.images.get(exported_image_id)
        assert 200 == code, (code, data)
        assert data[&#34;spec&#34;][&#34;displayName&#34;] == exported_image

        # Step 6: Cleanup in proper order
        delete_volume(api_client, test_volume, wait_timeout)
        delete_image(api_client, original_image, wait_timeout)
        delete_image(api_client, exported_image_id, wait_timeout)

    @pytest.mark.p1
    @pytest.mark.images
    @pytest.mark.negative
    def test_image_name_length_limits(self, api_client, unique_name, wait_timeout):
        &#34;&#34;&#34;
        Test image creation with various name lengths
        Steps:
        1. Test failure with image name &gt; 63 characters (should fail)
        2. Test success with image name ≤ 63 characters (should pass)
        &#34;&#34;&#34;
        aligned_size = 1024 * 1024  # 1MB exactly

        with NamedTemporaryFile(&#34;wb&#34;, suffix=&#34;.raw&#34;) as f:
            f.write(b&#34;\x00&#34; * aligned_size)
            f.flush()

            # Test 1: Invalid long name (should FAIL)
            long_base = &#34;very-long-image-name-for-testing-kubernetes-name-length-limits-that-exceeds-maximum&#34;  # NOQA
            invalid_long_name = f&#34;{long_base}-{unique_name}&#34;

            resp = api_client.images.create_by_file(invalid_long_name, Path(f.name))
            assert not resp.ok, f&#34;Expected failure with long name but&#34; \
                                f&#34; got success: {resp.status_code}&#34;
            assert resp.status_code in [400, 422], f&#34;Expected 400/422 for &#34; \
                                                   f&#34;invalid name, got: {resp.status_code}&#34;

            # Test 2: Valid name (should PASS)
            valid_base = &#34;valid-image-name&#34;
            valid_name = f&#34;{valid_base}-{unique_name}&#34;

            if len(valid_name) &gt; 63:
                valid_name = f&#34;{valid_base[:30]}-{unique_name}&#34;
            valid_name = valid_name.rstrip(&#39;-&#39;)

            resp = api_client.images.create_by_file(valid_name, Path(f.name))
            assert resp.ok, f&#34;Failed with valid name: {resp.status_code}, {resp.text}&#34;

            # Cleanup
            delete_image(api_client, valid_name, wait_timeout)

    @pytest.mark.p1
    @pytest.mark.images
    @pytest.mark.negative
    def test_image_metadata_limits(self, api_client, unique_name, wait_timeout):
        &#34;&#34;&#34;
        Test image metadata handling with excessive data
        Steps:
        1. Create a valid image
        2. Test behavior with excessive metadata (many labels/annotations)
        3. Verify proper handling of metadata limits
        &#34;&#34;&#34;
        aligned_size = 1024 * 1024  # 1MB exactly

        with NamedTemporaryFile(&#34;wb&#34;, suffix=&#34;.raw&#34;) as f:
            f.write(b&#34;\x00&#34; * aligned_size)
            f.flush()

            # Create base image for metadata testing
            image_name = f&#34;metadata-test-{unique_name}&#34;

            resp = api_client.images.create_by_file(image_name, Path(f.name))
            assert resp.ok, f&#34;Failed to create base image: {resp.status_code}, {resp.text}&#34;

            # Test excessive metadata
            excessive_metadata = {
                &#34;labels&#34;: {f&#34;test-label-key-{i:03d}&#34;: f&#34;test-label-value&#34;
                                                      f&#34;-{i:03d}&#34; for i in range(20)},
                &#34;annotations&#34;: {
                    f&#34;test.annotation.key/{i:03d}&#34;: &#34;x&#34; * 200 for i in range(10)
                }
            }

            code, data = api_client.images.update(image_name, dict(metadata=excessive_metadata))

            if code == 200:
                # Verify metadata was actually stored
                code, updated_data = api_client.images.get(image_name)
                assert code == 200, f&#34;Failed to retrieve updated image: {code}&#34;

            elif code in [400, 422, 413]:  # 413 = Request Entity Too Large
                print(f&#34;✓ Excessive metadata properly rejected with status: {code}&#34;)
            else:
                assert False, f&#34;Unexpected metadata response: {code}, {data}&#34;

            # Cleanup
            delete_image(api_client, image_name, wait_timeout)</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_checksum_validation"><code class="name flex">
<span>def <span class="ident">test_image_checksum_validation</span></span>(<span>self, api_client, image_info, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p1
@pytest.mark.images
def test_image_checksum_validation(self, api_client, image_info, unique_name, wait_timeout):
    &#34;&#34;&#34;
    Test image checksum validation during URL-based creation
    Steps:
    1. Create image with correct checksum - should succeed
    2. Create image with incorrect checksum - should fail
    3. Create image with no checksum - should succeed
    4. Verify checksum validation behavior
    &#34;&#34;&#34;
    # Extract OS name from image_info for naming
    os_name = image_info.name.lower()  # e.g., &#34;opensuse&#34; or &#34;ubuntu&#34;

    # Test with correct checksum
    correct_name = f&#34;correct-checksum-{os_name}-{unique_name}&#34;
    create_image_url(api_client, correct_name, image_info.url,
                     image_info.image_checksum, wait_timeout)

    # Test with incorrect checksum
    incorrect_name = f&#34;incorrect-checksum-{os_name}-{unique_name}&#34;
    fake_checksum = hashlib.sha512(b&#39;fake_checksum&#39;).hexdigest()

    code, data = api_client.images.create_by_url(incorrect_name, image_info.url, fake_checksum)
    assert 201 == code, &#34;Image creation should initially succeed&#34;

    # Should eventually fail due to checksum mismatch
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    checksum_error = False
    while endtime &gt; datetime.now():
        code, data = api_client.images.get(incorrect_name)
        if code == 200:
            status = data.get(&#34;status&#34;, {})
            if &#34;checksum&#34; in str(status).lower() and status.get(&#34;failed&#34;, 0) &gt; 0:
                checksum_error = True
                break
        sleep(5)

    assert checksum_error, &#34;Image should fail due to checksum mismatch&#34;

    # Test with no checksum
    no_checksum_name = f&#34;no-checksum-{os_name}-{unique_name}&#34;
    create_image_url(api_client, no_checksum_name, image_info.url, None, wait_timeout)

    # Cleanup
    delete_image(api_client, correct_name, wait_timeout)

    # Cleanup incorrect checksum image (may not exist if creation failed)
    try:
        delete_image(api_client, incorrect_name, wait_timeout)
    except AssertionError as e:
        print(f&#34;Cleanup failed/skipped for incorrect checksum image {incorrect_name}: {e}&#34;)

    # Cleanup no-checksum image (only if it was created)
    try:
        delete_image(api_client, no_checksum_name, wait_timeout)
    except AssertionError as e:
        print(f&#34;Cleanup failed/skipped for no-checksum image {no_checksum_name}: {e}&#34;)</code></pre>
</details>
<div class="desc"><p>Test image checksum validation during URL-based creation
Steps:
1. Create image with correct checksum - should succeed
2. Create image with incorrect checksum - should fail
3. Create image with no checksum - should succeed
4. Verify checksum validation behavior</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_complete_lifecycle"><code class="name flex">
<span>def <span class="ident">test_image_complete_lifecycle</span></span>(<span>self, api_client, image_info, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p2
@pytest.mark.images
def test_image_complete_lifecycle(self, api_client, image_info, unique_name, wait_timeout):
    &#34;&#34;&#34;
    Test complete image lifecycle from creation to deletion
    Steps:
    1. Create image from URL with metadata
    2. Update image properties multiple times
    3. Create volume from image
    4. Export volume back to new image
    5. Download and verify image content
    6. Perform cleanup and verify deletion
    &#34;&#34;&#34;
    original_image = f&#34;lifecycle-original-{unique_name}&#34;
    exported_image = f&#34;lifecycle-exported-{unique_name}&#34;
    test_volume = f&#34;lifecycle-volume-{unique_name}&#34;

    # Step 1: Create original image with metadata
    create_image_url(api_client, original_image, image_info.url,
                     image_info.image_checksum, wait_timeout)

    initial_metadata = {
        &#34;labels&#34;: {&#34;lifecycle&#34;: &#34;test&#34;, &#34;stage&#34;: &#34;original&#34;},
        &#34;annotations&#34;: {&#34;description&#34;: &#34;Lifecycle test original image&#34;}
    }

    code, data = api_client.images.update(original_image, dict(metadata=initial_metadata))
    assert 200 == code, f&#34;Failed to set initial metadata: {code}, {data}&#34;

    # Step 2: Multiple metadata updates
    for i in range(3):
        update_metadata = {
            &#34;labels&#34;: {&#34;update.count&#34;: str(i + 1)},
            &#34;annotations&#34;: {f&#34;update.{i}&#34;: f&#34;value-{i}&#34;}
        }
        code, data = api_client.images.update(original_image, dict(metadata=update_metadata))
        assert 200 == code, f&#34;Failed update {i}: {code}, {data}&#34;

    # Step 3: Create volume from image
    code, data = api_client.images.get(original_image)
    assert 200 == code, (code, data)

    image_size_gb = data[&#34;status&#34;][&#34;virtualSize&#34;] // 1024**3 + 1
    image_id = f&#34;{data[&#39;metadata&#39;][&#39;namespace&#39;]}/{original_image}&#34;

    spec = api_client.volumes.Spec(image_size_gb)
    code, data = api_client.volumes.create(test_volume, spec, image_id=image_id)
    assert 201 == code, (code, data)

    # Wait for volume to be bound
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.volumes.get(test_volume)
        if data[&#34;status&#34;][&#34;phase&#34;] == &#34;Bound&#34;:
            break
        sleep(5)
    else:
        raise AssertionError(f&#34;Volume binding timeout: {code}, {data}&#34;)

    # Step 4: Export volume back to image
    code, data = api_client.volumes.export(test_volume, exported_image, &#34;harvester-longhorn&#34;)
    assert 200 == code, (code, data)

    # Wait for export completion
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    exported_image_id = &#34;&#34;
    while endtime &gt; datetime.now():
        code, data = api_client.images.get()
        assert 200 == code, (code, data)

        for image in data[&#34;items&#34;]:
            if image[&#34;spec&#34;][&#34;displayName&#34;] == exported_image:
                if image.get(&#34;status&#34;, {}).get(&#34;progress&#34;) == 100:
                    exported_image_id = image[&#34;metadata&#34;][&#34;name&#34;]
                    break

        if exported_image_id:
            break
        sleep(5)

    assert exported_image_id, f&#34;Failed to find exported image {exported_image}&#34;

    # Step 5: Verify exported image properties
    code, data = api_client.images.get(exported_image_id)
    assert 200 == code, (code, data)
    assert data[&#34;spec&#34;][&#34;displayName&#34;] == exported_image

    # Step 6: Cleanup in proper order
    delete_volume(api_client, test_volume, wait_timeout)
    delete_image(api_client, original_image, wait_timeout)
    delete_image(api_client, exported_image_id, wait_timeout)</code></pre>
</details>
<div class="desc"><p>Test complete image lifecycle from creation to deletion
Steps:
1. Create image from URL with metadata
2. Update image properties multiple times
3. Create volume from image
4. Export volume back to new image
5. Download and verify image content
6. Perform cleanup and verify deletion</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_concurrent_operations"><code class="name flex">
<span>def <span class="ident">test_image_concurrent_operations</span></span>(<span>self, api_client, fake_image_file, gen_unique_name, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p1
@pytest.mark.images
def test_image_concurrent_operations(self, api_client, fake_image_file,
                                     gen_unique_name, wait_timeout):
    &#34;&#34;&#34;
    Test comprehensive concurrent image operations
    Steps:
    1. Test concurrent image uploads (multiple different images)
    2. Test concurrent operations on same image (get, update, status checks)
    3. Verify system handles all concurrency scenarios gracefully
    4. Ensure data integrity and proper resource management
    &#34;&#34;&#34;

    # Part 1: Concurrent uploads of different images
    concurrent_upload_count = 3
    upload_image_names = [f&#34;upload-concurrent-{i}-{gen_unique_name()}&#34;
                          for i in range(concurrent_upload_count)]
    successful_uploads = []
    upload_errors = []

    def upload_single_image(image_name):
        resp = api_client.images.create_by_file(image_name, fake_image_file)
        if resp.ok:
            return {&#34;name&#34;: image_name, &#34;success&#34;: True}
        else:
            return {&#34;name&#34;: image_name, &#34;success&#34;: False,
                    &#34;error&#34;: f&#34;{resp.status_code}: {resp.text}&#34;}

    # Execute concurrent uploads
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_upload_count) as executor:  # NOQA
        upload_futures = [executor.submit(upload_single_image, name)
                          for name in upload_image_names]

        for future in concurrent.futures.as_completed(upload_futures):
            result = future.result()
            if result[&#34;success&#34;]:
                successful_uploads.append(result[&#34;name&#34;])
            else:
                upload_errors.append(result)

    assert len(upload_errors) == 0, f&#34;Concurrent upload errors: {upload_errors}&#34;
    assert len(successful_uploads) == concurrent_upload_count, \
        f&#34;Expected {concurrent_upload_count} uploads, got {len(successful_uploads)}&#34;

    # Verify all uploaded images are accessible
    for image_name in successful_uploads:
        code, data = api_client.images.get(image_name)
        assert code == 200, f&#34;Image {image_name} not accessible: {code}&#34;

    # Part 2: Concurrent operations on the same image
    # Use the first uploaded image for operations testing
    target_image_name = successful_uploads[0]
    concurrent_results = []

    def concurrent_operation(operation_type, operation_id):
        if operation_type == &#34;get&#34;:
            code, data = api_client.images.get(target_image_name)
            return {&#34;id&#34;: operation_id, &#34;op&#34;: &#34;get&#34;, &#34;success&#34;: code == 200, &#34;code&#34;: code}

        elif operation_type == &#34;update&#34;:
            update_data = {&#34;labels&#34;: {f&#34;concurrent-op-{operation_id}&#34;: f&#34;timestamp-{time.time()}&#34;}}  # NOQA
            code, data = api_client.images.update(target_image_name,
                                                  dict(metadata=update_data))
            return {&#34;id&#34;: operation_id, &#34;op&#34;: &#34;update&#34;,
                    &#34;success&#34;: code in [200, 409], &#34;code&#34;: code}

        elif operation_type == &#34;status_check&#34;:
            code, data = api_client.images.get(target_image_name)
            if code == 200:
                progress = data.get(&#34;status&#34;, {}).get(&#34;progress&#34;, 0)
                return {&#34;id&#34;: operation_id, &#34;op&#34;: &#34;status_check&#34;,
                        &#34;success&#34;: True, &#34;code&#34;: code, &#34;progress&#34;: progress}
            return {&#34;id&#34;: operation_id, &#34;op&#34;: &#34;status_check&#34;, &#34;success&#34;: False, &#34;code&#34;: code}

    # Launch concurrent operations on the same image
    threads = []
    operations = [
        (&#34;get&#34;, 1), (&#34;update&#34;, 2), (&#34;status_check&#34;, 3),
        (&#34;get&#34;, 4), (&#34;update&#34;, 5), (&#34;get&#34;, 6), (&#34;status_check&#34;, 7)
    ]

    for op_type, op_id in operations:
        thread = threading.Thread(
            target=lambda ot=op_type, oid=op_id: concurrent_results.append(concurrent_operation(ot, oid))  # NOQA
        )
        threads.append(thread)
        thread.start()

    # Wait for all operations to complete
    for thread in threads:
        thread.join(timeout=15)
        assert not thread.is_alive(), &#34;Thread didn&#39;t complete in time&#34;

    # Analyze concurrent operation results
    successful_ops = [r for r in concurrent_results if r and r.get(&#34;success&#34;)]
    assert len(successful_ops) &gt; 0, &#34;No concurrent operations succeeded on same image&#34;

    # Target image should still be responsive after concurrent operations
    final_code, final_data = api_client.images.get(target_image_name)
    assert final_code == 200, f&#34;Target image not accessible after &#34; \
                              f&#34;concurrent operations: {final_code}&#34;

    # Part 3: Concurrent cleanup of all images
    cleanup_successful = 0

    def cleanup_single_image(image_name):
        delete_image(api_client, image_name, wait_timeout)
        return image_name

    with concurrent.futures.ThreadPoolExecutor(max_workers=len(successful_uploads)) as executor:  # NOQA
        cleanup_futures = [executor.submit(cleanup_single_image, name)
                           for name in successful_uploads]

        for _ in concurrent.futures.as_completed(cleanup_futures):
            cleanup_successful += 1

    assert cleanup_successful == len(successful_uploads), \
        f&#34;Cleanup failed: {cleanup_successful}/{len(successful_uploads)}&#34;</code></pre>
</details>
<div class="desc"><p>Test comprehensive concurrent image operations
Steps:
1. Test concurrent image uploads (multiple different images)
2. Test concurrent operations on same image (get, update, status checks)
3. Verify system handles all concurrency scenarios gracefully
4. Ensure data integrity and proper resource management</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_metadata_limits"><code class="name flex">
<span>def <span class="ident">test_image_metadata_limits</span></span>(<span>self, api_client, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p1
@pytest.mark.images
@pytest.mark.negative
def test_image_metadata_limits(self, api_client, unique_name, wait_timeout):
    &#34;&#34;&#34;
    Test image metadata handling with excessive data
    Steps:
    1. Create a valid image
    2. Test behavior with excessive metadata (many labels/annotations)
    3. Verify proper handling of metadata limits
    &#34;&#34;&#34;
    aligned_size = 1024 * 1024  # 1MB exactly

    with NamedTemporaryFile(&#34;wb&#34;, suffix=&#34;.raw&#34;) as f:
        f.write(b&#34;\x00&#34; * aligned_size)
        f.flush()

        # Create base image for metadata testing
        image_name = f&#34;metadata-test-{unique_name}&#34;

        resp = api_client.images.create_by_file(image_name, Path(f.name))
        assert resp.ok, f&#34;Failed to create base image: {resp.status_code}, {resp.text}&#34;

        # Test excessive metadata
        excessive_metadata = {
            &#34;labels&#34;: {f&#34;test-label-key-{i:03d}&#34;: f&#34;test-label-value&#34;
                                                  f&#34;-{i:03d}&#34; for i in range(20)},
            &#34;annotations&#34;: {
                f&#34;test.annotation.key/{i:03d}&#34;: &#34;x&#34; * 200 for i in range(10)
            }
        }

        code, data = api_client.images.update(image_name, dict(metadata=excessive_metadata))

        if code == 200:
            # Verify metadata was actually stored
            code, updated_data = api_client.images.get(image_name)
            assert code == 200, f&#34;Failed to retrieve updated image: {code}&#34;

        elif code in [400, 422, 413]:  # 413 = Request Entity Too Large
            print(f&#34;✓ Excessive metadata properly rejected with status: {code}&#34;)
        else:
            assert False, f&#34;Unexpected metadata response: {code}, {data}&#34;

        # Cleanup
        delete_image(api_client, image_name, wait_timeout)</code></pre>
</details>
<div class="desc"><p>Test image metadata handling with excessive data
Steps:
1. Create a valid image
2. Test behavior with excessive metadata (many labels/annotations)
3. Verify proper handling of metadata limits</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_metadata_operations"><code class="name flex">
<span>def <span class="ident">test_image_metadata_operations</span></span>(<span>self, api_client, fake_image_file, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p1
@pytest.mark.images
def test_image_metadata_operations(self, api_client, fake_image_file,
                                   unique_name, wait_timeout):
    &#34;&#34;&#34;
    Test comprehensive image metadata operations
    Steps:
    1. Create image and verify initial metadata
    2. Update labels, annotations, and description
    3. Test metadata persistence across operations
    4. Verify metadata validation and limits
    &#34;&#34;&#34;
    image_name = f&#34;metadata-test-{unique_name}&#34;

    # Create image
    resp = api_client.images.create_by_file(image_name, fake_image_file)
    assert resp.ok, f&#34;Failed to create image: {resp.status_code}, {resp.text}&#34;

    # Wait for initial processing
    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.images.get(image_name)
        if code == 200 and data.get(&#34;status&#34;, {}).get(&#34;progress&#34;) == 100:
            break
        sleep(2)

    # Test comprehensive metadata updates
    metadata_updates = {
        &#34;labels&#34;: {
            &#34;environment&#34;: &#34;test&#34;,
            &#34;team&#34;: &#34;qa&#34;,
            &#34;version&#34;: &#34;1.0.0&#34;,
            &#34;critical&#34;: &#34;false&#34;
        },
        &#34;annotations&#34;: {
            &#34;field.cattle.io/description&#34;: &#34;Test image for metadata operations&#34;,
            &#34;harvesterhci.io/imageId&#34;: image_name,
            &#34;custom.annotation/usage&#34;: &#34;automation-testing&#34;,
            &#34;created.by&#34;: &#34;harvester-e2e-tests&#34;
        }
    }

    code, data = api_client.images.update(image_name, dict(metadata=metadata_updates))
    assert 200 == code, f&#34;Failed to update metadata: {code}, {data}&#34;

    # Verify metadata was applied correctly
    code, data = api_client.images.get(image_name)
    assert 200 == code, (code, data)

    metadata = data[&#34;metadata&#34;]
    for field, expected_pairs in metadata_updates.items():
        for key, expected_value in expected_pairs.items():
            actual_value = metadata.get(field, {}).get(key)
            assert actual_value == expected_value, f&#34;Metadata {field}.{key}: expected&#34;\
                                                   f&#34; {expected_value}, got {actual_value}&#34;

    # Test incremental metadata updates
    incremental_updates = {
        &#34;labels&#34;: {&#34;priority&#34;: &#34;high&#34;},
        &#34;annotations&#34;: {&#34;last.modified&#34;: &#34;2025-09-30&#34;}
    }

    code, data = api_client.images.update(image_name, dict(metadata=incremental_updates))
    assert 200 == code, f&#34;Failed incremental update: {code}, {data}&#34;

    # Verify both old and new metadata exist
    code, data = api_client.images.get(image_name)
    assert 200 == code, (code, data)

    metadata = data[&#34;metadata&#34;]
    assert metadata.get(&#34;labels&#34;, {}).get(&#34;environment&#34;) == &#34;test&#34;  # Old label
    assert metadata.get(&#34;labels&#34;, {}).get(&#34;priority&#34;) == &#34;high&#34;  # New label

    delete_image(api_client, image_name, wait_timeout)</code></pre>
</details>
<div class="desc"><p>Test comprehensive image metadata operations
Steps:
1. Create image and verify initial metadata
2. Update labels, annotations, and description
3. Test metadata persistence across operations
4. Verify metadata validation and limits</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_name_length_limits"><code class="name flex">
<span>def <span class="ident">test_image_name_length_limits</span></span>(<span>self, api_client, unique_name, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p1
@pytest.mark.images
@pytest.mark.negative
def test_image_name_length_limits(self, api_client, unique_name, wait_timeout):
    &#34;&#34;&#34;
    Test image creation with various name lengths
    Steps:
    1. Test failure with image name &gt; 63 characters (should fail)
    2. Test success with image name ≤ 63 characters (should pass)
    &#34;&#34;&#34;
    aligned_size = 1024 * 1024  # 1MB exactly

    with NamedTemporaryFile(&#34;wb&#34;, suffix=&#34;.raw&#34;) as f:
        f.write(b&#34;\x00&#34; * aligned_size)
        f.flush()

        # Test 1: Invalid long name (should FAIL)
        long_base = &#34;very-long-image-name-for-testing-kubernetes-name-length-limits-that-exceeds-maximum&#34;  # NOQA
        invalid_long_name = f&#34;{long_base}-{unique_name}&#34;

        resp = api_client.images.create_by_file(invalid_long_name, Path(f.name))
        assert not resp.ok, f&#34;Expected failure with long name but&#34; \
                            f&#34; got success: {resp.status_code}&#34;
        assert resp.status_code in [400, 422], f&#34;Expected 400/422 for &#34; \
                                               f&#34;invalid name, got: {resp.status_code}&#34;

        # Test 2: Valid name (should PASS)
        valid_base = &#34;valid-image-name&#34;
        valid_name = f&#34;{valid_base}-{unique_name}&#34;

        if len(valid_name) &gt; 63:
            valid_name = f&#34;{valid_base[:30]}-{unique_name}&#34;
        valid_name = valid_name.rstrip(&#39;-&#39;)

        resp = api_client.images.create_by_file(valid_name, Path(f.name))
        assert resp.ok, f&#34;Failed with valid name: {resp.status_code}, {resp.text}&#34;

        # Cleanup
        delete_image(api_client, valid_name, wait_timeout)</code></pre>
</details>
<div class="desc"><p>Test image creation with various name lengths
Steps:
1. Test failure with image name &gt; 63 characters (should fail)
2. Test success with image name ≤ 63 characters (should pass)</p></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_processing_performance"><code class="name flex">
<span>def <span class="ident">test_image_processing_performance</span></span>(<span>self, api_client, unique_name, image_size, wait_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p2
@pytest.mark.images
@pytest.mark.performance
@pytest.mark.parametrize(&#34;image_size&#34;, [&#34;10Mi&#34;, &#34;50Mi&#34;, &#34;200Mi&#34;])
def test_image_processing_performance(self, api_client, unique_name, image_size, wait_timeout):
    &#34;&#34;&#34;
    Test Harvester image processing performance (excluding network upload time)
    Steps:
    1. Upload image files of different sizes
    2. Measure processing time from upload completion to ready state
    3. Verify Harvester processing meets performance expectations
    &#34;&#34;&#34;

    # Processing thresholds (seconds) - time from upload complete to ready
    processing_thresholds = {&#34;10Mi&#34;: 10, &#34;50Mi&#34;: 20, &#34;200Mi&#34;: 40}
    size_bytes = int(image_size[:-2]) * 1024 * 1024

    if size_bytes % 512 != 0:
        size_bytes = ((size_bytes // 512) + 1) * 512

    with NamedTemporaryFile(&#34;wb&#34;, suffix=&#34;.raw&#34;) as f:
        f.seek(size_bytes - 1)
        f.write(b&#34;\x00&#34;)
        f.seek(0)

        image_name = f&#34;proc-perf-{unique_name}&#34;

        # Step 1: Upload (don&#39;t measure this time - it&#39;s network dependent)
        resp = api_client.images.create_by_file(image_name, Path(f.name))
        assert resp.ok, f&#34;Failed to upload {image_size} image: {resp.status_code}, {resp.text}&#34;

        # Step 2: Wait for upload to be acknowledged by Harvester
        initial_timeout = datetime.now() + timedelta(seconds=30)
        while initial_timeout &gt; datetime.now():
            code, data = api_client.images.get(image_name)
            if code == 200:
                status = data.get(&#34;status&#34;, {})
                if &#34;progress&#34; in status:
                    # Upload acknowledged, start measuring processing time
                    processing_start_time = time()
                    break
            sleep(1)
        else:
            raise AssertionError(f&#34;Image {image_name} not acknowledged by Harvester&#34;)

        # Step 3: Measure time from processing start to completion
        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.images.get(image_name)
            if code == 200 and data.get(&#34;status&#34;, {}).get(&#34;progress&#34;) == 100:
                processing_end_time = time()
                break
            sleep(2)
        else:
            raise AssertionError(f&#34;Image processing did not complete within {wait_timeout}s&#34;)

        # Step 4: Validate Harvester processing performance
        processing_time = processing_end_time - processing_start_time
        threshold = processing_thresholds[image_size]
        assert processing_time &lt; threshold, f&#34;Harvester processing took&#34; \
                                            f&#34; {processing_time:.2f}s, expected &lt; {threshold}s&#34;

        print(f&#34;Harvester Processing Performance:&#34;
              f&#34; {image_size} processed in {processing_time:.2f}s&#34;)

        delete_image(api_client, image_name, wait_timeout)</code></pre>
</details>
<div class="desc"><p>Test Harvester image processing performance (excluding network upload time)
Steps:
1. Upload image files of different sizes
2. Measure processing time from upload completion to ready state
3. Verify Harvester processing meets performance expectations</p></div>
</dd>
</dl>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork"><code class="flex name class">
<span>class <span class="ident">TestImageWithStorageNetwork</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.p0
@pytest.mark.smoke
@pytest.mark.images
@pytest.mark.settings
@pytest.mark.networks
@pytest.mark.skip_version_if(&#34;&lt; v1.0.3&#34;)
@pytest.mark.usefixtures(&#34;storage_network&#34;)
class TestImageWithStorageNetwork:
    @pytest.mark.dependency(name=&#34;create_image_by_file&#34;)
    def test_create_image_by_file(self, api_client, fake_image_file, unique_name):
        resp = api_client.images.create_by_file(unique_name, fake_image_file)
        assert resp.ok, f&#34;Fail to upload fake image with error: {resp.status_code}, {resp.text}&#34;

        code, data = api_client.images.get(unique_name)
        assert 200 == code, (code, data)
        assert unique_name == data[&#34;metadata&#34;][&#34;name&#34;], (code, data)

    @pytest.mark.dependency(depends=[&#34;create_image_by_file&#34;])
    def test_download_image(self, api_client, fake_image_file, tmp_path, unique_name):
        resp = api_client.images.download(unique_name)
        assert resp.ok, f&#34;Fail to download fake image with error: {resp.status_code}, {resp.text}&#34;

        filename = re.search(r&#39;filename=(\S+)&#39;, resp.headers.get(&#34;Content-Disposition&#34;))
        assert filename, f&#34;No filename info in the response header: {resp.headers}&#34;
        filename = filename.groups()[0]

        tmp_image_file = tmp_path / filename
        tmp_image_file.write_bytes(
            zlib.decompress(resp.content, 32+15) if &#34;.gz&#34; in filename else resp.content
        )
        assert filecmp.cmp(fake_image_file, tmp_image_file), (
            &#34;Contents of downloaded image is NOT identical to the fake image&#34;
        )

    @pytest.mark.dependency(depends=[&#34;create_image_by_file&#34;])
    def test_delete_image(self, api_client, unique_name, wait_timeout, sleep_timeout):
        code, data = api_client.images.delete(unique_name)
        assert 200 == code, (code, data)

        endtime = datetime.now() + timedelta(seconds=wait_timeout)
        while endtime &gt; datetime.now():
            code, data = api_client.images.get(unique_name)
            if code == 404:
                break
            sleep(sleep_timeout)
        else:
            raise AssertionError(
                f&#34;Fail to delete image {unique_name} with error: {code}, {data}&#34;
            )</code></pre>
</details>
<div class="desc"></div>
<h3>Class variables</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.pytestmark"><code class="name">var <span class="ident">pytestmark</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.test_create_image_by_file"><code class="name flex">
<span>def <span class="ident">test_create_image_by_file</span></span>(<span>self, api_client, fake_image_file, unique_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(name=&#34;create_image_by_file&#34;)
def test_create_image_by_file(self, api_client, fake_image_file, unique_name):
    resp = api_client.images.create_by_file(unique_name, fake_image_file)
    assert resp.ok, f&#34;Fail to upload fake image with error: {resp.status_code}, {resp.text}&#34;

    code, data = api_client.images.get(unique_name)
    assert 200 == code, (code, data)
    assert unique_name == data[&#34;metadata&#34;][&#34;name&#34;], (code, data)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.test_delete_image"><code class="name flex">
<span>def <span class="ident">test_delete_image</span></span>(<span>self, api_client, unique_name, wait_timeout, sleep_timeout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;create_image_by_file&#34;])
def test_delete_image(self, api_client, unique_name, wait_timeout, sleep_timeout):
    code, data = api_client.images.delete(unique_name)
    assert 200 == code, (code, data)

    endtime = datetime.now() + timedelta(seconds=wait_timeout)
    while endtime &gt; datetime.now():
        code, data = api_client.images.get(unique_name)
        if code == 404:
            break
        sleep(sleep_timeout)
    else:
        raise AssertionError(
            f&#34;Fail to delete image {unique_name} with error: {code}, {data}&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.test_download_image"><code class="name flex">
<span>def <span class="ident">test_download_image</span></span>(<span>self, api_client, fake_image_file, tmp_path, unique_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.dependency(depends=[&#34;create_image_by_file&#34;])
def test_download_image(self, api_client, fake_image_file, tmp_path, unique_name):
    resp = api_client.images.download(unique_name)
    assert resp.ok, f&#34;Fail to download fake image with error: {resp.status_code}, {resp.text}&#34;

    filename = re.search(r&#39;filename=(\S+)&#39;, resp.headers.get(&#34;Content-Disposition&#34;))
    assert filename, f&#34;No filename info in the response header: {resp.headers}&#34;
    filename = filename.groups()[0]

    tmp_image_file = tmp_path / filename
    tmp_image_file.write_bytes(
        zlib.decompress(resp.content, 32+15) if &#34;.gz&#34; in filename else resp.content
    )
    assert filecmp.cmp(fake_image_file, tmp_image_file), (
        &#34;Contents of downloaded image is NOT identical to the fake image&#34;
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="harvester_e2e_tests.integrations" href="index.html">harvester_e2e_tests.integrations</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.cluster_network" href="#harvester_e2e_tests.integrations.test_1_images.cluster_network">cluster_network</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.create_image_url" href="#harvester_e2e_tests.integrations.test_1_images.create_image_url">create_image_url</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.delete_image" href="#harvester_e2e_tests.integrations.test_1_images.delete_image">delete_image</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.delete_volume" href="#harvester_e2e_tests.integrations.test_1_images.delete_volume">delete_volume</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.export_storage_class" href="#harvester_e2e_tests.integrations.test_1_images.export_storage_class">export_storage_class</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.fake_invalid_image_file" href="#harvester_e2e_tests.integrations.test_1_images.fake_invalid_image_file">fake_invalid_image_file</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.get_image" href="#harvester_e2e_tests.integrations.test_1_images.get_image">get_image</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.image_info" href="#harvester_e2e_tests.integrations.test_1_images.image_info">image_info</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.storage_network" href="#harvester_e2e_tests.integrations.test_1_images.storage_network">storage_network</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.vlan_cidr" href="#harvester_e2e_tests.integrations.test_1_images.vlan_cidr">vlan_cidr</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="harvester_e2e_tests.integrations.test_1_images.TestBackendImages" href="#harvester_e2e_tests.integrations.test_1_images.TestBackendImages">TestBackendImages</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.pytestmark" href="#harvester_e2e_tests.integrations.test_1_images.TestBackendImages.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_create_image_from_volume" href="#harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_create_image_from_volume">test_create_image_from_volume</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_create_image_url" href="#harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_create_image_url">test_create_image_url</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_create_invalid_file" href="#harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_create_invalid_file">test_create_invalid_file</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_delete_image_recreate" href="#harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_delete_image_recreate">test_delete_image_recreate</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_edit_image_in_use" href="#harvester_e2e_tests.integrations.test_1_images.TestBackendImages.test_edit_image_in_use">test_edit_image_in_use</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements" href="#harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements">TestImageEnhancements</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_checksum_validation" href="#harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_checksum_validation">test_image_checksum_validation</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_complete_lifecycle" href="#harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_complete_lifecycle">test_image_complete_lifecycle</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_concurrent_operations" href="#harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_concurrent_operations">test_image_concurrent_operations</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_metadata_limits" href="#harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_metadata_limits">test_image_metadata_limits</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_metadata_operations" href="#harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_metadata_operations">test_image_metadata_operations</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_name_length_limits" href="#harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_name_length_limits">test_image_name_length_limits</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_processing_performance" href="#harvester_e2e_tests.integrations.test_1_images.TestImageEnhancements.test_image_processing_performance">test_image_processing_performance</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork" href="#harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork">TestImageWithStorageNetwork</a></code></h4>
<ul class="">
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.pytestmark" href="#harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.pytestmark">pytestmark</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.test_create_image_by_file" href="#harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.test_create_image_by_file">test_create_image_by_file</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.test_delete_image" href="#harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.test_delete_image">test_delete_image</a></code></li>
<li><code><a title="harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.test_download_image" href="#harvester_e2e_tests.integrations.test_1_images.TestImageWithStorageNetwork.test_download_image">test_download_image</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
