<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hosts on Harvester manual test cases</title>
    <link>https://harvester.github.io/tests/manual/hosts/</link>
    <description>Recent content in Hosts on Harvester manual test cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://harvester.github.io/tests/manual/hosts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Add/remove disk to Host config</title>
      <link>https://harvester.github.io/tests/manual/hosts/1623-add-disk-to-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1623-add-disk-to-host/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1623&#34;&gt;#1623&lt;/a&gt; Unable to add additional disks to host config&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Add Disk that isn&amp;rsquo;t assigned to host&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Head to &amp;ldquo;Hosts&amp;rdquo; page&lt;/li&gt;&#xA;&lt;li&gt;Click &amp;ldquo;Edit Config&amp;rdquo; on a node and switch to &amp;ldquo;Disks&amp;rdquo; tab&lt;/li&gt;&#xA;&lt;li&gt;Validate: Open dropdown and see no disks&lt;/li&gt;&#xA;&lt;li&gt;Attach a disk on that node&lt;/li&gt;&#xA;&lt;li&gt;Validate: Open dropdown and see some disks&lt;/li&gt;&#xA;&lt;li&gt;Verify that host shows new disk as available storage and Longhorn is showing new schedulable space&lt;/li&gt;&#xA;&lt;li&gt;Detach a disk on that node&lt;/li&gt;&#xA;&lt;li&gt;Validate: Open dropdown and see no disks&lt;/li&gt;&#xA;&lt;li&gt;Verify that host shows new disk as available storage and Longhorn is showing new schedulable space&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Disk space should show appropriately&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/83787952/146289651-3c8b8da7-5ba1-4a15-aa4f-32f24af4b8dc.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Agent Node should not rely on specific master Node</title>
      <link>https://harvester.github.io/tests/manual/hosts/agent_node_connectivity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/agent_node_connectivity/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1521&#34;&gt;https://github.com/harvester/harvester/issues/1521&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;verify-items&#34;&gt;Verify Items&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Agent Node should keep connection when any master Node is down&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;case-agent-nodes-connecting-status&#34;&gt;Case: Agent Node&amp;rsquo;s connecting status&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with 4 nodes which joining node MUST join by VIP (point &lt;code&gt;server-url&lt;/code&gt; to use VIP)&lt;/li&gt;&#xA;&lt;li&gt;Make sure all nodes are ready&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Login to dashboard, check host &lt;strong&gt;state&lt;/strong&gt; become &lt;code&gt;Active&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;SSH to the 1st node, run command &lt;code&gt;kubectl get node&lt;/code&gt; to check all &lt;strong&gt;STATUS&lt;/strong&gt; should be &lt;code&gt;Ready&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;SSH to agent nodes which &lt;strong&gt;ROLES&lt;/strong&gt; IS &lt;code&gt;&amp;lt;none&amp;gt;&lt;/code&gt; in &lt;strong&gt;Step 2i&lt;/strong&gt;&amp;rsquo;s output&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Output should contains VIP in the server URL, by run command &lt;code&gt;cat /etc/rancher/rke2/config.yaml.d/90-harvester-vip.yaml&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Output should contain the line &lt;code&gt;server: https://127.0.0.1:6443&lt;/code&gt;, by run command &lt;code&gt;cat /var/lib/rancher/rke2/agent/kubelet.kubeconfig&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Output should contain the line &lt;code&gt;server: https://127.0.0.1:6443&lt;/code&gt;, by run command &lt;code&gt;cat /var/lib/rancher/rke2/agent/kubeproxy.kubeconfig&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;SSH to server nodes which &lt;strong&gt;ROLES&lt;/strong&gt; contains &lt;code&gt;control-plane&lt;/code&gt; in &lt;strong&gt;Step 2i&lt;/strong&gt;&amp;rsquo;s output&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Check file should not exist in the path &lt;code&gt;/etc/rancher/rke2/config.yaml.d/90-harvester-vip.yaml&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Shut down a server node, check following things&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Host &lt;strong&gt;State&lt;/strong&gt; should not be &lt;code&gt;Active&lt;/code&gt; in dashboard&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Node &lt;strong&gt;STATUS&lt;/strong&gt; should be &lt;code&gt;NotReady&lt;/code&gt; in the command output of  &lt;code&gt;kubectl get node&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;STATUS&lt;/strong&gt; of agent nodes should be &lt;code&gt;Ready&lt;/code&gt; in the command output of  &lt;code&gt;kubectl get node&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Power on the server node, wait until it back to cluster&lt;/li&gt;&#xA;&lt;li&gt;repeat &lt;strong&gt;Step 5-6&lt;/strong&gt; for other server nodes&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Attach unpartitioned NVMe disks to host</title>
      <link>https://harvester.github.io/tests/manual/hosts/attach-unpartitioned-nvme-disks-to-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/attach-unpartitioned-nvme-disks-to-host/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1414&#34;&gt;#1414&lt;/a&gt; Adding unpartitioned NVMe disks fails&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Storage&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Use &lt;code&gt;qemu-img create -f qcow2&lt;/code&gt; command to create three disk image locally&lt;/li&gt;&#xA;&lt;li&gt;Shutdown target node VM machine&lt;/li&gt;&#xA;&lt;li&gt;Directly edit VM xml content in virt manager page&lt;/li&gt;&#xA;&lt;li&gt;Add &lt;!-- raw HTML omitted --&gt; to the first line&lt;/li&gt;&#xA;&lt;li&gt;Add the following line before the end of &lt;!-- raw HTML omitted --&gt; quote&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;qemu:commandline&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme301.img,if=none,id=D22&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;-device&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;nvme,drive=D22,serial=1234&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme302.img,if=none,id=D23&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;-device&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;nvme,drive=D23,serial=1235&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme303.img,if=none,id=D24&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;-device&amp;#34;/&amp;gt;&#xA;    &amp;lt;qemu:arg value=&amp;#34;nvme,drive=D24,serial=1236&amp;#34;/&amp;gt;&#xA;  &amp;lt;/qemu:commandline&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol&gt;&#xA;&lt;li&gt;Start VM node&lt;/li&gt;&#xA;&lt;li&gt;Open Host page and edit the host config&lt;/li&gt;&#xA;&lt;li&gt;Click Disk -&amp;gt; Add disk to select available nvme disks&lt;/li&gt;&#xA;&lt;li&gt;Check disk can be mounted with &lt;code&gt;schedulable&lt;/code&gt; and &lt;code&gt;running status&lt;/code&gt; in edit node disk page&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Automatically get VIP during PXE installation</title>
      <link>https://harvester.github.io/tests/manual/hosts/1410-pxe-installation-automatically-get-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1410-pxe-installation-automatically-get-vip/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1410&#34;&gt;#1410&lt;/a&gt; Support getting VIP automatically during PXE boot installation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Comment &lt;code&gt;vip&lt;/code&gt; and &lt;code&gt;vip_hw_addr&lt;/code&gt; in &lt;code&gt;ipxe-examples/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Start vagrant-pxe-harvester&lt;/li&gt;&#xA;&lt;li&gt;Run &lt;code&gt;kubectl get cm -n harvester-system vip&lt;/code&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Check whether we can get &lt;code&gt;ip&lt;/code&gt; and &lt;code&gt;hwAddress&lt;/code&gt; in it&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Run &lt;code&gt;ip a show harvester-mgmt&lt;/code&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Check whether there are two IPs in it and one is the vip.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;VIP should automatically be assigned&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Check crash dump when there&#39;s a kernel panic</title>
      <link>https://harvester.github.io/tests/manual/hosts/1357-kernel-panic-check-crash-dump/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1357-kernel-panic-check-crash-dump/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1357&#34;&gt;#1357&lt;/a&gt; Crash dump not written when kernel panic occurs&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Created new single node cluster with 16GB RAM&lt;/li&gt;&#xA;&lt;li&gt;Booted into debug mode from GRUB entry&lt;/li&gt;&#xA;&lt;li&gt;Created several VMs&lt;/li&gt;&#xA;&lt;li&gt;triggered kernel panic with &lt;code&gt;echo c &amp;gt;/proc/sysrq-trigger&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Waited for reboot&lt;/li&gt;&#xA;&lt;li&gt;Verified that dump was saved in &lt;code&gt;/var/crash&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;dump should be saved in &lt;code&gt;/var/crash&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>check detailed network status in host page</title>
      <link>https://harvester.github.io/tests/manual/hosts/check-detailed-network-status-in-host-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/check-detailed-network-status-in-host-page/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/531&#34;&gt;#531&lt;/a&gt; Better error messages when misconfiguring multiple nics&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Host&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable vlan cluster network setting and set a default network interface&lt;/li&gt;&#xA;&lt;li&gt;Wait a while for the setting take effect on all harvester nodes&lt;/li&gt;&#xA;&lt;li&gt;Click nodes on host page&lt;/li&gt;&#xA;&lt;li&gt;Check the network tab&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;On the Host view page, now we can see detailed network status including &lt;code&gt;Name&lt;/code&gt;, &lt;code&gt;Type&lt;/code&gt;, &lt;code&gt;IP Address&lt;/code&gt;, &lt;code&gt;Status&lt;/code&gt; etc..&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/141070311-55ec4382-d777-4289-91c7-cebe81db3356.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Check all network interface can display&lt;/li&gt;&#xA;&lt;li&gt;Check the &lt;code&gt;Name&lt;/code&gt;, &lt;code&gt;Type&lt;/code&gt;, &lt;code&gt;IP Address&lt;/code&gt;, &lt;code&gt;Status&lt;/code&gt; display correct values&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Check Longhorn volume mount point</title>
      <link>https://harvester.github.io/tests/manual/hosts/1667-check-longhorn-volume-mount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1667-check-longhorn-volume-mount/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1667&#34;&gt;#1667&lt;/a&gt; data partition is not mounted to the LH path properly&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester node in VM from ISO&lt;/li&gt;&#xA;&lt;li&gt;Check partitions with &lt;code&gt;lsblk -f&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify mount point of &lt;code&gt;/var/lib/longhorn&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Mount point should show &lt;code&gt;/var/lib/longhorn&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/83787952/146290004-0584f817-d9df-4f4d-9069-d3ed4199b30f.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Check redirect for editing server URL setting</title>
      <link>https://harvester.github.io/tests/manual/hosts/1489-redirect-for-server-url-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1489-redirect-for-server-url-setting/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1489&#34;&gt;#1489&lt;/a&gt; Edit Advanced Setting option server-url will redirect to inappropriate page&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install harvester&lt;/li&gt;&#xA;&lt;li&gt;Access harvester&lt;/li&gt;&#xA;&lt;li&gt;Edit server-url form settings&lt;/li&gt;&#xA;&lt;li&gt;Check server-url save, cancel, and back.&#xA;Additional context:&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/18737885/140492691-969380aa-dbed-4999-9e90-e589dd93e4e4.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;URL should stay the same when navigating&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Cluster with Witness Node</title>
      <link>https://harvester.github.io/tests/manual/hosts/3266-witness-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/3266-witness-node/</guid>
      <description>&lt;p&gt;Witness node is a lightweight node only runs &lt;strong&gt;etcd&lt;/strong&gt; which is not schedulable and also not for workloads. The main use case is to form a quorum with the other 2 nodes.&lt;/p&gt;&#xA;&lt;p&gt;Kubernetes need at least 3 &lt;strong&gt;etcd&lt;/strong&gt; nodes to form a quorum, so Harvester also suggests using at least 3 nodes with similar hardware spec. This witness node feature aims for the edge case that user only have 2 powerful + 1 lightweight nodes thus helping benefit both cost and high availability.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Delete Host (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/delete-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/delete-host/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Navigate to the Hosts page and select the node&lt;/li&gt;&#xA;&lt;li&gt;Click Delete&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;SSH to the node and check the nodes has components deleted.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Delete host that has VMs on it</title>
      <link>https://harvester.github.io/tests/manual/hosts/delete-host-with-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/delete-host-with-vm/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Navigate to the Hosts page and select the node&lt;/li&gt;&#xA;&lt;li&gt;Click Delete&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;An alert message should appear.&lt;/li&gt;&#xA;&lt;li&gt;If VM exists it should stop user to delete the node or move VM to other node.&lt;/li&gt;&#xA;&lt;li&gt;If VM is getting moved to another node and there is no space, it should stop user to delete the node.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;existing-bugs&#34;&gt;Existing bugs&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harvester/harvester/issues/1004&#34;&gt;https://github.com/harvester/harvester/issues/1004&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Disk can only be added once on UI</title>
      <link>https://harvester.github.io/tests/manual/hosts/add_disk_on_ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/add_disk_on_ui/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1608&#34;&gt;https://github.com/harvester/harvester/issues/1608&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;verify-items&#34;&gt;Verify Items&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;NVMe disk can only be added once on UI&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;case-add-new-nvme-disk-on-dashboard-ui&#34;&gt;Case: add new NVMe disk on dashboard UI&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with 2 nodes&lt;/li&gt;&#xA;&lt;li&gt;Power off 2nd node&lt;/li&gt;&#xA;&lt;li&gt;Update VM&amp;rsquo;s xml definition (by using &lt;code&gt;virsh edit&lt;/code&gt; or virt-manager)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Create &lt;strong&gt;nvme.img&lt;/strong&gt; block: &lt;code&gt;dd if=/dev/zero of=/var/lib/libvirt/images/nvme.img bs=1M count=4096&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;change owner &lt;code&gt;chown qemu:qemu /var/lib/libvirt/images/nvme.img&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;update &lt;code&gt;&amp;lt;domain type=&amp;quot;kvm&amp;quot;&amp;gt;&lt;/code&gt; to &lt;code&gt;&amp;lt;domain type=&amp;quot;kvm&amp;quot; xmlns:qemu=&amp;quot;http://libvirt.org/schemas/domain/qemu/1.0&amp;quot;&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;append xml node into &lt;strong&gt;domain&lt;/strong&gt; as below:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;qemu:commandline&amp;gt;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;qemu:arg&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;value=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-drive&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;qemu:arg&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;value=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;file=/var/lib/libvirt/images/nvme.img,if=none,id=D22,format=raw&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;qemu:arg&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;value=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-device&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;qemu:arg&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;value=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nvme,drive=D22,serial=1234&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/qemu:commandline&amp;gt;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;&#xA;&lt;li&gt;Power on 2nd node&lt;/li&gt;&#xA;&lt;li&gt;login to dashboard, then click &lt;code&gt;Edit Config&lt;/code&gt; on 2nd node&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;strong&gt;Disks&lt;/strong&gt; tab, then add the NVMe disk from the drop-down list of &lt;strong&gt;Add Disk&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;The NVMe disk should disappear from the drop-down list&lt;/li&gt;&#xA;&lt;li&gt;Cick &lt;strong&gt;Save&lt;/strong&gt;, &lt;code&gt;Edit Config&lt;/code&gt; on 2nd node again&lt;/li&gt;&#xA;&lt;li&gt;The NVMe disk should not able to be added again&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/145861027-2b7575c8-a467-43eb-99ca-28a7c0026dba.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/145861276-d9cab21a-7c82-4287-af2f-3b60fab11a3f.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Disk devices used for VM storage should be globally configurable</title>
      <link>https://harvester.github.io/tests/manual/hosts/disk-devices-used-for-vm-storage-globally-configurable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/disk-devices-used-for-vm-storage-globally-configurable/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Related issue: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1241&#34;&gt;#1241&lt;/a&gt; Disk devices used for VM storage should be globally configurable&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Related issue: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1382&#34;&gt;#1382&lt;/a&gt; Exclude OS root disk and partitions on forced GPT partition&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Related issue: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1599&#34;&gt;#1599&lt;/a&gt; Extra disk auto provision from installation may cause NDM can&amp;rsquo;t find a valid longhorn node to provision&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Storage&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;test-scenarios&#34;&gt;Test Scenarios&lt;/h2&gt;&#xA;&lt;p&gt;(Checked means verification &lt;code&gt;PASS&lt;/code&gt;)&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;BIOS&lt;/code&gt; firmware + &lt;code&gt;No MBR&lt;/code&gt; (Default) + Auto disk` provisioning config&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;BIOS&lt;/code&gt; firmware + &lt;code&gt;MBR&lt;/code&gt; + Auto disk provisioning config&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;UEFI&lt;/code&gt; firmware + &lt;code&gt;GPT&lt;/code&gt; (Default) +  Auto disk provisioning config&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;BIOS&lt;/code&gt; firmware + &lt;code&gt;GPT&lt;/code&gt; (Default) +Auto Provisioning on harvester-config&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Scenario 1&lt;/strong&gt;:&#xA;Node type: &lt;code&gt;Create&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Download host YAML</title>
      <link>https://harvester.github.io/tests/manual/hosts/download-host-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/download-host-yaml/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Navigate to the Hosts page and select the node&lt;/li&gt;&#xA;&lt;li&gt;Click Download Yaml&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The Yaml should get downloaded.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Edit Config (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/edit-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/edit-config/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Navigate to the Hosts page and select the node&lt;/li&gt;&#xA;&lt;li&gt;Click edit config.&lt;/li&gt;&#xA;&lt;li&gt;Add description and other details&lt;/li&gt;&#xA;&lt;li&gt;Try to modify the network config&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The edited values should be saved and reflected on the page.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Edit Config YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/edit-config-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/edit-config-yaml/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Navigate to the Hosts page and select the node&lt;/li&gt;&#xA;&lt;li&gt;Click edit config through YAML.&lt;/li&gt;&#xA;&lt;li&gt;Add description and other details&lt;/li&gt;&#xA;&lt;li&gt;Try to modify the network config&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The edited values should be saved and reflected on the page.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Host list should display the disk error message on failure</title>
      <link>https://harvester.github.io/tests/manual/hosts/host-list-should-display-disk-error-message-on-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/host-list-should-display-disk-error-message-on-failure/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issue: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1167&#34;&gt;#1167&lt;/a&gt; Host list should display the disk error message on table&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Storage&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Shutdown existing node vm machine&lt;/li&gt;&#xA;&lt;li&gt;Run &amp;ldquo;qemu-img create&amp;rdquo; command to make a nvme.img&lt;/li&gt;&#xA;&lt;li&gt;Edit quem/kvm xml setting to attach the nvme image&lt;/li&gt;&#xA;&lt;li&gt;Start VM&lt;/li&gt;&#xA;&lt;li&gt;Open hostpage and edit your target node config&lt;/li&gt;&#xA;&lt;li&gt;Add the new nvme disk&lt;/li&gt;&#xA;&lt;li&gt;Shutdown VM&lt;/li&gt;&#xA;&lt;li&gt;Remove the attach device setting in Vï¼­ xml file&lt;/li&gt;&#xA;&lt;li&gt;Start VM&lt;/li&gt;&#xA;&lt;li&gt;Open Host page, the targe node will show warning with unready and unscheduable disk exists&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;If host encounter disk ready or schedule failure, on host page the &amp;ldquo;disk state&amp;rdquo; will show &lt;strong&gt;warning&lt;/strong&gt;&#xA;With a hover tip &amp;ldquo;&lt;strong&gt;Host have unready or unschedulable disks&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/138687164-877422a0-d33b-4e26-9c0b-d52b8f4e6995.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maintenance mode for host with multiple VMs</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-multiple-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-multiple-vm/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Put host in maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Migrate VMs&lt;/li&gt;&#xA;&lt;li&gt;Wait for VMs to migrate&lt;/li&gt;&#xA;&lt;li&gt;Wait for any vms to migrate off&lt;/li&gt;&#xA;&lt;li&gt;Do health check on VMs&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Host should start to go into maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Any VMs should migrate off&lt;/li&gt;&#xA;&lt;li&gt;Host should go into maintenance mode&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Maintenance mode for host with one VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-one-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-one-vm/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Put host in maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Migrate VMs&lt;/li&gt;&#xA;&lt;li&gt;Wait for VMs to migrate&lt;/li&gt;&#xA;&lt;li&gt;Wait for any vms to migrate off&lt;/li&gt;&#xA;&lt;li&gt;Do health check on VMs&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Host should start to go into maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Any VMs should migrate off&lt;/li&gt;&#xA;&lt;li&gt;Host should go into maintenance mode&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Maintenance mode on node with no vms (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-no-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-no-vm/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Put host in maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Wait for host to go from entering maintenance mode to maintenance mode.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Host should start to go into maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Host should go into maintenance mode&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Migrate back VMs that were on host after taking host out of maintenance mode</title>
      <link>https://harvester.github.io/tests/manual/hosts/q-maintenance-mode-migrate-back-vms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/q-maintenance-mode-migrate-back-vms/</guid>
      <description>&lt;h3 id=&#34;prerequisite&#34;&gt;Prerequisite:&lt;/h3&gt;&#xA;&lt;p&gt;Have a Harvester cluster with at least 2 nodes setup.&lt;/p&gt;&#xA;&lt;h3 id=&#34;test-steps&#34;&gt;Test Steps:&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Create a vm with node selector lets say node-1.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;And&lt;/strong&gt; Create a vm without node selector on node-1.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;AND&lt;/strong&gt; Write some data into both the VMs.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Put the host node-1 into maintenance mode.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; All the Vms on node-1 should be migrated to other nodes or the node should show warning that the vm with node selector can&amp;rsquo;t migrate.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Move Longhorn storage to another partition</title>
      <link>https://harvester.github.io/tests/manual/hosts/move-longhorn-storage-to-another-partition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/move-longhorn-storage-to-another-partition/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issue: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1316&#34;&gt;#1316&lt;/a&gt; Move Longhorn storage to another partition&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Storage&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;test-scenarios&#34;&gt;Test Scenarios&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/148171176-5dfe439b-8f61-484b-8c16-9c0236a5c1f2.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Case 1: UEFI + GPT (Disk &amp;lt; MBR Limit)&lt;/li&gt;&#xA;&lt;li&gt;Case 2: BIOS + No MBR (Disk &amp;lt; MBR Limit)&lt;/li&gt;&#xA;&lt;li&gt;Case 3: BIOS + Force MBR (Disk &amp;lt; MBR Limit)&lt;/li&gt;&#xA;&lt;li&gt;Case 4: BIOS + No MBR (Disk &amp;gt; MBR Limit)&lt;/li&gt;&#xA;&lt;li&gt;Case 5: BIOS + Force MBR (Disk &amp;gt; MBR Limit)&lt;/li&gt;&#xA;&lt;li&gt;Case 6: UEFI + GPT (Disk &amp;gt; MBR Limit)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Test Environment: 1 node harvester on local kvm machine&lt;/p&gt;</description>
    </item>
    <item>
      <title>Node Labeling for VM scheduling</title>
      <link>https://harvester.github.io/tests/manual/hosts/node_labeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/node_labeling/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1416&#34;&gt;https://github.com/harvester/harvester/issues/1416&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;verify-items&#34;&gt;Verify Items&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Host labels can be assigned during installation via config-create / config-join YAML.&lt;/li&gt;&#xA;&lt;li&gt;Host labels can be managed post installation via the Harvester UI.&lt;/li&gt;&#xA;&lt;li&gt;Host label information can be accessed in Rancher Virtualization Management UI.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;case-label-node-when-installing&#34;&gt;Case: Label node when installing&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with config file and &lt;a href=&#34;https://docs.harvesterhci.io/v1.0/install/harvester-configuration/#oslabels&#34;&gt;&lt;strong&gt;os.labels&lt;/strong&gt;&lt;/a&gt; option&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Host details then navigate to Labels in Config&lt;/li&gt;&#xA;&lt;li&gt;Check additional labels should be displayed&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;case-label-node-after-installed&#34;&gt;Case: Label node after installed&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with at least 2 nodes&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Host details then navigate to &lt;strong&gt;Labels&lt;/strong&gt; in Config&lt;/li&gt;&#xA;&lt;li&gt;Use &lt;strong&gt;edit config&lt;/strong&gt; to modify labels&lt;/li&gt;&#xA;&lt;li&gt;Reboot the Node and wait until its state become active&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Host details then Navigate to Labels in Config&lt;/li&gt;&#xA;&lt;li&gt;Check modified labels should be displayed&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;case-nodes-label-availability&#34;&gt;Case: Node&amp;rsquo;s Label availability&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with at least 2 nodes&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Host details then navigate to &lt;strong&gt;Labels&lt;/strong&gt; in Config&lt;/li&gt;&#xA;&lt;li&gt;Use &lt;strong&gt;edit config&lt;/strong&gt; to modify labels&lt;/li&gt;&#xA;&lt;li&gt;Reboot the Node and wait until its state become active&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Host details then Navigate to Labels in Config&lt;/li&gt;&#xA;&lt;li&gt;Check modified labels should be displayed&lt;/li&gt;&#xA;&lt;li&gt;Install Rancher with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Virtualization Management&lt;/em&gt; and import former created Harvester&lt;/li&gt;&#xA;&lt;li&gt;Wait Until state become &lt;strong&gt;Active&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Click &lt;em&gt;Name&lt;/em&gt; field to visit dashboard&lt;/li&gt;&#xA;&lt;li&gt;repeat step 2-7, and both compare from Harvester&amp;rsquo;s dashboard (accessing via Harvester&amp;rsquo;s VIP)&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Nodes with cordoned status should not be in VM migration list</title>
      <link>https://harvester.github.io/tests/manual/hosts/nodes-with-cordoned-status-should-not-be-in-vm-migration-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/nodes-with-cordoned-status-should-not-be-in-vm-migration-list/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1501&#34;&gt;#1501&lt;/a&gt; Nodes with cordoned status should not be in the selection list for VM migration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Host&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create multiple VMs on two of the nodes&lt;/li&gt;&#xA;&lt;li&gt;Set the idle node to cordoned state&lt;/li&gt;&#xA;&lt;li&gt;Edit any config of VM, click migrate&lt;/li&gt;&#xA;&lt;li&gt;Check the available node in the migration list&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;Node set in cordoned state will not show up in the available migration list&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/140716353-de5beb61-47c9-42bc-b553-21082f79267f.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/140715919-4e8794b6-105a-4a95-b177-1d7b0484ac08.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/140716077-4a61ecb1-e167-41a1-aa0b-ccc59814ff0a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Power down and power up the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-power-down-power-up-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-power-down-power-up-node/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create two vms on a cluster.&lt;/li&gt;&#xA;&lt;li&gt;Power down the node.&lt;/li&gt;&#xA;&lt;li&gt;Try to migrate a VM from the down node to active node.&lt;/li&gt;&#xA;&lt;li&gt;Leave the 2nd vm as it is.&lt;/li&gt;&#xA;&lt;li&gt;Power on the node&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The 1st VM should be migrated to other node on manually doing it.&lt;/li&gt;&#xA;&lt;li&gt;The 2nd VM should be accessible once the node is up.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;known-bugs&#34;&gt;Known bugs&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harvester/harvester/issues/982&#34;&gt;https://github.com/harvester/harvester/issues/982&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Power down the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-power-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-power-down-node/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create two vms on a cluster.&lt;/li&gt;&#xA;&lt;li&gt;Power down the node.&lt;/li&gt;&#xA;&lt;li&gt;Try to migrate a VM from the down node to active node.&lt;/li&gt;&#xA;&lt;li&gt;Leave the 2nd vm as it is.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The 1st VM should be migrated to other node on manually doing it.&lt;/li&gt;&#xA;&lt;li&gt;The 2nd VM should be recovered from the lost node&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Power node triggers VM reschedule</title>
      <link>https://harvester.github.io/tests/manual/hosts/vm_rescheduled_after_host_poweroff/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/vm_rescheduled_after_host_poweroff/</guid>
      <description>&lt;p&gt;Ref: N/A, legacy test case, VM is not migrated but rescheduled&lt;/p&gt;&#xA;&lt;h3 id=&#34;criteria&#34;&gt;Criteria&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; VM should created and started successfully&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Node should be unavailable after shutdown&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; VM should be restarted automatically&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with at least 2 nodes&lt;/li&gt;&#xA;&lt;li&gt;Create a image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create a VM &lt;code&gt;vm1&lt;/code&gt; and start it&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;vm1&lt;/code&gt; should started successfully&lt;/li&gt;&#xA;&lt;li&gt;Power off the node hosting &lt;code&gt;vm1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;the node should becomes unavailable on dashboard&lt;/li&gt;&#xA;&lt;li&gt;VM &lt;code&gt;vm1&lt;/code&gt; should be restarted automatically after &lt;code&gt;vm-force-reset-policy&lt;/code&gt; seconds&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>PXE instll without iso_url field</title>
      <link>https://harvester.github.io/tests/manual/hosts/1439-pxe-install-without-iso-url-field/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1439-pxe-install-without-iso-url-field/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1439&#34;&gt;#1439&lt;/a&gt; PXE boot installation doesn&amp;rsquo;t give an error if iso_url field is missing&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h2&gt;&#xA;&lt;p&gt;This is easiest to test with the vagrant setup at &lt;a href=&#34;https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester&#34;&gt;https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;edit &lt;a href=&#34;https://github.com/harvester/ipxe-examples/blob/main/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2#L27&#34;&gt;https://github.com/harvester/ipxe-examples/blob/main/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2#L27&lt;/a&gt; to be blank&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Run the vagrant &lt;code&gt;./setup.sh&lt;/code&gt; from the vagrant repo&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;You should get an error in the console for the VM when installing&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Reboot a cluster and check VIP</title>
      <link>https://harvester.github.io/tests/manual/hosts/1669-reboot-cluster-check-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1669-reboot-cluster-check-vip/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1669&#34;&gt;#1669&lt;/a&gt; Unable to access harvester VIP nor node IP after reboot or fully power cycle node machines (Intermittent)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable VLAN with NIC harvester-mgmt&lt;/li&gt;&#xA;&lt;li&gt;Create VLAN 1&lt;/li&gt;&#xA;&lt;li&gt;Disable VLAN&lt;/li&gt;&#xA;&lt;li&gt;Enable VLAN again&lt;/li&gt;&#xA;&lt;li&gt;shutdown node 3, 2, 1 server machine&lt;/li&gt;&#xA;&lt;li&gt;Wait for 15 minutes&lt;/li&gt;&#xA;&lt;li&gt;Power on node 1 server machine, wait for 20 seconds&lt;/li&gt;&#xA;&lt;li&gt;Power on node 2 server machine, wait for 20 seconds&lt;/li&gt;&#xA;&lt;li&gt;Power on node 3 server machine&lt;/li&gt;&#xA;&lt;li&gt;Check if you can access VIP and each node IP&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;VIP should load the page and show on every node in the terminal&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Reboot host that is in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-reboot-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-reboot-host/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;For Host that is in maintenance mode and turned on&lt;/li&gt;&#xA;&lt;li&gt;Reboot host&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Host should reboot&lt;/li&gt;&#xA;&lt;li&gt;Maintenance mode label in hosts list should go from yellow to red to yellow&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;known-bugs&#34;&gt;Known Bugs&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harvester/harvester/issues/1272&#34;&gt;https://github.com/harvester/harvester/issues/1272&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reboot host trigger VM migration</title>
      <link>https://harvester.github.io/tests/manual/hosts/vm_migrated_after_host_reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/vm_migrated_after_host_reboot/</guid>
      <description>&lt;p&gt;Ref: N/A, legacy test case&lt;/p&gt;&#xA;&lt;h3 id=&#34;criteria&#34;&gt;Criteria&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; VM should created and started successfully&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Node should be unavailable while rebooting&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; VM should be migrated to ohter node&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with at least 2 nodes&lt;/li&gt;&#xA;&lt;li&gt;Create a image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create a VM &lt;code&gt;vm1&lt;/code&gt; and start it&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;vm1&lt;/code&gt; should started successfully&lt;/li&gt;&#xA;&lt;li&gt;Reboot the node hosting &lt;code&gt;vm1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;the node should becomes unavailable on dashboard&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;vm1&lt;/code&gt; should be automatically migrated to another node&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Reboot node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-reboot-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-reboot-node/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create a vm on the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Reboot the node where the vm exists.&lt;/li&gt;&#xA;&lt;li&gt;Reboot the node where there is no vm&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;On rebooting the node, once the node is back up and Harvester is started, the host should become available on the cluster.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Recover cordon and maintenace node after harvester node machine reboot</title>
      <link>https://harvester.github.io/tests/manual/hosts/recover-cordon-or-maintenace-node-after-harvester-node-machine-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/recover-cordon-or-maintenace-node-after-harvester-node-machine-reboot/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1493&#34;&gt;#1493&lt;/a&gt; When hosts are stuck in maintenance mode and the cluster is unstable you can&amp;rsquo;t access the UI&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Host&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create 3 virtual machine on 3 harvester nodes&lt;/li&gt;&#xA;&lt;li&gt;Cordon 1st and 2nd node,&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/141106858-cdfb35f3-50af-48d0-b776-1f1cc5dfcedc.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Enable maintenance mode on 1st and 2nd node&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/141106968-e4d7a6be-6c60-4771-aabd-8df0ccafe252.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;We can&amp;rsquo;t cordon and enable maintenance node on the remaining node&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/141107044-774166b8-117e-4635-b8a2-eeedb65e48fc.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Reboot 1st and 2nd node bare machine&lt;/li&gt;&#xA;&lt;li&gt;Wait for harvester machine back to service&lt;/li&gt;&#xA;&lt;li&gt;Login dashboard&lt;/li&gt;&#xA;&lt;li&gt;Disable maintenance mode on 1st and 2nd node&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Cordon node and enter maintenance mode, after machine reboot, user can login harvester dashboard.&lt;/li&gt;&#xA;&lt;li&gt;Node remain it&amp;rsquo;s original status&lt;/li&gt;&#xA;&lt;li&gt;Can disable/uncordon node, it can back to original status&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/141111698-64d9d648-9018-4c14-8828-539f6e44361e.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Remove a management node from a 3 nodes cluster and add it back to the cluster by reinstalling it</title>
      <link>https://harvester.github.io/tests/manual/hosts/remove-management-node-then-reinstall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/remove-management-node-then-reinstall/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;From a HA cluster with 3 nodes&lt;/li&gt;&#xA;&lt;li&gt;Delete one of the nodes after the node promotion(all 3 nodes are management nodes)&lt;/li&gt;&#xA;&lt;li&gt;Reinstall the removed node with the same node name and IP&lt;/li&gt;&#xA;&lt;li&gt;The rejoined node will be promoted to master automatically&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The removed node should be able to rejoin the cluster without issues&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Purpose is to cover this scenario: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1040&#34;&gt;https://github.com/harvester/harvester/issues/1040&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the job promotion with the command kubectl get jobs -n harvester-system&lt;/li&gt;&#xA;&lt;li&gt;If a node is stuck in the removing status, you likely face to this issue, execute this command as workaround: &lt;code&gt;kubectl get node -o name &amp;lt;nodename&amp;gt; | xargs -i kubectl patch {} -p &#39;{&amp;quot;metadata&amp;quot;:{&amp;quot;finalizers&amp;quot;:[]}}&#39; --type=merge&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Remove unavailable node with VMs on it</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-remove-unavailable-node-with-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-remove-unavailable-node-with-vm/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create VMs on a host.&lt;/li&gt;&#xA;&lt;li&gt;Turn off Host&lt;/li&gt;&#xA;&lt;li&gt;Remove Host from hosts list&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;VMs should migrate to new host&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;known-bugs&#34;&gt;Known Bugs&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harvester/harvester/issues/983&#34;&gt;https://github.com/harvester/harvester/issues/983&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Set maintenance mode on the last available node shouldn&#39;t be allowed</title>
      <link>https://harvester.github.io/tests/manual/hosts/set-maintenance-mode-on-the-last-available-node-shouldnt-be-allowed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/set-maintenance-mode-on-the-last-available-node-shouldnt-be-allowed/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1014&#34;&gt;#1014&lt;/a&gt; Trying to set maintenance mode on the last available node shouldn&amp;rsquo;t be allowed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Host&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create 3 vms located on node2 and node3&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/140375836-50cfdb48-a37f-4d86-b931-04983e837cdc.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Open host page&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Set node 3 into maintenance mode&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Wait for virtual machine migrate to node 2&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Set node 2 into maintenance mode&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;wait for virtual machine migrate to node 1&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Set node 2 into maintenance mode&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;Within 3 nodes and 3 virtual machines testing environment.&#xA;Set maintenance from node 3 -&amp;gt; node 2 -&amp;gt; node 1 in sequence&#xA;The final remaining node will prompt &lt;code&gt;can&#39;t enable maintenance in last available node &lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Shut down host in maintenance mode and verify label change</title>
      <link>https://harvester.github.io/tests/manual/hosts/1272-shutdown-host-in-maintenance-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1272-shutdown-host-in-maintenance-mode/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1272&#34;&gt;#1272&lt;/a&gt; Shut down a node with maintenance mode should show red label&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Open host page&lt;/li&gt;&#xA;&lt;li&gt;Set a node to maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Turn off host vm of the node&lt;/li&gt;&#xA;&lt;li&gt;Check node status&lt;/li&gt;&#xA;&lt;li&gt;Turn on host&lt;/li&gt;&#xA;&lt;li&gt;Check node status&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The node should go into maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;The node label should go red&lt;/li&gt;&#xA;&lt;li&gt;When turned on the node status should go back to yellow&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Shut down host then delete hosted VM</title>
      <link>https://harvester.github.io/tests/manual/hosts/delete_vm_after_host_shutdown/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/delete_vm_after_host_shutdown/</guid>
      <description>&lt;p&gt;Ref: N/A, legacy test case&lt;/p&gt;&#xA;&lt;h3 id=&#34;criteria&#34;&gt;Criteria&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; VM should created and started successfully&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Node should be unavailable after shutdown&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; VM should able to be deleted&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with at least 2 nodes&lt;/li&gt;&#xA;&lt;li&gt;Create a image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create a VM &lt;code&gt;vm1&lt;/code&gt; and start it&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;vm1&lt;/code&gt; should started successfully&lt;/li&gt;&#xA;&lt;li&gt;Power off the node hosting &lt;code&gt;vm1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;the node should becomes unavailable on dashboard&lt;/li&gt;&#xA;&lt;li&gt;Delete &lt;code&gt;vm1&lt;/code&gt;, &lt;code&gt;vm1&lt;/code&gt; should be deleted successfully&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Start Host in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-start-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-start-host/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;For Host that is in maintenance mode and turned off&lt;/li&gt;&#xA;&lt;li&gt;Start host&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Host should turn on&lt;/li&gt;&#xA;&lt;li&gt;Maintenance mode label in hosts list should go from red to yellow&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;known-bugs&#34;&gt;Known bugs&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harvester/harvester/issues/1272&#34;&gt;https://github.com/harvester/harvester/issues/1272&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Take host out of maintenance mode that has been rebooted(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-rebooted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-rebooted/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;For host in maintenance mode that has been rebooted take host out of maintenance mode&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Host should go to Active&lt;/li&gt;&#xA;&lt;li&gt;Label shbould go green&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Take host out of maintenance mode that has not been rebooted (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-not-rebooted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-not-rebooted/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;For host in maintenance mode that has not been rebooted take host out of maintenance mode&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Host should go to Active&lt;/li&gt;&#xA;&lt;li&gt;Label shbould go green&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Temporary network disruption</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-network-disruption/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-network-disruption/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create a vms on the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Disable network of a node for sometime. e.g. 5 sec, 5 mins&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;VM should be accessible after the network is up.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test NTP server timesync</title>
      <link>https://harvester.github.io/tests/manual/hosts/1535-test-ntp-timesync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/1535-test-ntp-timesync/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1535&#34;&gt;#1535&lt;/a&gt; NTP daemon in host OS&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h2&gt;&#xA;&lt;p&gt;This should be on at least a 3 node setup that has been running for several hours that had NTP servers setup during install&lt;/p&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;SSH into nodes and verify times are close&lt;/li&gt;&#xA;&lt;li&gt;Verify NTP is active with &lt;code&gt;sudo timedatectl status&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Times should be within a minute of each other&lt;/li&gt;&#xA;&lt;li&gt;NTP should show as active&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Turn off host that is in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-turn-off-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-turn-off-host/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Put host in maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Migrate VMs&lt;/li&gt;&#xA;&lt;li&gt;Wait for VMs to migrate&lt;/li&gt;&#xA;&lt;li&gt;Wait for any vms to migrate off&lt;/li&gt;&#xA;&lt;li&gt;Shut down Host&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Host should start to go into maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Any VMs should migrate off&lt;/li&gt;&#xA;&lt;li&gt;Host should go into maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;host should shut down&lt;/li&gt;&#xA;&lt;li&gt;Maintenance mode label in hosts list should go red&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;known-bugs&#34;&gt;Known bugs&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harvester/harvester/issues/1272&#34;&gt;https://github.com/harvester/harvester/issues/1272&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Verify Enabling maintenance mode</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-enabling-maintenance-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/verify-enabling-maintenance-mode/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Navigate to the Hosts page and select the node&lt;/li&gt;&#xA;&lt;li&gt;Click Maintenance Mode&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The existing VM should get migrated to other nodes.&lt;/li&gt;&#xA;&lt;li&gt;Verify the CRDs to see the maintenance mode is enabled.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Needs other test cases to be added&lt;/li&gt;&#xA;&lt;li&gt;If VM migration fails&lt;/li&gt;&#xA;&lt;li&gt;How does live migration work&lt;/li&gt;&#xA;&lt;li&gt;What happens if there are no schedulable resources on nodes&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Check CRDs on hosts&#xA;&lt;ul&gt;&#xA;&lt;li&gt;On going into maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;kubectl get virtualmachines &amp;ndash;all-namespaces&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Kubectl get virtualmachines/name -o yaml&#xA;&lt;ul&gt;&#xA;&lt;li&gt;On coming out of maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;kubectl get virtualmachines &amp;ndash;all-namespaces&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Kubectl get virtualmachines/name -o yaml&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Check that maintenance mode host isn&amp;rsquo;t schedulable&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Fully provision all nodes and try to create a VM&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;It should fail&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Migration with maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;What if migration gets stuck, can you cancel&lt;/li&gt;&#xA;&lt;li&gt;VMs going to different hosts&lt;/li&gt;&#xA;&lt;li&gt;Canceling maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;P1&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Put in maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Check migration of VMs&lt;/li&gt;&#xA;&lt;li&gt;Check status of VMs&lt;/li&gt;&#xA;&lt;li&gt;modify filesystem on VMs&lt;/li&gt;&#xA;&lt;li&gt;Check status of host&lt;/li&gt;&#xA;&lt;li&gt;Take host out of maintenance mode&lt;/li&gt;&#xA;&lt;li&gt;Check status of host&lt;/li&gt;&#xA;&lt;li&gt;Migrate VMs back to host&lt;/li&gt;&#xA;&lt;li&gt;Check filesystem&lt;/li&gt;&#xA;&lt;li&gt;Create new VMs on host&lt;/li&gt;&#xA;&lt;li&gt;Check status of VMs&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Verify the Filter on the Host page</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-filter-on-host-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/verify-filter-on-host-page/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Enter name of a host and verify the nodes get filtered out.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The edited name should be reflected on the host.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Verify the info of the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-node-info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/verify-node-info/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Navigate to the hosts tab and verify the following.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;State&lt;/li&gt;&#xA;&lt;li&gt;Name&lt;/li&gt;&#xA;&lt;li&gt;Host IP&lt;/li&gt;&#xA;&lt;li&gt;CPU&lt;/li&gt;&#xA;&lt;li&gt;Memory&lt;/li&gt;&#xA;&lt;li&gt;Storage Size&lt;/li&gt;&#xA;&lt;li&gt;Age&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;All the data/status shown on the page should be correct.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Verify the state for Powered down node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-verify-state-powered-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/hosts/negative-verify-state-powered-down-node/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Power down the node and check the state of the node in the cluster&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The node state should show unavilable&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
