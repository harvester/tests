<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Incoming Test Cases on Harvester manual test cases</title>
    <link>https://harvester.github.io/tests/manual/_incoming/</link>
    <description>Recent content in Incoming Test Cases on Harvester manual test cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://harvester.github.io/tests/manual/_incoming/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Adapt alertmanager to dedicated storage network</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2715_adapt_alertmanager_to_dedicated_storage_network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2715_adapt_alertmanager_to_dedicated_storage_network/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2715&#34;&gt;https://github.com/harvester/harvester/issues/2715&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;criteria&#34;&gt;criteria&lt;/h3&gt;&#xA;&lt;p&gt;PVCs (alertmanager/grafana/Prometheus) will attach back after dedicated storage network switched.&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Networks -&amp;gt; Cluster Networks/Configs&lt;/em&gt;, create Cluster Network named &lt;code&gt;vlan&lt;/code&gt;, create &lt;strong&gt;Network Config&lt;/strong&gt; for all nodes&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Advanced -&amp;gt; Settings&lt;/em&gt;, edit &lt;code&gt;storage-network&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Select &lt;code&gt;Enable&lt;/code&gt; then select &lt;code&gt;vlan&lt;/code&gt; as cluster network, fill in &lt;strong&gt;VLAN ID&lt;/strong&gt; and &lt;strong&gt;IP Range&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Wait until error message (displayed under &lt;em&gt;storage network&lt;/em&gt; setting) disappeared&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Monitoring &amp;amp; Logging -&amp;gt; Monitoring -&amp;gt; Configuration&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Dashboard of Prometheus Graph, Grafana and Altertmanager should able to access, and should contain old data.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Add backup-taget connection status</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2631_add_backup-taget_connection_status/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2631_add_backup-taget_connection_status/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2631&#34;&gt;https://github.com/harvester/harvester/issues/2631&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Verified this feature has been implemented.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/190369936-c07b0a5f-8685-4813-8108-1032caf09183.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-information&#34;&gt;Test Information&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Environment: &lt;strong&gt;qemu/KVM 2 nodes&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Harvester Version: &lt;strong&gt;master-032742f0-head&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ui-source&lt;/strong&gt; Option: &lt;strong&gt;Auto&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard then navigate to &lt;em&gt;Advanced/Settings&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Setup a invalid NFS/S3 backup-target, then click &lt;strong&gt;Test connection&lt;/strong&gt; button, error message should displayed&lt;/li&gt;&#xA;&lt;li&gt;Setup a valid NFS/S3 backup-target, then click &lt;strong&gt;Test connection&lt;/strong&gt; button, notify message should displayed&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Advanced/VM Backups&lt;/em&gt;, notify message should NOT displayed&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Advanced/Settings&lt;/em&gt; and stop the backup-target server, then navigate to &lt;em&gt;Advanced/VM Backups&lt;/em&gt;, error message should displayed&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Add extra disks by using raw disks</title>
      <link>https://harvester.github.io/tests/manual/_incoming/extra-disk-using-raw-disk/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/extra-disk-using-raw-disk/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Prepare a disk (with WWN) and attach it to the node.&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &amp;ldquo;Host&amp;rdquo; &amp;gt; &amp;ldquo;Edit Config&amp;rdquo; &amp;gt; &amp;ldquo;Disks&amp;rdquo; and open the dropdown menu &amp;ldquo;Add disks&amp;rdquo;.&lt;/li&gt;&#xA;&lt;li&gt;Choose a disk to add, e.g. &lt;code&gt;/dev/sda&lt;/code&gt; but not &lt;code&gt;/dev/sda1&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The raw disk shall be schedulable as a longhorn disk as a whole (without any partition).&lt;/li&gt;&#xA;&lt;li&gt;Ths raw disk shall be in &lt;code&gt;provisioned&lt;/code&gt; phase.&lt;/li&gt;&#xA;&lt;li&gt;Reboot the host and the disk shall be reattached and added back as a longhorn disk.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Add websocket disconnect notification</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2186_add_websocket_disconnect_notification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2186_add_websocket_disconnect_notification/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2186&#34;&gt;https://github.com/harvester/harvester/issues/2186&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/177529443-a9478e33-a955-4b48-8485-ab6eabbf3824.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with at least 2 nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard via Node IP&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Advanced/Settings&lt;/em&gt; and update &lt;strong&gt;ui-index&lt;/strong&gt; to &lt;code&gt;https://releases.rancher.com/harvester-ui/dashboard/release-harvester-v1.0/index.html&lt;/code&gt; and force refresh to make it applied.&lt;/li&gt;&#xA;&lt;li&gt;restart the Node which holding the IP&lt;/li&gt;&#xA;&lt;li&gt;Notification of websocket disconnected should appeared&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Alertmanager supports main stream receivers</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2521-alertmanager-supports-main-stream-receivers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2521-alertmanager-supports-main-stream-receivers/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2521&#34;&gt;#2521&lt;/a&gt; [FEATURE] Alertmanager supports main stream receivers&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Alter manager&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare another VM or machine have the same subnet with the Harvester&lt;/li&gt;&#xA;&lt;li&gt;Prepare a webhook server on the VM, reference to &lt;a href=&#34;https://github.com/w13915984028/harvester-develop-summary/blob/main/test-log-event-audit-with-webhook-server.md&#34;&gt;https://github.com/w13915984028/harvester-develop-summary/blob/main/test-log-event-audit-with-webhook-server.md&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;You may need to install python3 web package, refer to &lt;a href=&#34;https://webpy.org/install&#34;&gt;https://webpy.org/install&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Run &lt;code&gt;export PORT=8094&lt;/code&gt; on the webhook server VM&lt;/li&gt;&#xA;&lt;li&gt;Launch the webhook server &lt;code&gt;python3 simple-webhook-server.py&lt;/code&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;davidtclin@ubuntu-clean:~$ python3 simple-webhook-server.py&#xA;usage: export PORT=1234 to set http server port number as 1234&#xA;start a simple webhook server, PORT 8094 @ 2022-09-21 16:39:58.706792 &#xA;&#xA;http://0.0.0.0:8094/&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Access Harvester &lt;code&gt;Alertmanager Configs&lt;/code&gt; page in Monitoring&lt;/li&gt;&#xA;&lt;li&gt;Create an Alertmanager config&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/191452211-9892187b-4af2-4f73-8d34-19fedcd830b8.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Access the created Alertmanager config&lt;/li&gt;&#xA;&lt;li&gt;Click Add receiver&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/191452302-929fe806-6485-4837-be84-f2814c28d4ac.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Select webhook page&lt;/li&gt;&#xA;&lt;li&gt;Click Add webhook&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/191452393-de222b27-9d0a-4dc5-bed0-e9c449b95766.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Add `http://server_ip:8094 to the URL&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/191458438-98d3e36d-27af-4f26-aaf4-c910036ed124.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Access Harvester node&lt;/li&gt;&#xA;&lt;li&gt;Run &lt;code&gt;kubectl get alertmanagerconfig -A -o yaml&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the webhook server have applied to the &lt;code&gt;Altertmanagerconfig&lt;/code&gt; pod&lt;/li&gt;&#xA;&lt;li&gt;Check the webhook receiver can be created&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/191464505-d1a2d718-a742-4072-853a-a4c221abe97a.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Refer to HEP document for more details&#xA;&lt;a href=&#34;https://github.com/w13915984028/harvester/blob/fix2517/enhancements/20220720-alertmanager.md&#34;&gt;https://github.com/w13915984028/harvester/blob/fix2517/enhancements/20220720-alertmanager.md&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>All Namespace filtering in VM list</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2578-all-namespace-filtering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2578-all-namespace-filtering/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2578&#34;&gt;#2578&lt;/a&gt; [BUG] When first entering the harvester cluster from Virtualization Managements, some vm&amp;rsquo;s in namespace are not shown in the list&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;UI&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a harvester cluster&lt;/li&gt;&#xA;&lt;li&gt;Create a VM in the default namespace&lt;/li&gt;&#xA;&lt;li&gt;Creating a Namespace (eg: test-vm)&lt;/li&gt;&#xA;&lt;li&gt;Import the Harvester cluster in Rancher&lt;/li&gt;&#xA;&lt;li&gt;access to the harvester cluster from Virtualization Management&lt;/li&gt;&#xA;&lt;li&gt;click Virtual Machines tab&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;test-vm-1 should also be shown in the list&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/24985926/181211867-4f3889cd-a14e-463c-9a7f-0aee2d5f358e.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Auto provision lots of extra disks</title>
      <link>https://harvester.github.io/tests/manual/_incoming/large-amount-of-extra-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/large-amount-of-extra-disks/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;:warning: This is a heuristic test plan since real world race condition is hard to reproduce.&#xA;If you find any better alternative, feel free to update.&lt;/p&gt;&#xA;&lt;p&gt;This test is better to perform under QEMU/libvirt environment.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1718&#34;&gt;#1718&lt;/a&gt; [BUG] Automatic disk provisioning result in unusable ghost disks on NVMe drives&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Storage&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a harvester cluster and attach 10 or more extra disks (needs WWN so that they can be identified uniquely).&lt;/li&gt;&#xA;&lt;li&gt;Add &lt;a href=&#34;https://docs.harvesterhci.io/v1.0/settings/settings/#auto-disk-provision-paths-experimental&#34;&gt;&lt;code&gt;auto-disk-provision-paths&lt;/code&gt;&lt;/a&gt; setting and provide a value that matches all the disks added from previous step.&lt;/li&gt;&#xA;&lt;li&gt;Wait for minutes for the auto-provisioning process.&lt;/li&gt;&#xA;&lt;li&gt;Eventually, all disks matching the pattern should be partitioned, formatted and mounted successfully.&lt;/li&gt;&#xA;&lt;li&gt;Navigate to longhorn dashboard to see if each disk is successfully added and scheduled.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;A large amout of disks can be auto-provisioned simultaneously.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Boot installer under Legacy BIOS and UEFI</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2023-boot-installer-legacy-and-uefi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2023-boot-installer-legacy-and-uefi/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues &lt;a href=&#34;https://github.com/harvester/harvester/issues/2023&#34;&gt;#2023&lt;/a&gt; Legacy Iso for older servers&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;h3 id=&#34;bios-test&#34;&gt;BIOS Test&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Build &lt;a href=&#34;https://github.com/harvester/harvester-installer&#34;&gt;harvester-installer&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Boot build artifact using BIOS Legacy mode: &lt;code&gt;qemu-system-x86_64 -m 2048 -cdrom ../dist/artifacts/harvester-master-amd64&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that the installer boot process reaches the screen that says &amp;ldquo;Create New Cluster&amp;rdquo; or &amp;ldquo;Join existing cluster&amp;rdquo;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;uefi-test&#34;&gt;UEFI Test&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Build &lt;a href=&#34;https://github.com/harvester/harvester-installer&#34;&gt;harvester-installer&lt;/a&gt; (or use the same one from the BIOS Test, it&amp;rsquo;s a hybrid ISO)&lt;/li&gt;&#xA;&lt;li&gt;Boot build artifact using UEFI mode: &lt;code&gt;qemu-system-x86_64 -m 2048 -cdrom ../dist/artifacts/harvester-master-amd64 -bios /usr/share/qemu/ovmf-x86_64.bin&lt;/code&gt; (OVMF is a port of the UEFI firmware to qemu)&lt;/li&gt;&#xA;&lt;li&gt;Verify that the installer boot process reaches the screen that says &amp;ldquo;Create New Cluster&amp;rdquo; or &amp;ldquo;Join existing cluster&amp;rdquo;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Check can start VM after Harvester upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/start-vm-after-harvester-upgrade-complete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/start-vm-after-harvester-upgrade-complete/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2270&#34;&gt;#2270&lt;/a&gt; [BUG] Unable start VM after upgraded v1.0.1 to v1.0.2-rc2&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Harvester Upgrade&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare the previous stable Harvester release cluster&lt;/li&gt;&#xA;&lt;li&gt;Create image&lt;/li&gt;&#xA;&lt;li&gt;Enable Network and create VM&lt;/li&gt;&#xA;&lt;li&gt;Create several virtual machine&lt;/li&gt;&#xA;&lt;li&gt;Follow the &lt;a href=&#34;https://docs.harvesterhci.io/v1.0/upgrade/automatic/&#34;&gt;official document steps&lt;/a&gt; to prepare the online or offline upgrade&lt;/li&gt;&#xA;&lt;li&gt;Shutdown all virtual machines&lt;/li&gt;&#xA;&lt;li&gt;Start the upgrade&lt;/li&gt;&#xA;&lt;li&gt;Confirm all the upgrade process complete&lt;/li&gt;&#xA;&lt;li&gt;Start all the virtual machines&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;All virtual machine could be correctly started and work as expected&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Check conditions when stop/pause VM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1987-failure-message-in-stopping-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1987-failure-message-in-stopping-vm/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1987&#34;&gt;#1987&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;p&gt;Stop Request should not have failure message&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a VM with &lt;code&gt;runStrategy: RunStrategyAlways&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Stop the VM.&lt;/li&gt;&#xA;&lt;li&gt;Check there is no &lt;code&gt;Failure attempting to delete VMI: &amp;lt;nil&amp;gt;&lt;/code&gt; in VM status.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;UI should not show pause message&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a VM.&lt;/li&gt;&#xA;&lt;li&gt;Pause the VM.&lt;/li&gt;&#xA;&lt;li&gt;Although the message &lt;code&gt;The status of pod readliness gate &amp;quot;kubevirt.io/virtual-machine-unpaused&amp;quot; is not &amp;quot;True&amp;quot;, but False&lt;/code&gt; is in the VM condition, UI should not show it.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Check DNS on install with Github SSH keys</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1903-dns-github-ssh-keys/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1903-dns-github-ssh-keys/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1903&#34;&gt;#1903&lt;/a&gt; DNS server not available during install&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;h3 id=&#34;without-pxe&#34;&gt;Without PXE&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Start a new install&lt;/li&gt;&#xA;&lt;li&gt;Set DNS as &lt;code&gt;8.8.8.8&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Add in github SSH keys&lt;/li&gt;&#xA;&lt;li&gt;Finish install&lt;/li&gt;&#xA;&lt;li&gt;SSH into node with SSH keys from github (&lt;code&gt;rancher@hostname&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Verify login was successful&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;with-pxe&#34;&gt;With PXE&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Got vagrant setup from &lt;a href=&#34;https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester&#34;&gt;https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Changed &lt;code&gt;settings.yml&lt;/code&gt; DHCP config and added &lt;code&gt;dns: 8.8.8.8&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;dhcp_server:&#xA;    ip: 192.168.0.254&#xA;    subnet: 192.168.0.0&#xA;    netmask: 255.255.255.0&#xA;    range: 192.168.0.50 192.168.0.130&#xA;    dns: 8.8.8.8&#xA;    https: false&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Also changed &lt;code&gt;ssh_authorized_keys&lt;/code&gt; and commented out default SSH key and added username for github&lt;/p&gt;</description>
    </item>
    <item>
      <title>Check IPAM configuration with IPAM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1697-ipam-load-balancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1697-ipam-load-balancer/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1697&#34;&gt;#1697&lt;/a&gt; Optimization for the Harvester load balancer&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install the latest rancher and import a Harvester cluster&lt;/li&gt;&#xA;&lt;li&gt;Create a cluster by Harvester node driver&lt;/li&gt;&#xA;&lt;li&gt;Navigate to the workload Page, create a workload&lt;/li&gt;&#xA;&lt;li&gt;Click &amp;ldquo;Add ports&amp;rdquo;, select type as LB, protocol as TCP&lt;/li&gt;&#xA;&lt;li&gt;Check IPAM selector&lt;/li&gt;&#xA;&lt;li&gt;Navigate to the service page, create a LB&lt;/li&gt;&#xA;&lt;li&gt;Click &amp;ldquo;Add-on config&amp;rdquo; tab and check IPAM and port&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/83787952/152212105-2b2335be-b12b-42ac-bfcf-aa1d2aeb6fd3.png&#34; alt=&#34;image.png&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/83787952/152212109-039a3e23-9eae-4ffc-9318-58f048a112c1.png&#34; alt=&#34;image.png&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Check IPv4 static method in ISO installer</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2796-check-ipv4-static-method-in-iso-installer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2796-check-ipv4-static-method-in-iso-installer/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2796&#34;&gt;#2796&lt;/a&gt; [BUG] configure network failed if use static mode&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Newtork&lt;/li&gt;&#xA;&lt;li&gt;Harvester Installer&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Use latest ISO to install&lt;/li&gt;&#xA;&lt;li&gt;Enter VLAN field with&#xA;&lt;ul&gt;&#xA;&lt;li&gt;empty&lt;/li&gt;&#xA;&lt;li&gt;1&lt;/li&gt;&#xA;&lt;li&gt;1000&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;choose static method&lt;/li&gt;&#xA;&lt;li&gt;fill other fields&lt;/li&gt;&#xA;&lt;li&gt;press enter to the next page&lt;/li&gt;&#xA;&lt;li&gt;no error found, and show DNS config page&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;During Harvester ISO installer&#xA;We can configure VLAN network on the static mode with the following settings:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;No error message blocked&lt;/li&gt;&#xA;&lt;li&gt;Can proceed to dns config page&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Check logs on Harvester</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2528-check-logs-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2528-check-logs-harvester/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2528&#34;&gt;#2528&lt;/a&gt; [BUG] Tons of AppArmor denied messages&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Logging&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;This should be run on a Harvester node that has been up for a while and has been in use&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;SSH to harvester node&lt;/li&gt;&#xA;&lt;li&gt;Execute &lt;code&gt;journalctl -b -f&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Look through logs and verify that there isn&amp;rsquo;t anything generating lots of erroneous messages&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;There shouldn&amp;rsquo;t be large volumes of erroneous messages&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Check Network interface link status can match the available NICs in Harvester vlanconfig</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2988-check-network-link-match-vlanconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2988-check-network-link-match-vlanconfig/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2988&#34;&gt;#2988&lt;/a&gt; [BUG] Network interface link status judgement did not match the available NICs in Harvester vlanconfig&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Network&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create cluster network &lt;code&gt;cn1&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196580297-57541544-48f5-4492-b3e9-a3450697f490.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a vlanconfig &lt;code&gt;config-n1&lt;/code&gt; on &lt;code&gt;cn1&lt;/code&gt; which applied to node 1 only&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196580491-0572c539-5828-4f2e-a0a6-59b40fcc549b.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Select an available NIC on the Uplink&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196580574-d38d59de-251c-4cf8-885d-655b76a78659.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a vlan, the cluster network &lt;code&gt;cn1&lt;/code&gt; vlanconfig and provide valid vlan id &lt;code&gt;91&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196584602-b663ca69-da9a-42e3-94e0-41e094ff1d0b.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Edit &lt;code&gt;config-n1&lt;/code&gt;,&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Check NICs list in Uplink&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ssh to node 1&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Check the available NICs on Network config&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/197510924-b070f305-b6e4-477d-97b8-75006b264c30.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Check rancher-monitoring-grafana volume size</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2282-check-rancher-monitoring-grafana-volume-size/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2282-check-rancher-monitoring-grafana-volume-size/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2282&#34;&gt;#2282&lt;/a&gt; [BUG] rancher-monitoring-grafana is too small and it keeps growing&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Monitoring&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Harvester cluster running after 24 hours&lt;/li&gt;&#xA;&lt;li&gt;Access Harvester Longhorn dashboard via https://&lt;!-- raw HTML omitted --&gt;/dashboard/c/local/longhorn&lt;/li&gt;&#xA;&lt;li&gt;Open the Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Open the volume page&lt;/li&gt;&#xA;&lt;li&gt;Check the &lt;code&gt;rancher-monitoring-grafana&lt;/code&gt; size and usage&lt;/li&gt;&#xA;&lt;li&gt;Shutdown a management node machine&lt;/li&gt;&#xA;&lt;li&gt;Power on the management node machine&lt;/li&gt;&#xA;&lt;li&gt;Wait for 60 minutes&lt;/li&gt;&#xA;&lt;li&gt;Check the &lt;code&gt;rancher-monitoring-grafana&lt;/code&gt; size and usage in Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Shutdown all management node machines in sequence&lt;/li&gt;&#xA;&lt;li&gt;Power on all management node machines in sequence&lt;/li&gt;&#xA;&lt;li&gt;Wait for 60 minutes&lt;/li&gt;&#xA;&lt;li&gt;Check the &lt;code&gt;rancher-monitoring-grafana&lt;/code&gt; size and usage in Longhorn UI&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;The &lt;code&gt;rancher-monitoring-grafana&lt;/code&gt; default allocated with &lt;code&gt;2Gi&lt;/code&gt; and Actual usage &lt;code&gt;108 Mi&lt;/code&gt; after running after 24 hours&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/191000121-9c3c640e-7d7f-4d1b-84f6-39745abca0ce.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Check support bundle for SLE Micro OS</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2420-2464-check-support-bundle-sle-micro-os/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2420-2464-check-support-bundle-sle-micro-os/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2420&#34;&gt;#2420&lt;/a&gt; [FEATURE] support bundle: support SLE Micro OS&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2464&#34;&gt;#2464&lt;/a&gt; [backport v1.0] [FEATURE] support bundle: support SLE Micro OS&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Support Bundle&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Download support bundle in support page&lt;/li&gt;&#xA;&lt;li&gt;Extract the support bundle, check every file have content&lt;/li&gt;&#xA;&lt;li&gt;ssh to harvester node&lt;/li&gt;&#xA;&lt;li&gt;Check the /etc/os-release file content&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Check can download support bundle correctly, check can access every file without empty&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Checked every harvester nodes, the ID have changed to &lt;code&gt;sle-micro-rancher&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Check the OS types in Advanced Options</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2776-check-os-types-in-advanced-options/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2776-check-os-types-in-advanced-options/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2776&#34;&gt;#2776&lt;/a&gt; [FEATURE] remove some dead OS types&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Network&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Login harvester dashboard&lt;/li&gt;&#xA;&lt;li&gt;Open the VM create page, check the OS type list&lt;/li&gt;&#xA;&lt;li&gt;Open the image create page, check the OS type list&lt;/li&gt;&#xA;&lt;li&gt;Open the template create page, check the OS type list&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;The following OS types should be removed from list&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Turbolinux&lt;/li&gt;&#xA;&lt;li&gt;Mandriva&lt;/li&gt;&#xA;&lt;li&gt;Xandros&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;In v1.1.0 master  we add the &lt;code&gt;SUSE Linux Enterprise&lt;/code&gt; in the VM creation page&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/190973269-764e425f-20be-4cb1-8334-e7af668a7798.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Check the VM is available when Harvester upgrade failed</title>
      <link>https://harvester.github.io/tests/manual/_incoming/vm-availability-when-harvester-upgrade-failed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/vm-availability-when-harvester-upgrade-failed/</guid>
      <description>&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Harvester Upgrade&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare the previous stable Harvester release cluster&lt;/li&gt;&#xA;&lt;li&gt;Create image&lt;/li&gt;&#xA;&lt;li&gt;Enable Network and create VM&lt;/li&gt;&#xA;&lt;li&gt;Create several virtual machine&lt;/li&gt;&#xA;&lt;li&gt;Follow the &lt;a href=&#34;https://docs.harvesterhci.io/v1.0/upgrade/automatic/&#34;&gt;official document steps&lt;/a&gt; to prepare the online or offline upgrade&lt;/li&gt;&#xA;&lt;li&gt;Do not shutdown virtual machine&lt;/li&gt;&#xA;&lt;li&gt;Start the upgrade&lt;/li&gt;&#xA;&lt;li&gt;Check the VM status if the upgrade failed at &lt;code&gt;Preload images&lt;/code&gt;, &lt;code&gt;Upgrade Rancher&lt;/code&gt; and &lt;code&gt;Upgrade Harvester&lt;/code&gt; phase&lt;/li&gt;&#xA;&lt;li&gt;Check the VM status if the upgrade failed at the &lt;code&gt;Pre-drain&lt;/code&gt;, &lt;code&gt;Post-drain&lt;/code&gt; and &lt;code&gt;RKE2 &amp;amp; OS upgrade&lt;/code&gt; phase&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The VM should be work when upgrade failed at &lt;code&gt;Preload images&lt;/code&gt;, &lt;code&gt;Upgrade Rancher&lt;/code&gt; and &lt;code&gt;Upgrade Harvester&lt;/code&gt; phase&lt;/li&gt;&#xA;&lt;li&gt;The VM could not able to function well when upgrade failed at the &lt;code&gt;Pre-drain&lt;/code&gt;, &lt;code&gt;Post-drain&lt;/code&gt; and &lt;code&gt;RKE2 &amp;amp; OS upgrade&lt;/code&gt; phase&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Check version compatibility during an upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2431-check-version-compatibility-during-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2431-check-version-compatibility-during-upgrade/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2431&#34;&gt;#2431&lt;/a&gt; [FEATURE] Check version compatibility during an upgrade&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Upgrade&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;h3 id=&#34;test-plan-1-v102-upgrade-to-v110-with-release-tag&#34;&gt;Test Plan 1: v1.0.2 upgrade to v1.1.0 with release tag&lt;/h3&gt;&#xA;&lt;h3 id=&#34;test-plan-2-v103-upgrade-to-v110-with-release-tag&#34;&gt;Test Plan 2: v1.0.3 upgrade to v1.1.0 with release tag&lt;/h3&gt;&#xA;&lt;h3 id=&#34;test-plan-3-v102-upgrade-to-v110-without-release-tag&#34;&gt;Test Plan 3: v1.0.2 upgrade to v1.1.0 without release tag&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare v1.0.2, v1.0.3 Harvester ISO image&lt;/li&gt;&#xA;&lt;li&gt;Prepare v1.1.0 ISO image with release tag&lt;/li&gt;&#xA;&lt;li&gt;Prepare v1.1.0 ISO image without release tag&lt;/li&gt;&#xA;&lt;li&gt;Put different ISO image to HTTP server&lt;/li&gt;&#xA;&lt;li&gt;Create the upgrade yaml to create service&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF | kubectl apply -f -&#xA;apiVersion: harvesterhci.io/v1beta1&#xA;kind: Version&#xA;metadata:&#xA;    name: v1.1.0&#xA;    namespace: harvester-system&#xA;spec:&#xA;    isoURL: &amp;#34;http://192.168.1.110:8000/harvester-eeeb1be-dirty-amd64.iso&amp;#34;&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Upgrade from v1.0.2 upgrade to v1.1.0 with release tag&lt;/li&gt;&#xA;&lt;li&gt;Upgrade from v1.0.3 to v1.1.0 with release tag&lt;/li&gt;&#xA;&lt;li&gt;Upgrade from v1.0.2 to v1.1.0 without release tag&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;h3 id=&#34;test-result-1&#34;&gt;Test Result 1&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Upgrade from v1.0.2 upgrade to v1.1.0 with release tag &lt;code&gt;failed&lt;/code&gt; on upgrade dashboard UI&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193567678-7653aa7b-b93a-4cf2-98a8-37e4fcfd06f6.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Check volume status after upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2920-volume-status-after-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2920-volume-status-after-upgrade/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2920&#34;&gt;#2920&lt;/a&gt; [BUG] Volume can&amp;rsquo;t turn into healthy when upgrading from v1.0.3 to v1.1.0-rc2&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Volume&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare a 4 nodes v1.0.3 Harvester cluster&lt;/li&gt;&#xA;&lt;li&gt;Install several images&lt;/li&gt;&#xA;&lt;li&gt;Create three VMs&lt;/li&gt;&#xA;&lt;li&gt;Enable Network&lt;/li&gt;&#xA;&lt;li&gt;Create vlan1 network&lt;/li&gt;&#xA;&lt;li&gt;Shutdown all VMs&lt;/li&gt;&#xA;&lt;li&gt;Upgrade to v1.1.0-rc3&lt;/li&gt;&#xA;&lt;li&gt;Check the volume status in Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Open K9s, Check the pvc status after upgrade&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Can finish the pre-drain of each node and successfully upgrade to v1.1.0-rc3&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196434398-a61b5111-7723-4fa6-ac57-2a68ffef73ee.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Clone image (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2562-clone-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2562-clone-image/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2562&#34;&gt;#2562&lt;/a&gt; [[BUG] Image&amp;rsquo;s labels will not be copied when execute Clone&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Images&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Create a Image via URL&lt;/li&gt;&#xA;&lt;li&gt;Clone the Image and named image-b&lt;/li&gt;&#xA;&lt;li&gt;Check image-b labels in Labels tab&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;All labels should be cloned and shown in labels tab&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>collect Fleet logs and YAMLs in support bundles</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2297_collect_fleet_logs_and_yamls_in_support_bundles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2297_collect_fleet_logs_and_yamls_in_support_bundles/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2297&#34;&gt;https://github.com/harvester/harvester/issues/2297&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard then navigate to support page&lt;/li&gt;&#xA;&lt;li&gt;Click &lt;strong&gt;Generate Support Bundle&lt;/strong&gt; and do Generate&lt;/li&gt;&#xA;&lt;li&gt;log files should be exist in the zipfile of support bundle:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;logs/cattle-fleet-local-system/fleet-agent-&amp;lt;randomID&amp;gt;/fleet-agent.log&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;logs/cattle-fleet-system/fleet-controller-&amp;lt;randomID&amp;gt;/fleet-controller.log&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;logs/cattle-fleet-system/gitjob-&amp;lt;randomID&amp;gt;/gitjob.log&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Collect system logs</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2647_collect_system_logs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2647_collect_system_logs/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2647&#34;&gt;https://github.com/harvester/harvester/issues/2647&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install &lt;em&gt;Graylog&lt;/em&gt; via docker[^1]&lt;/li&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard then navigate to &lt;em&gt;Monitoring &amp;amp; Logging/Logging&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;strong&gt;Cluster Output&lt;/strong&gt; with following:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: gelf-evts&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: &lt;code&gt;Logging/Event&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: GELF&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt;: &lt;code&gt;&amp;lt;Graylog_IP&amp;gt;, &amp;lt;Graylog_Port&amp;gt;, &amp;lt;UDP&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;strong&gt;Cluster Flow&lt;/strong&gt; with following:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: gelf-flow&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt; of Matches: &lt;code&gt;Logging&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cluster Outputs&lt;/strong&gt;: &lt;code&gt;gelf-evts&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create an Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create a vm &lt;code&gt;vm1&lt;/code&gt; and start it&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;code&gt;Graylog&lt;/code&gt; dashboard then navigate to search&lt;/li&gt;&#xA;&lt;li&gt;Select update frequency&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/191725169-d1203674-13d8-487b-9fa2-e1d9394fa5c0.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;New logs should be posted continuously.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;code-snippets-to-setup-graylog&#34;&gt;code snippets to setup Graylog&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --name mongo -d mongo:4.2.22-rc0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sysctl -w vm.max_map_count&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;262145&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --name elasticsearch -p 9200:9200 -p 9300:9300 -e xpack.security.enabled&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false  -e node.name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;es01 -it docker.elastic.co/elasticsearch/elasticsearch:6.8.23&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --name graylog --link mongo --link elasticsearch -p 9000:9000 -p 12201:12201 -p 1514:1514 -p 5555:5555 -p 12202:12202 -p 12202:12202/udp -e GRAYLOG_PASSWORD_SECRET&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Graypass3WordMor!e&amp;#34;&lt;/span&gt; -e GRAYLOG_ROOT_PASSWORD_SHA2&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;899e9793de44cbb14f48b4fce810de122093d03705c0971752a5c15b0fa1ae03   -e GRAYLOG_HTTP_EXTERNAL_URI&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://127.0.0.1:9000/&amp;#34;&lt;/span&gt;  -d graylog/graylog:4.3.5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;Login to &lt;em&gt;Graylog&lt;/em&gt; dashboard by the URL &lt;code&gt;http://&amp;lt;server_ip&amp;gt;:9000/&lt;/code&gt; with &lt;code&gt;admin&lt;/code&gt;/&lt;code&gt;ROOT_PASSWORDa1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;System/Inputs&lt;/em&gt; then select input &lt;strong&gt;GELF UDP&lt;/strong&gt;, update the port to &lt;code&gt;12202&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/191723749-7d796243-5996-4884-90b4-d8227f81adc5.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Config logging in Harvester Dashboard</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2646_config_logging_in_harvester_dashboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2646_config_logging_in_harvester_dashboard/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2646&#34;&gt;https://github.com/harvester/harvester/issues/2646&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/191697822-6bc0d7b8-2c56-42e0-805a-408c1ef19845.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/191697860-7ef66c19-cd3e-4e4c-b485-315e7eec771d.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard then navigate to &lt;em&gt;Monitoring &amp;amp; Logging/Logging&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Configurations of Fluentbit and Fluentd should be available in &lt;em&gt;Logging/Configuration&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Configure VLAN interface on ISO installer UI</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1647-configure-vlan-interface-on-iso-installer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1647-configure-vlan-interface-on-iso-installer/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1647&#34;&gt;#1647&lt;/a&gt; [FEATURE] Support configuring a VLAN at the management interface in the ISO installer UI&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Network&lt;/li&gt;&#xA;&lt;li&gt;Harvester Installer&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare a &lt;code&gt;No&lt;/code&gt; VLAN network environment&lt;/li&gt;&#xA;&lt;li&gt;Prepare a &lt;code&gt;VLAN&lt;/code&gt; network environment&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Boot Harvester ISO installer&lt;/li&gt;&#xA;&lt;li&gt;Set VLAN id or keep empty&lt;/li&gt;&#xA;&lt;li&gt;Keep installing&lt;/li&gt;&#xA;&lt;li&gt;Check can complete installation&lt;/li&gt;&#xA;&lt;li&gt;Check harvester has network connectivity&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;test-plan-matrix&#34;&gt;Test Plan Matrix&lt;/h2&gt;&#xA;&lt;h3 id=&#34;create-mode&#34;&gt;Create mode&lt;/h3&gt;&#xA;&lt;h4 id=&#34;no-vlan&#34;&gt;No VLAN&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;DHCP VIP + DHCP node ip&lt;/li&gt;&#xA;&lt;li&gt;DHCP VIP + Static node ip&lt;/li&gt;&#xA;&lt;li&gt;static VIP + DHCP node ip&lt;/li&gt;&#xA;&lt;li&gt;static VIP + Static node ip&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;vlan&#34;&gt;VLAN&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;DHCP VIP + DHCP node ip&lt;/li&gt;&#xA;&lt;li&gt;DHCP VIP + Static node ip&lt;/li&gt;&#xA;&lt;li&gt;static VIP + DHCP node ip&lt;/li&gt;&#xA;&lt;li&gt;static VIP + Static node ip&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;join-mode&#34;&gt;Join mode&lt;/h3&gt;&#xA;&lt;h4 id=&#34;no-vlan-1&#34;&gt;No VLAN&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;DHCP VIP + DHCP node ip&lt;/li&gt;&#xA;&lt;li&gt;DHCP VIP + Static node ip&lt;/li&gt;&#xA;&lt;li&gt;static VIP + DHCP node ip&lt;/li&gt;&#xA;&lt;li&gt;static VIP + Static node ip&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;vlan-1&#34;&gt;VLAN&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;DHCP VIP + DHCP node ip&lt;/li&gt;&#xA;&lt;li&gt;DHCP VIP + Static node ip&lt;/li&gt;&#xA;&lt;li&gt;static VIP + DHCP node ip&lt;/li&gt;&#xA;&lt;li&gt;static VIP + Static node ip&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Check can complete installation&lt;/li&gt;&#xA;&lt;li&gt;Check harvester has network connectivity&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;ip a show dev mgmt-br [VLAN ID]&lt;/code&gt; has IP&lt;/li&gt;&#xA;&lt;li&gt;e.g ip a show dev mgmt-br.100&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Create a harvester-specific StorageClass for Longhorn</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2692_create_a_harvester-specific_storageclass_for_longhorn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2692_create_a_harvester-specific_storageclass_for_longhorn/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2692&#34;&gt;https://github.com/harvester/harvester/issues/2692&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/192323716-c863af2a-388f-49d6-8636-d57f8abbad35.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with 2+ nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard and create an image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Advanced/Storage Classes&lt;/em&gt;, &lt;code&gt;harvester-longhorn&lt;/code&gt; and &lt;code&gt;longhorn&lt;/code&gt; should be available, and &lt;code&gt;harvester-longhorn&lt;/code&gt; should be settled as &lt;strong&gt;Default&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Volumes&lt;/em&gt; and create &lt;code&gt;vol-old&lt;/code&gt; where Storage Class is &lt;code&gt;longhorn&lt;/code&gt; and &lt;code&gt;vol-new&lt;/code&gt; where Storage Class is &lt;code&gt;harvester-longhorn&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create VM &lt;code&gt;vm1&lt;/code&gt; attaching &lt;code&gt;vol-old&lt;/code&gt; and &lt;code&gt;vol-new&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;code&gt;vm1&lt;/code&gt; and use &lt;code&gt;fdisk&lt;/code&gt; format volumes and mount to folders: &lt;code&gt;old&lt;/code&gt; and &lt;code&gt;new&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create file and move into both volumes as following commands:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dd &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/dev/zero of&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;file1 bs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10485760&lt;/span&gt; count&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cp file1 old &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; cp file1 new&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;&#xA;&lt;li&gt;Migrate &lt;code&gt;vm1&lt;/code&gt; to another host, migration should success&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;code&gt;vm1&lt;/code&gt;, volumes should still attaching to folders &lt;code&gt;old&lt;/code&gt; and &lt;code&gt;new&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Execute command &lt;code&gt;sha256sum&lt;/code&gt; on &lt;code&gt;old/file1&lt;/code&gt; and &lt;code&gt;new/file1&lt;/code&gt; should show the same value.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Create multiple VM instances using VM template with EFI mode selected</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2577-create-multiple-vm-using-template-efi-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2577-create-multiple-vm-using-template-efi-mode/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2577&#34;&gt;#2577&lt;/a&gt; [BUG] Boot in EFI mode not selected when creating multiple VM instances using VM template with EFI mode selected.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Virtual Machine&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a VM template, check the &lt;code&gt;Booting in EFI mode&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create multiple VM instance and use the VM template have &lt;code&gt;Booting in EFI mode&lt;/code&gt; checked&lt;/li&gt;&#xA;&lt;li&gt;Wait for all VM running&lt;/li&gt;&#xA;&lt;li&gt;Check the EFI mode is enabled in VM config&lt;/li&gt;&#xA;&lt;li&gt;ssh to each VM&lt;/li&gt;&#xA;&lt;li&gt;Check the /etc/firmware/efi file&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;Can create multiple VM instance using VM template with EFI mode selected&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dashboard Storage usage display when node disk have warning</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2622-dashboard-storage-usage-display-when-node-disk-have-warning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2622-dashboard-storage-usage-display-when-node-disk-have-warning/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2622&#34;&gt;#2622&lt;/a&gt; [BUG] Dashboard Storage used is wrong when a node disk is warning&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Storage&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Login harvester dashboard&lt;/li&gt;&#xA;&lt;li&gt;Access Longhorn UI from url https://192.168.122.136/dashboard/c/local/longhorn&lt;/li&gt;&#xA;&lt;li&gt;Go to Node page&lt;/li&gt;&#xA;&lt;li&gt;Click edit node and disks&lt;/li&gt;&#xA;&lt;li&gt;Select disabling Node scheduling&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/187578343-653d0235-92a9-4979-aae0-b62b606df525.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Select disabling storage scheduling on the bottom&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/187578175-326b5909-cd6a-4e31-a1cf-92df5e619a5c.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Open Longhorn dashboard page, check the Storage Schedulable&lt;/li&gt;&#xA;&lt;li&gt;Open Harvester dashboard page, check the used and scheduled storage size&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;After disabling the node and storage scheduling on Longhorn UI.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dedicated storage network</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1055_dedicated_storage_network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1055_dedicated_storage_network/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1055&#34;&gt;https://github.com/harvester/harvester/issues/1055&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Verified this feature has been implemented partially.&#xA;Mentioned problem in &lt;a href=&#34;https://github.com/harvester/harvester/issues/1055#issuecomment-1283754519&#34;&gt;https://github.com/harvester/harvester/issues/1055#issuecomment-1283754519&lt;/a&gt; will be introduced as a enhancement in #2995&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-information&#34;&gt;Test Information&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Environment: &lt;strong&gt;baremetal DL360G9 5 nodes&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Harvester Version: &lt;strong&gt;master-bd1d49a9-head&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ui-source&lt;/strong&gt; Option: &lt;strong&gt;Auto&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Networks -&amp;gt; Cluster Networks/Configs&lt;/em&gt;, create Cluster Network named &lt;code&gt;vlan&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Advanced -&amp;gt; Settings&lt;/em&gt;, edit &lt;code&gt;storage-network&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Select &lt;code&gt;Enable&lt;/code&gt; then select &lt;code&gt;vlan&lt;/code&gt; as cluster network, fill in &lt;strong&gt;VLAN ID&lt;/strong&gt; and &lt;strong&gt;IP Range&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Click Save, warning or error message should displayed.&lt;/li&gt;&#xA;&lt;li&gt;edit &lt;code&gt;storage-network&lt;/code&gt; again, &lt;code&gt;mgmt&lt;/code&gt; should not in the drop-down list of &lt;code&gt;Cluster Network&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Networks -&amp;gt; Cluster Networks/Configs&lt;/em&gt;, create Cluster Network named &lt;code&gt;vlan2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;code&gt;Network Config&lt;/code&gt; for all nodes&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Advanced -&amp;gt; Settings&lt;/em&gt;, edit &lt;code&gt;storage-network&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Select &lt;code&gt;Enable&lt;/code&gt; then select &lt;code&gt;vlan2&lt;/code&gt; as cluster network, fill in &lt;strong&gt;VLAN ID&lt;/strong&gt; and &lt;strong&gt;IP Range&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Networks -&amp;gt; Cluster Networks/Configs&lt;/em&gt;, delete Cluster Network &lt;code&gt;vlan2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Warning or error message should displayed&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Delete VM template default version (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2376-2379-delete-vm-template-default-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2376-2379-delete-vm-template-default-version/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2376&#34;&gt;#2376&lt;/a&gt; [BUG] Cannot delete Template&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2379&#34;&gt;#2379&lt;/a&gt; [backport v1.0.3] Cannot delete Template&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VM Template&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Go to Advanced -&amp;gt; Templates&lt;/li&gt;&#xA;&lt;li&gt;Create a new template&lt;/li&gt;&#xA;&lt;li&gt;Modify the template to create a new version&lt;/li&gt;&#xA;&lt;li&gt;Click the config button of the &lt;code&gt;default version&lt;/code&gt; template&lt;/li&gt;&#xA;&lt;li&gt;Click the config button of the &lt;code&gt;non default version&lt;/code&gt; template&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;If the template is the &lt;code&gt;default version&lt;/code&gt;, it will not display the &lt;code&gt;delete&lt;/code&gt; button&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/174030567-b2c6ae52-40d1-4dd6-9ede-783409bd3c87.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deny the vlanconfigs overlap with the other</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2828-deny-vlanconfig-overlap-others/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2828-deny-vlanconfig-overlap-others/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2828&#34;&gt;#2828&lt;/a&gt; [BUG][FEATURE] Deny the vlanconfigs overlap with the other&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Network&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare a 3 nodes Harvester on local kvm&lt;/li&gt;&#xA;&lt;li&gt;Each VM have five NICs attached.&lt;/li&gt;&#xA;&lt;li&gt;Create a cluster network &lt;code&gt;cn1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a vlanconfig &lt;code&gt;config-all&lt;/code&gt; which applied to &lt;code&gt;all nodes&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196409238-dd1a5d9f-bf00-46cd-93b2-c9469bf7c58a.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Set one of the NIC&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196409451-5279f4e5-e66a-4960-8889-cc1c186acfdc.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;On the same cluster network, create another vlan network &lt;code&gt;config-one&lt;/code&gt; which applied to only &lt;code&gt;node 1&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196409565-67e2e418-1efc-4c50-a016-7fea4dd582a3.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Provide another NIC&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196409613-e214183d-b665-453e-8fa8-246f21a11243.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Click the create button&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;Under the same Cluster Network:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploy guest cluster to specific node with Node selector label</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2316-2384-deploy-guest-cluster-node-selector-label-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2316-2384-deploy-guest-cluster-node-selector-label-copy/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2316&#34;&gt;#2316&lt;/a&gt; [BUG] Guest cluster nodes distributed across failure domain&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2384&#34;&gt;#2384&lt;/a&gt; [backport v1.0.3] Guest cluster nodes distributed across failure domains&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;h3 id=&#34;rke2-verification-steps&#34;&gt;RKE2 Verification Steps&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Open Harvester Host page then edit host config&lt;/li&gt;&#xA;&lt;li&gt;Add the following key value in the labels page:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;topology.kubernetes.io/zone: zone_bp&lt;/li&gt;&#xA;&lt;li&gt;topology.kubernetes.io/region: region_bp&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/179735384-77e99870-92ad-41c2-b414-a872130c0b27.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Open the RKE2 provisioning page&lt;/li&gt;&#xA;&lt;li&gt;Expand the show advanced&lt;/li&gt;&#xA;&lt;li&gt;Click add Node selector in &lt;code&gt;Node scheduling&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Use default &lt;code&gt;Required&lt;/code&gt; priority&lt;/li&gt;&#xA;&lt;li&gt;Click Add Rule&lt;/li&gt;&#xA;&lt;li&gt;Provide the following key/value pairs&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;topology.kubernetes.io/zone: zone_bp&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;topology.kubernetes.io/region: region_bp&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/179736419-78612fd1-9990-44d8-b9be-d9a850bd27a0.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Provide the following user data&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;password: 123456&#xA;chpasswd: { expire: False }&#xA;ssh_pwauth: True&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create the RKE2 guest cluster&lt;/li&gt;&#xA;&lt;li&gt;Go to Harvester Virtual Machine page&lt;/li&gt;&#xA;&lt;li&gt;Edit yaml of the RKE2 guest cluster&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/179737541-f921d841-13ee-4a97-b096-5e4bdca39320.png&#34; alt=&#34;image&#34;&gt;&#xA;Check the node affinity label have written into the yaml&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/179737712-9c0dac59-78be-4386-b1e4-646d7d7fbd90.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the guest cluster VM have no error message&lt;/li&gt;&#xA;&lt;li&gt;Check can provision RKE2 cluster without error&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/179739296-33f3292b-3eb9-4823-80e2-64d3e3014765.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Provide mismatch or not exists node scheduling or pod scheduling selector key/value pair in step 13 or 18&lt;/li&gt;&#xA;&lt;li&gt;Provision another RKE2 cluster&lt;/li&gt;&#xA;&lt;li&gt;Check VM should have error icon with message and automatically suspend the RKE2 provisioning&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;rke1-verification-steps&#34;&gt;RKE1 Verification Steps&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Follow the steps 1 ~ 7 of the RKE2 verification section&lt;/li&gt;&#xA;&lt;li&gt;Go to Rancher Cluster Management page, add the RKE1 node template&lt;/li&gt;&#xA;&lt;li&gt;Click add Node selector in &lt;code&gt;Node scheduling&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Use default &lt;code&gt;Required&lt;/code&gt; priority&lt;/li&gt;&#xA;&lt;li&gt;Click Add Rule&lt;/li&gt;&#xA;&lt;li&gt;Provide the following key/value pairs&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;topology.kubernetes.io/zone: zone_bp&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;topology.kubernetes.io/region: region_bp&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/179751217-773ffbaf-6df8-44c2-b4fe-382df4508c38.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create the RKE1 guest cluster&lt;/li&gt;&#xA;&lt;li&gt;Go to Harvester Virtual Machine page&lt;/li&gt;&#xA;&lt;li&gt;Edit yaml of the RKE1 guest cluster&lt;/li&gt;&#xA;&lt;li&gt;Check the node label have written into the yaml&lt;/li&gt;&#xA;&lt;li&gt;Check the guest cluster VM have no error message&lt;/li&gt;&#xA;&lt;li&gt;Check can provision RKE1 cluster without error&lt;/li&gt;&#xA;&lt;li&gt;Provide mismatch or not exists node scheduling or pod scheduling selector key/value pair in step 6 and 10&lt;/li&gt;&#xA;&lt;li&gt;Provision another RKE1 cluster&lt;/li&gt;&#xA;&lt;li&gt;Check VM should have error icon with message and automatically suspend the RKE1 provisioning&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Can&lt;/strong&gt; deploy guest RKE1 cluster vm to specific Harvester node matching the &lt;code&gt;node scheduling&lt;/code&gt; and &lt;code&gt;pod scheduling&lt;/code&gt; selector&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Can&lt;/strong&gt; deploy guest RKE2 cluster vm to specific Harvester node matching the &lt;code&gt;node scheduling&lt;/code&gt; and &lt;code&gt;pod scheduling&lt;/code&gt; selector&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Can&amp;rsquo;t&lt;/strong&gt; deploy guest RKE1 cluster vm to specific Harvester node if any of the &lt;code&gt;node scheduling&lt;/code&gt; and &lt;code&gt;pod scheduling&lt;/code&gt; selector not found on the Harvester node&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Can&amp;rsquo;t&lt;/strong&gt; deploy guest RKE2 cluster vm to specific Harvester node if any of the &lt;code&gt;node scheduling&lt;/code&gt; and &lt;code&gt;pod scheduling&lt;/code&gt; selector not found on the Harvester node&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Download backing images</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1436__allowing_users_to_download_backing_images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1436__allowing_users_to_download_backing_images/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1436&#34;&gt;https://github.com/harvester/harvester/issues/1436&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/189675005-a7509189-f0c3-42e4-b5a4-d8c1bc1f6341.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Create a Image &lt;code&gt;img1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Click the details of &lt;code&gt;img1&lt;/code&gt;, &lt;strong&gt;Download&lt;/strong&gt; Button should be available&lt;/li&gt;&#xA;&lt;li&gt;Click &lt;strong&gt;Download&lt;/strong&gt; button, &lt;code&gt;img1&lt;/code&gt; should able to be downloaded and downloaded successfully.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>enable/disable alertmanager on demand</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2518_enabledisable_alertmanager_on_demand/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2518_enabledisable_alertmanager_on_demand/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2518&#34;&gt;https://github.com/harvester/harvester/issues/2518&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/193554680-c2d6f7c0-5cf0-44ee-803e-c7abda408774.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/193554761-1f28c3b9-8964-4bfa-8069-d5bcc7d8d837.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard, navigate to &lt;strong&gt;Monitoring &amp;amp; Logging/Monitoring/Configuration&lt;/strong&gt; then select &lt;strong&gt;Alertmanager&lt;/strong&gt; tab&lt;/li&gt;&#xA;&lt;li&gt;Option Button &lt;code&gt;Enabled&lt;/code&gt; should be checked&lt;/li&gt;&#xA;&lt;li&gt;Select &lt;strong&gt;Grafana&lt;/strong&gt; tab then access Grafana&lt;/li&gt;&#xA;&lt;li&gt;Search &lt;em&gt;Alertmanager&lt;/em&gt; to access &lt;em&gt;Overview&lt;/em&gt; dashboard&lt;/li&gt;&#xA;&lt;li&gt;Data should be available and keep updating&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Enabling and Tuning KSM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2302_enabling_and_tuning_ksm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2302_enabling_and_tuning_ksm/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2302&#34;&gt;https://github.com/harvester/harvester/issues/2302&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard and Navigate to hosts&lt;/li&gt;&#xA;&lt;li&gt;Edit &lt;em&gt;node1&lt;/em&gt;&amp;rsquo;s &lt;strong&gt;Ksmtuned&lt;/strong&gt; to &lt;code&gt;Run&lt;/code&gt; and &lt;strong&gt;ThresCoef&lt;/strong&gt; to &lt;code&gt;85&lt;/code&gt; then Click &lt;strong&gt;Save&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;em&gt;node1&lt;/em&gt;&amp;rsquo;s console, execute &lt;code&gt;kubectl get ksmtuned -oyaml --field-selector metadata.name=&amp;lt;node1&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Fields in &lt;code&gt;spec&lt;/code&gt; should be the same as Dashboard configured&lt;/li&gt;&#xA;&lt;li&gt;Create an image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create multiple VMs with 2Gi+ memory and schedule on &lt;code&gt;&amp;lt;node1&amp;gt;&lt;/code&gt; (memory size reflect to &lt;!-- raw HTML omitted --&gt;&amp;rsquo;s maximum size, total of VMs&amp;rsquo; memory should greater than 40%)&lt;/li&gt;&#xA;&lt;li&gt;Execute &lt;code&gt;watch -n1 grep . /sys/kernel/mm/ksm/*&lt;/code&gt; to monitor ksm&amp;rsquo;s status change&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/run&lt;/code&gt; should be update to &lt;code&gt;1&lt;/code&gt; after VMs started&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/page_*&lt;/code&gt; should updating continuously&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard then navigate to &lt;em&gt;Hosts&lt;/em&gt;, click &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;In the Tab of &lt;strong&gt;Ksmtuned&lt;/strong&gt;, values in Statistics section should not be &lt;code&gt;0&lt;/code&gt;.  (data in this section will be updated per min, so it not equals to console&amp;rsquo;s output was expected.)&lt;/li&gt;&#xA;&lt;li&gt;Stop all VMs scheduling to &lt;code&gt;&amp;lt;node1&amp;gt;&lt;/code&gt;, the monitor data &lt;code&gt;/sys/kernel/mm/ksm/run&lt;/code&gt; should be update to &lt;code&gt;0&lt;/code&gt; (this is expected as it is designed to dynamically spawn ksm up when &lt;code&gt;ThresCoef&lt;/code&gt; hits)&lt;/li&gt;&#xA;&lt;li&gt;Update &lt;!-- raw HTML omitted --&gt;&amp;rsquo;s &lt;strong&gt;Ksmtuned&lt;/strong&gt; to &lt;code&gt;Run: Prune&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Monitor data in Step.8 should reflect to:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/run&lt;/code&gt; should be update to &lt;code&gt;2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/pages_*&lt;/code&gt; should be update to &lt;code&gt;0&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Update &lt;!-- raw HTML omitted --&gt;&amp;rsquo;s &lt;strong&gt;Ksmtuned&lt;/strong&gt; to &lt;code&gt;Run: Stop&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Monitor data in Step.8 should reflect to:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/run&lt;/code&gt; should be update to &lt;code&gt;0&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>enhance double check of VM&#39;s resource modification</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2869_enhance_double_check_of_vms_resource_modification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2869_enhance_double_check_of_vms_resource_modification/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2869&#34;&gt;https://github.com/harvester/harvester/issues/2869&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Create an Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create VM &lt;code&gt;vm1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Imitate video recording (as below) to test&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/5169694/193790263-19379641-e282-445f-831f-8da039c15e77.mp4&#34;&gt;https://user-images.githubusercontent.com/5169694/193790263-19379641-e282-445f-831f-8da039c15e77.mp4&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>enhance node scheduling when vm selects network</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2982_enhance_node_scheduling_when_vm_selects_network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2982_enhance_node_scheduling_when_vm_selects_network/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2982&#34;&gt;https://github.com/harvester/harvester/issues/2982&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;criteria&#34;&gt;Criteria&lt;/h3&gt;&#xA;&lt;p&gt;Scheduling rule added automatically when select specific network&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/197729616-a6fcda2e-42ba-469f-b6c1-9c297bef1a45.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;go to &lt;code&gt;Cluster Networks / Config&lt;/code&gt; page,  create a new Cluster Network (eg: test)&lt;/li&gt;&#xA;&lt;li&gt;Create a new &lt;code&gt;network config&lt;/code&gt; in the &lt;code&gt;test&lt;/code&gt; Cluster Network. (Select a specific node)&#xA;&lt;img src=&#34;https://images.zenhubusercontent.com/60345555ec1db310c78aa2b8/431ba9b2-56e7-48af-bf4d-6e0ba964ebd3&#34; alt=&#34;image.png&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;go to &lt;code&gt;Network&lt;/code&gt; page&lt;/li&gt;&#xA;&lt;li&gt;to create a new network (e.g:  &lt;code&gt;test-untagged&lt;/code&gt;), select &lt;code&gt;UntaggedNetwork&lt;/code&gt; type and select &lt;code&gt;test&lt;/code&gt; cluster network. click &lt;code&gt;Create&lt;/code&gt; button&lt;/li&gt;&#xA;&lt;li&gt;go to VM create page,    fill all required value,  Click &lt;code&gt;Networks&lt;/code&gt; tab,   select &lt;code&gt;default/test-untagged&lt;/code&gt; network,  click &lt;code&gt;Create&lt;/code&gt; button&lt;/li&gt;&#xA;&lt;li&gt;The VM is successfully created, but the scheduled node may not match the Network Config&#xA;![image.png]&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Function keys on web VNC interface</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1461-function-keys-on-web-vnc-interface/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1461-function-keys-on-web-vnc-interface/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1461&#34;&gt;#1461&lt;/a&gt; [UI] F keys and Alt-F keys in web VNC interface&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Network&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a new VM with Ubuntu desktop 20.04&lt;/li&gt;&#xA;&lt;li&gt;Prepare two volume&lt;/li&gt;&#xA;&lt;li&gt;Complete the installation process&lt;/li&gt;&#xA;&lt;li&gt;Open a web browser on Ubuntu desktop&lt;/li&gt;&#xA;&lt;li&gt;Check the shortcut keys combination&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Check the soft shortcut keys can display and work correctly on Linux OS VM (Ubuntu desktop 20.04)&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/177092853-0a9d570e-39b1-4127-ac22-2b9508d5b4f6.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Checked the following short cut can work as expected&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generate Install Support Config Bundle For Single Node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1864-generate-install-support-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1864-generate-install-support-config/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Related issue: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1864&#34;&gt;#1864&lt;/a&gt;  Support bundle for a single node (Live/Installed)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Related issue: &lt;a href=&#34;https://github.com/harvester/harvester-installer/pull/272&#34;&gt;#272&lt;/a&gt;  Generate supportconfig for failed installations&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Support&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h2&gt;&#xA;&lt;p&gt;Setup a single node harvester from ISO install but don&amp;rsquo;t complete the installation&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Gain SSH Access to the Single Harvester Node&lt;/li&gt;&#xA;&lt;li&gt;Once Shelled into the Single Harvester Node edit the &lt;code&gt;/usr/sbin/harv-install&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Using: &lt;a href=&#34;https://github.com/harvester/harvester-installer/blob/master/package/harvester-os/files/usr/sbin/harv-install#L362&#34;&gt;harvester-installer&amp;rsquo;s harv-install as a reference&lt;/a&gt; edit around line #362 adding &lt;code&gt;exit 1&lt;/code&gt;:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;exit 1&#xA;trap cleanup exit&#xA;check_iso&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;save the file.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Harvester Cloud Provider compatibility check</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2753-harvester-cloud-provider-compatibility/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2753-harvester-cloud-provider-compatibility/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2753&#34;&gt;#2753&lt;/a&gt; [FEATURE] Harvester Cloud Provider compatibility check enhancement&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher Integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Open Rancher Global settings&lt;/li&gt;&#xA;&lt;li&gt;Edit the &lt;code&gt;rke-metadata-config&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Change the default url to &lt;code&gt;https://harvester-dev.oss-cn-hangzhou.aliyuncs.com/Untitled-1.json&lt;/code&gt; which include the following cloud provider and csi-driver chart changes&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;#34;charts&amp;#34;: {&#xA;    &amp;#34;harvester-cloud-provider&amp;#34;: {&#xA;    &amp;#34;repo&amp;#34;: &amp;#34;rancher-rke2-charts&amp;#34;,&#xA;    &amp;#34;version&amp;#34;: &amp;#34;1.1.0&amp;#34;&#xA;    },&#xA;    &amp;#34;harvester-csi-driver&amp;#34;: {&#xA;    &amp;#34;repo&amp;#34;: &amp;#34;rancher-rke2-charts&amp;#34;,&#xA;    &amp;#34;version&amp;#34;: &amp;#34;1.1.0&amp;#34;&#xA;    },&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Save and reload page&lt;/li&gt;&#xA;&lt;li&gt;Open the create RKE2 cluster page&lt;/li&gt;&#xA;&lt;li&gt;Select the incomparable RKE2 version&lt;/li&gt;&#xA;&lt;li&gt;Check the Cloud provider drop down&lt;/li&gt;&#xA;&lt;li&gt;Enable Harvester API in Preference -&amp;gt; Enable Developer Tools &amp;amp; Features&lt;/li&gt;&#xA;&lt;li&gt;Open settings&lt;/li&gt;&#xA;&lt;li&gt;Click view API of any setting&lt;/li&gt;&#xA;&lt;li&gt;Click up&lt;/li&gt;&#xA;&lt;li&gt;open the  id&amp;quot;: &amp;ldquo;harvester-csi-ccm-versions&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;Or directly access https://192.168.122.162/v1/harvester/harvesterhci.io.settings/harvester-csi-ccm-versions&lt;/li&gt;&#xA;&lt;li&gt;Click Edit button&lt;/li&gt;&#xA;&lt;li&gt;Change the default value to &lt;code&gt;{&amp;quot;harvester-cloud-provider&amp;quot;:&amp;quot;&amp;gt;=0.0.1 &amp;lt;0.1.2&amp;quot;,&amp;quot;harvester-csi-provider&amp;quot;:&amp;quot;&amp;gt;=0.0.1 &amp;lt;0.1.2&amp;quot;}&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193208619-fd35fc50-9cff-433a-8f7f-ef59420031d6.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Show and Send request&lt;/li&gt;&#xA;&lt;li&gt;Repeat step 5 - 7&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;When we change the cloud provider and csi-driver chart version in Rancher &lt;code&gt;rke-metadata-config&lt;/code&gt; setting,&#xA;Then we select the incompatibility RKE2 version, the Harvester cloud provider is &lt;code&gt;disabled&lt;/code&gt; and there will be a warning banner&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193209810-214210ae-59ad-438f-bed3-d1ba1ae07cab.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Harvester pull Rancher agent image from private registry</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2175-2332-harvester-pull-rancher-image-private-registry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2175-2332-harvester-pull-rancher-image-private-registry/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2175&#34;&gt;#2175&lt;/a&gt; [BUG] Harvester fails to pull Rancher agent image from private registry&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2332&#34;&gt;#2332&lt;/a&gt; [Backport v1.0] Harvester fails to pull Rancher agent image from private registry&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Virtual Machine&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a harvester cluster and a ubuntu server. Make sure they can reach each other.&lt;/li&gt;&#xA;&lt;li&gt;On each harvester node, add ubuntu IP to &lt;code&gt;/etc/hosts&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# vim /etc/hosts&#xA;&amp;lt;host ip&amp;gt; myregistry.local&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;On the ubuntu server, install docker and run the following commands.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ mkdir -p certs&#xA;$ openssl req \&#xA;  -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \&#xA;  -addext &amp;#34;subjectAltName = DNS:myregistry.local&amp;#34; \&#xA;  -x509 -days 365 -out certs/domain.crt&#xA;$ sudo mkdir -p /etc/docker/certs.d/myregistry.local:5000&#xA;$ sudo cp certs/domain.crt /etc/docker/certs.d/myregistry.local:5000/domain.crt&#xA;$ sudo docker run -d \&#xA;  -p 5000:5000 \&#xA;  --restart=always \&#xA;  --name registry \&#xA;  -v &amp;#34;$(pwd)&amp;#34;/certs:/certs \&#xA;  -v &amp;#34;$(pwd)&amp;#34;/registry:/var/lib/registry \&#xA;  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \&#xA;  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \&#xA;  registry:2&#xA;$ sudo docker pull rancher/rancher-agent:v2.6.5&#xA;$ sudo docker tag rancher/rancher-agent:v2.6.5 myregistry.local:5000/rancher/rancher-agent:v2.6.5&#xA;$ sudo docker push myregistry.local:5000/rancher/rancher-agent:v2.6.5&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;Create a rancher v2.6.5 (it&amp;rsquo;s can be a docker container or VM). After it starts, update &lt;code&gt;system-default-registry&lt;/code&gt; setting to &lt;code&gt;myregistry.local:5000&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Open harvester dashboard and update &lt;code&gt;additional-ca&lt;/code&gt; setting with content in &lt;code&gt;~/certs/domain.crt&lt;/code&gt; in ubuntu server.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-1-import-harvester-to-rancher&#34;&gt;Case 1: Import Harvester to Rancher&lt;/h3&gt;&#xA;&lt;p&gt;Import harvester cluster to the rancher and doesn&amp;rsquo;t have any error.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Harvester rebase check on SLE Micro</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1933-2420-harvester-rebase-check-on-sle-micro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1933-2420-harvester-rebase-check-on-sle-micro/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1933&#34;&gt;#1933&lt;/a&gt; [FEATURE] Rebase Harvester on SLE Micro for Rancher&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2420&#34;&gt;#2420&lt;/a&gt; [FEATURE] support bundle: support SLE Micro OS&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;System&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Download support bundle in support page&lt;/li&gt;&#xA;&lt;li&gt;Extract support bundle and check every file content&lt;/li&gt;&#xA;&lt;li&gt;Vagrant install master release&lt;/li&gt;&#xA;&lt;li&gt;Execute backend E2E regression test&lt;/li&gt;&#xA;&lt;li&gt;Run frontend Cypress automated test against feature Images, Networks, Virtual machines&lt;/li&gt;&#xA;&lt;li&gt;Run manual test against feature Volume, Live migration and Backup and rancher integration&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Check can download support bundle correctly, check can access every file without empty&lt;/p&gt;</description>
    </item>
    <item>
      <title>Harvester supports event log</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2748_harvester_supports_event_log/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2748_harvester_supports_event_log/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2748&#34;&gt;https://github.com/harvester/harvester/issues/2748&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Verified this feature has been implemented.&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-information&#34;&gt;Test Information&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Environment: &lt;strong&gt;qemu/KVM 3 nodes&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Harvester Version: &lt;strong&gt;master-250f41e4-head&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ui-source&lt;/strong&gt; Option: &lt;strong&gt;Auto&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install &lt;em&gt;Graylog&lt;/em&gt; via docker[^1]&lt;/li&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard then navigate to &lt;em&gt;Monitoring &amp;amp; Logging/Logging&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;strong&gt;Cluster Output&lt;/strong&gt; with following:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: gelf-evts&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: &lt;code&gt;Logging/Event&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: GELF&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt;: &lt;code&gt;&amp;lt;Graylog_IP&amp;gt;, &amp;lt;Graylog_Port&amp;gt;, &amp;lt;UDP&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;strong&gt;Cluster Flow&lt;/strong&gt; with following:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: gelf-flow&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt; of Matches: &lt;code&gt;Event&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cluster Outputs&lt;/strong&gt;: &lt;code&gt;gelf-evts&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create an Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create a vm &lt;code&gt;vm1&lt;/code&gt; and start it&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;code&gt;Graylog&lt;/code&gt; dashboard then navigate to search&lt;/li&gt;&#xA;&lt;li&gt;Select update frequency&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/191725169-d1203674-13d8-487b-9fa2-e1d9394fa5c0.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;New logs should be posted continuously.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;code-snippets-to-setup-graylog&#34;&gt;code snippets to setup Graylog&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --name mongo -d mongo:4.2.22-rc0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sysctl -w vm.max_map_count&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;262145&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --name elasticsearch -p 9200:9200 -p 9300:9300 -e xpack.security.enabled&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false  -e node.name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;es01 -it docker.elastic.co/elasticsearch/elasticsearch:6.8.23&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --name graylog --link mongo --link elasticsearch -p 9000:9000 -p 12201:12201 -p 1514:1514 -p 5555:5555 -p 12202:12202 -p 12202:12202/udp -e GRAYLOG_PASSWORD_SECRET&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Graypass3WordMor!e&amp;#34;&lt;/span&gt; -e GRAYLOG_ROOT_PASSWORD_SHA2&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;899e9793de44cbb14f48b4fce810de122093d03705c0971752a5c15b0fa1ae03   -e GRAYLOG_HTTP_EXTERNAL_URI&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://127.0.0.1:9000/&amp;#34;&lt;/span&gt;  -d graylog/graylog:4.3.5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;Login to &lt;em&gt;Graylog&lt;/em&gt; dashboard by the URL &lt;code&gt;http://&amp;lt;server_ip&amp;gt;:9000/&lt;/code&gt; with &lt;code&gt;admin&lt;/code&gt;/&lt;code&gt;ROOT_PASSWORDa1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;System/Inputs&lt;/em&gt; then select input &lt;strong&gt;GELF UDP&lt;/strong&gt;, update the port to &lt;code&gt;12202&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/191723749-7d796243-5996-4884-90b4-d8227f81adc5.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Harvester supports kube-audit log</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2747_harvester_supports_kube-audit_log/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2747_harvester_supports_kube-audit_log/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2747&#34;&gt;https://github.com/harvester/harvester/issues/2747&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install &lt;em&gt;Graylog&lt;/em&gt; via docker[^1]&lt;/li&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard then navigate to &lt;em&gt;Monitoring &amp;amp; Logging/Logging&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;strong&gt;Cluster Output&lt;/strong&gt; with following:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: gelf-evts&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: &lt;code&gt;Audit Only&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: GELF&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt;: &lt;code&gt;&amp;lt;Graylog_IP&amp;gt;, &amp;lt;Graylog_Port&amp;gt;, &amp;lt;UDP&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;strong&gt;Cluster Flow&lt;/strong&gt; with following:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: gelf-flow&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt; of Matches: &lt;code&gt;Audit&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cluster Outputs&lt;/strong&gt;: &lt;code&gt;gelf-evts&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create an Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create a vm &lt;code&gt;vm1&lt;/code&gt; and start it&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;code&gt;Graylog&lt;/code&gt; dashboard then navigate to search&lt;/li&gt;&#xA;&lt;li&gt;Select update frequency&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/191725169-d1203674-13d8-487b-9fa2-e1d9394fa5c0.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;New logs should be posted continuously.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;code-snippets-to-setup-graylog&#34;&gt;code snippets to setup Graylog&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --name mongo -d mongo:4.2.22-rc0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sysctl -w vm.max_map_count&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;262145&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --name elasticsearch -p 9200:9200 -p 9300:9300 -e xpack.security.enabled&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false  -e node.name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;es01 -it docker.elastic.co/elasticsearch/elasticsearch:6.8.23&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --name graylog --link mongo --link elasticsearch -p 9000:9000 -p 12201:12201 -p 1514:1514 -p 5555:5555 -p 12202:12202 -p 12202:12202/udp -e GRAYLOG_PASSWORD_SECRET&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Graypass3WordMor!e&amp;#34;&lt;/span&gt; -e GRAYLOG_ROOT_PASSWORD_SHA2&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;899e9793de44cbb14f48b4fce810de122093d03705c0971752a5c15b0fa1ae03   -e GRAYLOG_HTTP_EXTERNAL_URI&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://127.0.0.1:9000/&amp;#34;&lt;/span&gt;  -d graylog/graylog:4.3.5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;Login to &lt;em&gt;Graylog&lt;/em&gt; dashboard by the URL &lt;code&gt;http://&amp;lt;server_ip&amp;gt;:9000/&lt;/code&gt; with &lt;code&gt;admin&lt;/code&gt;/&lt;code&gt;ROOT_PASSWORDa1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;System/Inputs&lt;/em&gt; then select input &lt;strong&gt;GELF UDP&lt;/strong&gt;, update the port to &lt;code&gt;12202&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/191723749-7d796243-5996-4884-90b4-d8227f81adc5.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Harvester uses active-backup as the default bond mode</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2472_harvester_uses_active-backup_as_the_default_bond_mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2472_harvester_uses_active-backup_as_the_default_bond_mode/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2472&#34;&gt;https://github.com/harvester/harvester/issues/2472&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/184838334-a723f066-8eef-4cbc-ab66-6e02b758823d.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/184839241-3702fa7c-950e-4b51-8c18-d29d4121f848.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester via ISO&lt;/li&gt;&#xA;&lt;li&gt;The default &lt;strong&gt;Bond Mode&lt;/strong&gt; should select &lt;code&gt;active-backup&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Ater installed with &lt;code&gt;active-backup&lt;/code&gt; mode, login to console&lt;/li&gt;&#xA;&lt;li&gt;Execute &lt;code&gt;cat /etc/sysconfig/network/ifcfg-harvester-mgmt&lt;/code&gt;, &lt;strong&gt;BONDING_MODULE_OPTS&lt;/strong&gt; should contains &lt;code&gt;mode=active-backup&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Image filtering by labels</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2319-image-filtering-by-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2319-image-filtering-by-labels/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2319&#34;&gt;#2319&lt;/a&gt; [FEATURE] Image filtering by labels&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Image&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Upload several images and add related label&lt;/li&gt;&#xA;&lt;li&gt;Go to the image list page&lt;/li&gt;&#xA;&lt;li&gt;Add filter according to test plan 1&lt;/li&gt;&#xA;&lt;li&gt;Go to VM creation page&lt;/li&gt;&#xA;&lt;li&gt;Check the image list and search by name&lt;/li&gt;&#xA;&lt;li&gt;Import Harvester in Rancher&lt;/li&gt;&#xA;&lt;li&gt;Go to cluster management page&lt;/li&gt;&#xA;&lt;li&gt;Create a RKE2 cluster&lt;/li&gt;&#xA;&lt;li&gt;Check the image list and search by name&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;h4 id=&#34;test-result-1&#34;&gt;Test Result 1:&lt;/h4&gt;&#xA;&lt;p&gt;The image list page can be filtered by label in the following cases&lt;/p&gt;</description>
    </item>
    <item>
      <title>Image filtering by labels (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2474-image-filtering-by-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2474-image-filtering-by-labels/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2474&#34;&gt;#2474&lt;/a&gt; [backport v1.0] [FEATURE] Image filtering by labels&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Image&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Upload several images and add related label&lt;/li&gt;&#xA;&lt;li&gt;Go to the image list page&lt;/li&gt;&#xA;&lt;li&gt;Add filter according to test plan 1&lt;/li&gt;&#xA;&lt;li&gt;Go to VM creation page&lt;/li&gt;&#xA;&lt;li&gt;Check the image list and search by name&lt;/li&gt;&#xA;&lt;li&gt;Import Harvester in Rancher&lt;/li&gt;&#xA;&lt;li&gt;Go to cluster management page&lt;/li&gt;&#xA;&lt;li&gt;Create a RKE2 cluster&lt;/li&gt;&#xA;&lt;li&gt;Check the image list and search by name&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;The image list page can be filtered by label in the following cases&lt;/p&gt;</description>
    </item>
    <item>
      <title>Image handling consistency between terraform data resource and Harvester UI created image</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2443-image-consistency-terraform-data-harvester-ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2443-image-consistency-terraform-data-harvester-ui/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2443&#34;&gt;#2443&lt;/a&gt; [BUG] Image handling inconsistency between &amp;ldquo;Harvester Terraform harvester_image data source&amp;rdquo; vs. &amp;ldquo;UI created Image&amp;rdquo;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Terraform&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Download latest terraform-provider &lt;a href=&#34;https://github.com/harvester/terraform-provider-harvester/releases/download/v0.5.1/terraform-provider-harvester_0.5.1_linux_amd64.zip&#34;&gt;terraform-provider-harvester_0.5.1_linux_amd64.zip&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Extra the zip file&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create the install-terraform-provider-harvester.sh with the following content&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#!/usr/bin/env bash&#xA;[[ -n $DEBUG ]] &amp;amp;&amp;amp; set -x&#xA;set -eou pipefail&#xA;&#xA;usage() {&#xA;    cat &amp;lt;&amp;lt;HELP&#xA;USAGE:&#xA;    install-terraform-provider-harvester.sh&#xA;HELP&#xA;}&#xA;&#xA;version=0.5.1&#xA;arch=linux_amd64&#xA;terraform_harvester_provider_bin=./terraform-provider-harvester&#xA;&#xA;terraform_harvester_provider_dir=&amp;#34;${HOME}/.terraform.d/plugins/registry.terraform.io/harvester/harvester/${version}/${arch}/&amp;#34;&#xA;mkdir -p &amp;#34;${terraform_harvester_provider_dir}&amp;#34;&#xA;cp ${terraform_harvester_provider_bin} &amp;#34;${terraform_harvester_provider_dir}/terraform-provider-harvester_v${version}&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Rename the extraced &lt;code&gt;terraform-provider-harvester_v0.5.1&lt;/code&gt; to &lt;code&gt;terraform-provider-harvester&lt;/code&gt; in the same folder with the install script&lt;/p&gt;</description>
    </item>
    <item>
      <title>Image naming with inline CSS (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2563-image-naming-inline-css/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2563-image-naming-inline-css/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2563&#34;&gt;#2563&lt;/a&gt; [[BUG] harvesterhci.io.virtualmachineimage spec.displayName displays differently in single view of image&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Images&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Go to images&lt;/li&gt;&#xA;&lt;li&gt;Click &amp;ldquo;Create&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;Upload an image or leverage an url - but name the image something like:&#xA;&lt;code&gt;&amp;lt;strong&amp;gt;&amp;lt;em&amp;gt;something_interesting&amp;lt;/em&amp;gt;&amp;lt;/strong&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Wait for upload to complete.&lt;/li&gt;&#xA;&lt;li&gt;Observe the display name within the list of images&lt;/li&gt;&#xA;&lt;li&gt;Compare that to clicking into the single image and viewing it&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The list view naming would be the same as the single view of the image&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Image upload does not start when HTTP Proxy is configured</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2436-2524-image-upload-failed-when-http-proxy-configured/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2436-2524-image-upload-failed-when-http-proxy-configured/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2436&#34;&gt;#2436&lt;/a&gt; [BUG] Image upload does not start when HTTP Proxy is configured&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2524&#34;&gt;#2524&lt;/a&gt; [backport v1.0] [BUG] Image upload does not start when HTTP Proxy is configured&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Image&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Clone ipxe-example vagrant project &lt;a href=&#34;https://github.com/harvester/ipxe-examples&#34;&gt;https://github.com/harvester/ipxe-examples&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Edit settings.yml&lt;/li&gt;&#xA;&lt;li&gt;Set &lt;code&gt;harvester_network_config.offline=true&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a one node air gapped Harvester with a HTTP proxy server&lt;/li&gt;&#xA;&lt;li&gt;Access Harvester settings page&lt;/li&gt;&#xA;&lt;li&gt;Add the following http proxy configuration&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{&#xA;  &amp;#34;httpProxy&amp;#34;: &amp;#34;http://192.168.0.254:3128&amp;#34;,&#xA;  &amp;#34;httpsProxy&amp;#34;: &amp;#34;http://192.168.0.254:3128&amp;#34;,&#xA;  &amp;#34;noProxy&amp;#34;: &amp;#34;localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,cattle-system.svc,192.168.0.0/16,.svc,.cluster.local,example.com&amp;#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180407430-bfe9140b-b0c1-44dc-9463-a478a6f705d3.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improved resource reservation</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2347_improved_resource_reservation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2347_improved_resource_reservation/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2347&#34;&gt;https://github.com/harvester/harvester/issues/2347&lt;/a&gt;, &lt;a href=&#34;https://github.com/harvester/harvester/issues/1700&#34;&gt;https://github.com/harvester/harvester/issues/1700&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/174753699-f65e66c6-677b-4a3a-8f71-bfbb7a3b1bb2.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/174754418-c5786f38-5909-40ce-8076-c3eddcd3059a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-information&#34;&gt;Test Information&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Environment: &lt;strong&gt;Baremetal DL160G9 5 nodes&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Harvester Version: &lt;strong&gt;master-96b90714-head&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ui-source&lt;/strong&gt; Option: &lt;strong&gt;Auto&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login and Navigate to Hosts&lt;/li&gt;&#xA;&lt;li&gt;CPU/Memory/Storage should display &lt;strong&gt;Reserved&lt;/strong&gt; and &lt;strong&gt;Used&lt;/strong&gt; percentage.&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Host&amp;rsquo;s details&lt;/li&gt;&#xA;&lt;li&gt;Monitor Data should display &lt;strong&gt;Reserved&lt;/strong&gt; and &lt;strong&gt;Used&lt;/strong&gt; percentage, and should equals to the value in Hosts.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Install Harvester over previous GNU/Linux install</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2230-2450-install-harvester-over-gnu-linux/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2230-2450-install-harvester-over-gnu-linux/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2230&#34;&gt;#2230&lt;/a&gt; [BUG] harvester installer - always first attempt failed if before was linux installed&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2450&#34;&gt;#2450&lt;/a&gt; [backport v1.0][BUG] harvester installer - always first attempt failed if before was linux installed #2450&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Installtion&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install GNU/LInux LVM configuration&lt;/li&gt;&#xA;&lt;li&gt;reboot&lt;/li&gt;&#xA;&lt;li&gt;Install Harvester via ISO over previous linux install&lt;/li&gt;&#xA;&lt;li&gt;Verifiy Harvester install by changing password and logging in.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install should complete&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Instance metadata variables are not expanded</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2342_instance_metadata_variables_are_not_expanded/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2342_instance_metadata_variables_are_not_expanded/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2342&#34;&gt;https://github.com/harvester/harvester/issues/2342&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/177121301-f30bf8ec-0a70-4549-b11b-895161ee30ad.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Create Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create VM with following &lt;em&gt;CloudConfig&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## template: jinja&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#cloud-config&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;package_update&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;password&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;password&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;chpasswd&lt;/span&gt;: { &lt;span style=&#34;color:#f92672&#34;&gt;expire&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt; }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;sshpwauth&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;write_files&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;content&lt;/span&gt;: |&lt;span style=&#34;color:#e6db74&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      #!/bin/bash&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      vmName=$1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      echo &amp;#34;VM Name is: $vmName&amp;#34; &amp;gt; /home/cloudinitscript.log&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/home/exec_initscript.sh&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;permissions&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;0755&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;runcmd&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - - &lt;span style=&#34;color:#ae81ff&#34;&gt;systemctl&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#ae81ff&#34;&gt;enable&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - --&lt;span style=&#34;color:#ae81ff&#34;&gt;now&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#ae81ff&#34;&gt;qemu-guest-agent.service&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - - &lt;span style=&#34;color:#ae81ff&#34;&gt;echo&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{{ ds.meta_data.local_hostname }}&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - - &lt;span style=&#34;color:#ae81ff&#34;&gt;/home/exec_initscript.sh&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{{ ds.meta_data.local_hostname }}&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;packages&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#ae81ff&#34;&gt;qemu-guest-agent&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;&#xA;&lt;li&gt;login to the VM&#xA;&lt;ul&gt;&#xA;&lt;li&gt;file &lt;code&gt;/home/exec_initscript.sh&lt;/code&gt; should exists&lt;/li&gt;&#xA;&lt;li&gt;file &lt;code&gt;/home/cloudinitscript.log&lt;/code&gt; should exists&lt;/li&gt;&#xA;&lt;li&gt;execute &lt;code&gt;cat /home/cloudinitscript.log&lt;/code&gt;, VM name should be the same as the VM.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>ISO installation console UI Display</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2402-iso-installation-console-ui-display/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2402-iso-installation-console-ui-display/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2402&#34;&gt;#2402&lt;/a&gt; [FEATURE] Enhance the information display of ISO installation console UI (tty)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Harvester Installer&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;ISO install a single node Harvester&lt;/li&gt;&#xA;&lt;li&gt;Monitoring the ISO installation console UI&lt;/li&gt;&#xA;&lt;li&gt;ISO install a three node Harvester cluster&lt;/li&gt;&#xA;&lt;li&gt;Monitoring the ISO installation console UI of the first node&lt;/li&gt;&#xA;&lt;li&gt;Monitoring the ISO installation console UI of the second node&lt;/li&gt;&#xA;&lt;li&gt;Monitoring the ISO installation console UI of the third node&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;The ISO installation console UI enhancement can display correctly under the following single and multiple nodes scenarios.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ksmd support merge_across_node on/off</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2827_ksmd_support_merge_across_node_onoff_/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2827_ksmd_support_merge_across_node_onoff_/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2827&#34;&gt;https://github.com/harvester/harvester/issues/2827&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/193305898-48255477-1d19-48af-b132-3c019bd3f58b.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/193314630-7add9b5a-2d9e-49cb-8d3a-1075531145e8.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard and Navigate to hosts&lt;/li&gt;&#xA;&lt;li&gt;Edit &lt;em&gt;node1&lt;/em&gt;&amp;rsquo;s &lt;strong&gt;Ksmtuned&lt;/strong&gt; to &lt;code&gt;Run&lt;/code&gt; and &lt;strong&gt;ThresCoef&lt;/strong&gt; to &lt;code&gt;85&lt;/code&gt; then Click &lt;strong&gt;Save&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;em&gt;node1&lt;/em&gt;&amp;rsquo;s console, execute &lt;code&gt;kubectl get ksmtuned -oyaml --field-selector metadata.name=&amp;lt;node1&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Fields in &lt;code&gt;spec&lt;/code&gt; should be the same as Dashboard configured&lt;/li&gt;&#xA;&lt;li&gt;Create an image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create multiple VMs with 2Gi+ memory and schedule on &lt;code&gt;&amp;lt;node1&amp;gt;&lt;/code&gt; (memory size reflect to &lt;!-- raw HTML omitted --&gt;&amp;rsquo;s maximum size, total of VMs&amp;rsquo; memory should greater than 40%)&lt;/li&gt;&#xA;&lt;li&gt;Execute &lt;code&gt;watch -n1 grep . /sys/kernel/mm/ksm/*&lt;/code&gt; to monitor ksm&amp;rsquo;s status change&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/run&lt;/code&gt; should be update to &lt;code&gt;1&lt;/code&gt; after VMs started&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/page_*&lt;/code&gt; should updating continuously&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard then navigate to &lt;em&gt;Hosts&lt;/em&gt;, click &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;In the Tab of &lt;strong&gt;Ksmtuned&lt;/strong&gt;, values in Statistics section should not be &lt;code&gt;0&lt;/code&gt;.  (data in this section will be updated per min, so it not equals to console&amp;rsquo;s output was expected.)&lt;/li&gt;&#xA;&lt;li&gt;Update &lt;!-- raw HTML omitted --&gt;&amp;rsquo;s &lt;strong&gt;Ksmtuned&lt;/strong&gt; to check &lt;code&gt;Enable Merge Across Nodes&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Monitor data in Step.8 should reflect to:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/run&lt;/code&gt; should be updated to &lt;code&gt;2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/pages_*&lt;/code&gt; should be updated to &lt;code&gt;0&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Restart all VMs scheduling to &lt;code&gt;&amp;lt;node1&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Monitor data in Step.8 should reflect to:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/run&lt;/code&gt; should be updated to &lt;code&gt;1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;/sys/kernel/mm/ksm/pages_*&lt;/code&gt; should be updated and less than Step.8 monitored&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Limit VM of guest cluster in the same namespace</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2354-limit-vm-of-guest-cluster-same-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2354-limit-vm-of-guest-cluster-same-namespace/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2354&#34;&gt;#2354&lt;/a&gt; [FEATURE] Limit all VMs of the Harvester guest cluster in the same namespace&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Import Harvester from Rancher&lt;/li&gt;&#xA;&lt;li&gt;Access Harvester via virtualization management&lt;/li&gt;&#xA;&lt;li&gt;Create a test project and &lt;code&gt;ns1&lt;/code&gt; namespace&lt;/li&gt;&#xA;&lt;li&gt;Create two RKE1 node template, one set to default namespace and another set to ns1 namespace&lt;/li&gt;&#xA;&lt;li&gt;Create a RKE1 cluster, select the first pool using the first node template&lt;/li&gt;&#xA;&lt;li&gt;Create another pool, check can&amp;rsquo;t select the second node template&lt;/li&gt;&#xA;&lt;li&gt;Create a RKE2 cluster, set the first pool using specific namespace&lt;/li&gt;&#xA;&lt;li&gt;Add another machine pool, check it will automatically assigned the same namespace as the first pool&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;On RKE2 cluster page, when we select the first machine pool to specific namespace, then the second pool will automatically and can only use the same namespace as the first pool&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local cluster user input topology key</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2567-local-cluster-user-input-topology-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2567-local-cluster-user-input-topology-key/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2567&#34;&gt;#2567&lt;/a&gt; [BUG] Local cluster owner create Harvester cluster failed(RKE2)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Import Harvester from Rancher&lt;/li&gt;&#xA;&lt;li&gt;Create a standard user &lt;code&gt;local&lt;/code&gt; in Rancher User &amp;amp; Authentication&lt;/li&gt;&#xA;&lt;li&gt;Open Cluster Management page&lt;/li&gt;&#xA;&lt;li&gt;Edit cluster config&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/182781682-5cdd3c6a-517b-4f61-980d-3ee3cab86745.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Expand Member Roles&lt;/li&gt;&#xA;&lt;li&gt;Add &lt;code&gt;local&lt;/code&gt; user with Cluster Owner role&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/182781823-b71ba504-6488-4581-b50d-17c333496b8c.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create cloud credential of Harvester&lt;/li&gt;&#xA;&lt;li&gt;Login with &lt;code&gt;local&lt;/code&gt; user&lt;/li&gt;&#xA;&lt;li&gt;Open the provisioning RKE2 cluster page&lt;/li&gt;&#xA;&lt;li&gt;Select Advanced settings&lt;/li&gt;&#xA;&lt;li&gt;Add Pod Scheduling&lt;/li&gt;&#xA;&lt;li&gt;Select &lt;code&gt;Pods in these namespaces&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check can input Topology key value&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Login with cluster owner role and provision a RKE2 cluster&lt;/li&gt;&#xA;&lt;li&gt;we can input the topology key in the Topology key field of the pod selector&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/182752496-1fa49c1d-1b93-4147-9d5b-ef3a56d5bd2b.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Logging Output Filter</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2817-logging-output-filter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2817-logging-output-filter/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2817&#34;&gt;#2817&lt;/a&gt; [BUG]Logging Output needs filter&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Audit Logging&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create an &lt;code&gt;Audit Only&lt;/code&gt; type of Output named &lt;code&gt;audit-output&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193509247-09f5efd9-c43d-4514-bb84-55cd34c243b1.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create an &lt;code&gt;Audit Only&lt;/code&gt; type of ClusterOutput named &lt;code&gt;audit-cluster-output&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a Flow, select the type to &lt;code&gt;Logging&lt;/code&gt; or &lt;code&gt;Event&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check you &lt;strong&gt;can&amp;rsquo;t&lt;/strong&gt; select the &lt;code&gt;audit-output&lt;/code&gt; and &lt;code&gt;audiot-cluster-output&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;select the type to &lt;code&gt;Audit &lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check you &lt;strong&gt;can&lt;/strong&gt; select the &lt;code&gt;audit-output&lt;/code&gt; and &lt;code&gt;audit-cluster-output&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193510780-2f2f6d09-7ee6-433b-80ae-eb3879337513.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a ClusterFlow, select the type to &lt;code&gt;Logging&lt;/code&gt; or &lt;code&gt;Event&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check you &lt;strong&gt;can&amp;rsquo;t&lt;/strong&gt; select the &lt;code&gt;audiot-cluster-output&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;select the type to &lt;code&gt;Audit&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check you &lt;strong&gt;can&lt;/strong&gt; select the &lt;code&gt;audiot-cluster-output&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create an &lt;code&gt;logging/event&lt;/code&gt; type of Output named &lt;code&gt;logging-event-output&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193512327-8ff2cadf-d02d-453f-96e9-fbc7d64ad91f.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create an &lt;code&gt;logging/event&lt;/code&gt; type of ClusterOutput named &lt;code&gt;logging-event-cluster-output&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193512534-82d03364-b2f2-4bcb-b676-814ab5a9da6d.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a Flow, select the type to &lt;code&gt;Logging&lt;/code&gt; or &lt;code&gt;Event&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check you &lt;strong&gt;can&lt;/strong&gt; select the &lt;code&gt;logging-event-output&lt;/code&gt; and &lt;code&gt;logging-event-output&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a ClusterFlow, select the type to &lt;code&gt;Logging&lt;/code&gt; or &lt;code&gt;Event&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check you &lt;strong&gt;can&lt;/strong&gt; select the &lt;code&gt;logging-event-output&lt;/code&gt; and &lt;code&gt;logging-event-output&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;The &lt;code&gt;logging&lt;/code&gt; or the &lt;code&gt;Event&lt;/code&gt; type of &lt;code&gt;Flow&lt;/code&gt; can only select &lt;code&gt;Logging&lt;/code&gt; or &lt;code&gt;Event&lt;/code&gt; type of  &lt;code&gt;Output&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193512689-d56ddf11-0db8-4a10-ba9f-0425fb22710d.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193512719-4056e234-7e0a-49e4-9503-7bbd75075e0f.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multiple Disks Swapping Paths</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1874-extra-disk-swap-path/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1874-extra-disk-swap-path/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1874&#34;&gt;#1874&lt;/a&gt; Multiple Disks Swapping Paths&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare a harvester cluster (single node is sufficient)&lt;/li&gt;&#xA;&lt;li&gt;Prepare two additional disks and format both of them.&lt;/li&gt;&#xA;&lt;li&gt;Hotplug both disks and add them to the host via Harvester Dashboard (&amp;ldquo;Hosts&amp;rdquo; &amp;gt; &amp;ldquo;Edit Config&amp;rdquo; &amp;gt; &amp;ldquo;Disks&amp;rdquo;)&lt;/li&gt;&#xA;&lt;li&gt;Shutdown the host.&lt;/li&gt;&#xA;&lt;li&gt;Swap the address and slot of the two disks in order to make their dev paths swapped&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For libvirt environment, you can swap &lt;code&gt;&amp;lt;address&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;target&amp;gt;&lt;/code&gt; in the XML of the disk.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Reboot the host&lt;/li&gt;&#xA;&lt;li&gt;Navigate to the &amp;ldquo;Host&amp;rdquo; page, both disks should be healthy and scheduled.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Disks should be healthy and &lt;code&gt;scheduable&lt;/code&gt; after paths swapped.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Namespace pending on terminating</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2591_namespace_pending_on_terminating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2591_namespace_pending_on_terminating/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2591&#34;&gt;https://github.com/harvester/harvester/issues/2591&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/185376639-66d10a36-7f68-4689-9cd6-4ef6034f1aac.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to dashboard and navigate to &lt;em&gt;Namespaces&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Trying to delete any namespaces, prompt windows should shows warning message&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Negative change backup target while restoring backup</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2560-change-backup-target-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2560-change-backup-target-while-restoring/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2560&#34;&gt;#2560&lt;/a&gt; [BUG] VM hanging on restoring state when backup-target disconnected suddenly&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Category&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard then navigate to Advanced/Settings, setup backup-target with NFS or S3&lt;/li&gt;&#xA;&lt;li&gt;Create Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create VM vm1&lt;/li&gt;&#xA;&lt;li&gt;Take Backup vm1b from vm1&lt;/li&gt;&#xA;&lt;li&gt;Restore the backup vm1b to New/Existing VM&lt;/li&gt;&#xA;&lt;li&gt;When the VM still in restoring state, update backup-target settings to Use the default value then setup it back.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;You should get an error&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/182815277-98baa7bc-42d1-4404-be87-d60f3b6ba1fd.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Negative Harvester installer input same NIC IP and VIP</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2229-2377-negative-installer-same-nic-ip-and-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2229-2377-negative-installer-same-nic-ip-and-vip/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2229&#34;&gt;#2229&lt;/a&gt; [BUG] input nic ip and vip with same ip address in Harvester-Installer&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2377&#34;&gt;#2377&lt;/a&gt; [Backport v1.0.3] input nic ip and vip with same ip address in Harvester-Installer&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Installation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Boot into ISO installer&lt;/li&gt;&#xA;&lt;li&gt;Specify same IP for NIC and VIP&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Error message is displayed&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/83787952/178049998-e4eec9fe-d687-4efc-9618-940432d37a3d.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Negative Restore a backup while VM is restoring</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2559-negative-restore-backup-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2559-negative-restore-backup-restoring/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2559&#34;&gt;#2559&lt;/a&gt; [BUG] Backup unable to be restored and the VM can&amp;rsquo;t be deleted&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Backup/Restore&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to Dashboard then navigate to Advanced/Settings, setup backup-target with NFS or S3&lt;/li&gt;&#xA;&lt;li&gt;Create Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create VM vm1&lt;/li&gt;&#xA;&lt;li&gt;Take backup from vm1 as vm1b&lt;/li&gt;&#xA;&lt;li&gt;Take backup from vm1 as vm1b2&lt;/li&gt;&#xA;&lt;li&gt;Click Edit YAML of vm1b, update field status.source.spec.spec.domain.cpu.cores, increase 1&lt;/li&gt;&#xA;&lt;li&gt;Stop VM vm1&lt;/li&gt;&#xA;&lt;li&gt;Restore backup vm1b2 with Replace Existing&lt;/li&gt;&#xA;&lt;li&gt;Restore backup vm1b with Replace Existing when the VM vm1 still in state restoring&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;You should get an error when trying to restore.&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5370752/182722180-3e2f606b-beef-4f8b-8f33-8d235587db4b.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Networkconfigs function check</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2841-networkconfigs-function-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2841-networkconfigs-function-check/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2841&#34;&gt;#2841&lt;/a&gt; [FEATURE] Reorganize the networkconfigs UI&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Network&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Go to Cluster Networks/Configs&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a cluster network and provide the name&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/194039791-90a88cc0-879d-44d1-8b81-66a141c13732.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a Network Config&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Given the NICs that not been used by mgmt-bo (eg. &lt;code&gt;ens1f1&lt;/code&gt;)&lt;br&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/194040174-72813f78-868f-4d02-9f79-023c61632994.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Use default &lt;code&gt;active-backup&lt;/code&gt; mode&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Check the cluster network config in &lt;code&gt;Active&lt;/code&gt; status&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/194040378-3f98d96a-b92a-4769-aff8-b3d16465ac80.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Go to Networks&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a VLAN network&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Given the name and vlan id&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Select the cluster network from drop down list&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/194040798-28aced46-38f1-47f6-b1e1-2c98f27fbe9e.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>NIC ip and vip can&#39;t be the same in Harvester installer</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2229-2449-nic-ip-vip-different-harvester-installer-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2229-2449-nic-ip-vip-different-harvester-installer-copy/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2229&#34;&gt;#2229&lt;/a&gt; [BUG] input nic ip and vip with same ip address in Harvester-Installer&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2449&#34;&gt;#2449&lt;/a&gt; [backport v1.0] [BUG] input nic ip and vip with same ip address in Harvester-Installer&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Harvester Installer&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Launch ISO install process&lt;/li&gt;&#xA;&lt;li&gt;Set static node IP and gateway&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/173719118-1fd1609d-74f2-4f7d-9ff3-e1d21227e542.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Set the same node IP to the VIP field and press enter&lt;br&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/173719257-f60b55fd-0211-4fb7-8f45-3176eef4e577.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;During Harvester ISO installer process, when we set static node IP address with the same one as the VIP IP address&lt;/li&gt;&#xA;&lt;li&gt;There will be an error message to prevent the installation process&#xA;&lt;code&gt;VIP must not be the same as Management NIC IP&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/173719257-f60b55fd-0211-4fb7-8f45-3176eef4e577.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Node disk manager should prevent too many concurrent disk formatting occur within a short period</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1831_node_disk_manager_should_prevent_too_many_concurrent_disk_formatting_occur_within_a_short_period/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1831_node_disk_manager_should_prevent_too_many_concurrent_disk_formatting_occur_within_a_short_period/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1831&#34;&gt;https://github.com/harvester/harvester/issues/1831&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;criteria&#34;&gt;Criteria&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; exceed the maximum, there should have requeue devices which equals the exceeds&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; hit the maximum, there should not have requeue devices&lt;/li&gt;&#xA;&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; less than maximum, there should not have requeue devices&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/177324553-3b4800b2-9db9-45ec-a3cf-a630acb384cf.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any node having at least 6 additional disks&lt;/li&gt;&#xA;&lt;li&gt;Login to console and execute command to update log level to &lt;code&gt;debug&lt;/code&gt; and &lt;code&gt;max-concurrent-ops&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt; (On KVM environment, we have to set to &lt;code&gt;1&lt;/code&gt; to make sure the &lt;em&gt;requeuing&lt;/em&gt; will happen.)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;kubectl patch ds -n harvester-system harvester-node-disk-manager --type=json -p&#39;[{&amp;quot;op&amp;quot;:&amp;quot;replace&amp;quot;, &amp;quot;path&amp;quot;:&amp;quot;/spec/template/spec/containers/0/command&amp;quot;, &amp;quot;value&amp;quot;: [&amp;quot;node-disk-manager&amp;quot;, &amp;quot;--debug&amp;quot;, &amp;quot;--max-concurrent-ops&amp;quot;, &amp;quot;1&amp;quot;]}]&#39;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Watching log output by executing &lt;code&gt;kubectl get pods -A | grep node-disk | awk &#39;{system(&amp;quot;kubectl logs -fn &amp;quot;$1&amp;quot; &amp;quot;$2)}&#39;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to dashboard then navigate and edit host to add more than &lt;code&gt;1&lt;/code&gt; disks&lt;/li&gt;&#xA;&lt;li&gt;In the console log, should display &lt;code&gt;Hit maximum concurrent count. Requeue device &amp;lt;device id&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;In the dashboard, disks should be added successfully.&lt;/li&gt;&#xA;&lt;li&gt;Login to console and execute command to update log level to &lt;code&gt;debug&lt;/code&gt; and &lt;code&gt;max-concurrent-ops&lt;/code&gt; to &lt;code&gt;2&lt;/code&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;kubectl patch ds -n harvester-system harvester-node-disk-manager --type=json -p&#39;[{&amp;quot;op&amp;quot;:&amp;quot;replace&amp;quot;, &amp;quot;path&amp;quot;:&amp;quot;/spec/template/spec/containers/0/command&amp;quot;, &amp;quot;value&amp;quot;: [&amp;quot;node-disk-manager&amp;quot;, &amp;quot;--debug&amp;quot;, &amp;quot;--max-concurrent-ops&amp;quot;, &amp;quot;2&amp;quot;]}]&#39;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Watching log output by executing &lt;code&gt;kubectl get pods -A | grep node-disk | awk &#39;{system(&amp;quot;kubectl logs -fn &amp;quot;$1&amp;quot; &amp;quot;$2)}&#39;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to dashboard then navigate and edit host to add &lt;code&gt;2&lt;/code&gt; disks&lt;/li&gt;&#xA;&lt;li&gt;In the console log, there should not display &lt;code&gt;Hit maximum concurrent count. Requeue device &amp;lt;device id&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;In the dashboard, disks should be added successfully.&lt;/li&gt;&#xA;&lt;li&gt;Login to console and execute command to update log level to &lt;code&gt;debug&lt;/code&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;kubectl patch ds -n harvester-system harvester-node-disk-manager --type=json -p&#39;[{&amp;quot;op&amp;quot;:&amp;quot;replace&amp;quot;, &amp;quot;path&amp;quot;:&amp;quot;/spec/template/spec/containers/0/command&amp;quot;, &amp;quot;value&amp;quot;: [&amp;quot;node-disk-manager&amp;quot;, &amp;quot;--debug&amp;quot;]}]&#39;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Watching log output by executing &lt;code&gt;kubectl get pods -A | grep node-disk | awk &#39;{system(&amp;quot;kubectl logs -fn &amp;quot;$1&amp;quot; &amp;quot;$2)}&#39;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to dashboard then navigate and edit host to add less than &lt;code&gt;5&lt;/code&gt; disks&lt;/li&gt;&#xA;&lt;li&gt;In the console log, there should not display &lt;code&gt;Hit maximum concurrent count. Requeue device &amp;lt;device id&amp;gt;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;In the dashboard, disks should be added successfully.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Node join fails with self-signed certificate</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2736_node_join_fails_with_self-signed_certificate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2736_node_join_fails_with_self-signed_certificate/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2736&#34;&gt;https://github.com/harvester/harvester/issues/2736&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Verified this bug has been fixed.&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-information&#34;&gt;Test Information&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Environment: &lt;strong&gt;qemu/KVM 2 nodes&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Harvester Version: &lt;strong&gt;master-032742f0-head&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ui-source&lt;/strong&gt; Option: &lt;strong&gt;Auto&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Follow Steps in &lt;a href=&#34;https://github.com/harvester/harvester-installer/pull/335&#34;&gt;https://github.com/harvester/harvester-installer/pull/335&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Node promotion for topology label</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2325-node-promotion-for-topology-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2325-node-promotion-for-topology-label/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2325&#34;&gt;#2325&lt;/a&gt; [FEATURE] Harvester control plane should spread across failure domains&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Host&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install first node, the role of this node should be Management Node&lt;/li&gt;&#xA;&lt;li&gt;Install second node, the role of this node should be Compute Node, the second node shouldn&amp;rsquo;t be promoted to Management Node&lt;/li&gt;&#xA;&lt;li&gt;Add label topology.kubernetes.io/zone=zone1 to the first node&lt;/li&gt;&#xA;&lt;li&gt;Install third node, the second node and third node shouldn&amp;rsquo;t be promoted&lt;/li&gt;&#xA;&lt;li&gt;Add label topology.kubernetes.io/zone=zone1 to the second node, the second node and third node shouldn&amp;rsquo;t be promoted&lt;/li&gt;&#xA;&lt;li&gt;Add label topology.kubernetes.io/zone=zone3 to the third node, the second node and third node shouldn&amp;rsquo;t be promoted&lt;/li&gt;&#xA;&lt;li&gt;Change the value of label topology.kubernetes.io/zone from zone1 to zone2 in the second node, the second node and third node will be promoted to Management Node one by one&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;Checked can pass the following test scenarios.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Polish harvester machine config in Rancher</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2598-polish-harvester-machine-config-in-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2598-polish-harvester-machine-config-in-rancher/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2598&#34;&gt;#2598&lt;/a&gt; [BUG]Polish harvester machine config&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Import Harvester from Rancher&lt;/li&gt;&#xA;&lt;li&gt;Create a standard user &lt;code&gt;local&lt;/code&gt; in Rancher User &amp;amp; Authentication&lt;/li&gt;&#xA;&lt;li&gt;Open Cluster Management page&lt;/li&gt;&#xA;&lt;li&gt;Edit cluster config&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/182781682-5cdd3c6a-517b-4f61-980d-3ee3cab86745.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Expand Member Roles&lt;/li&gt;&#xA;&lt;li&gt;Add &lt;code&gt;local&lt;/code&gt; user with Cluster Owner role&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/182781823-b71ba504-6488-4581-b50d-17c333496b8c.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create cloud credential of Harvester&lt;/li&gt;&#xA;&lt;li&gt;Login with &lt;code&gt;local&lt;/code&gt; user&lt;/li&gt;&#xA;&lt;li&gt;Open the provisioning RKE2 cluster page&lt;/li&gt;&#xA;&lt;li&gt;Select Advanced settings&lt;/li&gt;&#xA;&lt;li&gt;Add Pod Scheduling&lt;/li&gt;&#xA;&lt;li&gt;Select &lt;code&gt;Pods in these namespaces&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the list of available pods with the namespaces options above&lt;/li&gt;&#xA;&lt;li&gt;Check can input Topology key value&lt;/li&gt;&#xA;&lt;li&gt;Access Harvester UI (Not from Rancher)&lt;/li&gt;&#xA;&lt;li&gt;Open project/namespace&lt;/li&gt;&#xA;&lt;li&gt;Create several namespaces&lt;/li&gt;&#xA;&lt;li&gt;Login &lt;code&gt;local&lt;/code&gt; user to Rancher&lt;/li&gt;&#xA;&lt;li&gt;Open the the provisioning RKE2 cluster page&lt;/li&gt;&#xA;&lt;li&gt;Check the available &lt;code&gt;Pods in these namespaces&lt;/code&gt; list have been updated&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;Checked the following test plan for &lt;code&gt;RKE2&lt;/code&gt; cluster are working as expected&lt;/p&gt;</description>
    </item>
    <item>
      <title>Press the Enter key in setting field shouldn&#39;t refresh page</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2569-press-enter-settings-should-not-refresh-page-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2569-press-enter-settings-should-not-refresh-page-copy/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2569&#34;&gt;#2569&lt;/a&gt; [BUG] Press the Enter key, the page will be refreshed automatically&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Settings&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Check every page have input filed in the Settings page&lt;/li&gt;&#xA;&lt;li&gt;Move cursor to any input field&lt;/li&gt;&#xA;&lt;li&gt;Click the &lt;code&gt;Enter&lt;/code&gt; button&lt;/li&gt;&#xA;&lt;li&gt;Check the page will not be automatically loaded&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;On v1.0.3 backport, when we press the &lt;code&gt;Enter&lt;/code&gt; key in the following page fields, it will not being refreshed automatically.&lt;/p&gt;&#xA;&lt;p&gt;Also checked the following pages&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prevent normal users create harvester-public namespace</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2485-prevent-normal-user-create-harvesterpublic-ns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2485-prevent-normal-user-create-harvesterpublic-ns/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2485&#34;&gt;#2485&lt;/a&gt; [FEATURE] [Harvester Node Driver v2] Prevent normal users from creating VMs in harvester-public namespace&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Import Harvester from Rancher&lt;/li&gt;&#xA;&lt;li&gt;Create standard &lt;code&gt;user&lt;/code&gt; in Rancher User &amp;amp; Authentication&lt;/li&gt;&#xA;&lt;li&gt;Edit Harvester in virtualization Management, assign Cluster Member role to user&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/191748214-50fd7290-e2ae-4910-9a27-c9b67c581886.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login with user&lt;/li&gt;&#xA;&lt;li&gt;Create cloud credential&lt;/li&gt;&#xA;&lt;li&gt;Provision an RKE2 cluster&lt;/li&gt;&#xA;&lt;li&gt;Check the namespace dropdown list&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Now the standard user with cluster member rights won&amp;rsquo;t display &lt;code&gt;harvester-public&lt;/code&gt; while user node driver to provision the RKE2 cluster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Project owner role on customized project open Harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2394-2395-project-owner-customized-project-open-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2394-2395-project-owner-customized-project-open-harvester/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2394&#34;&gt;#2394&lt;/a&gt; [BUG] Standard rancher user with project owner role of customized project to access Harvester get &amp;ldquo;404 Not Found&amp;rdquo; error&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2395&#34;&gt;#2395&lt;/a&gt; [backport v1.0] [BUG] Standard rancher user with project owner role of customized project to access Harvester get &amp;ldquo;404 Not Found&amp;rdquo; error&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Import Harvester from Rancher&lt;/li&gt;&#xA;&lt;li&gt;Access Harvester on virtualization management page&lt;/li&gt;&#xA;&lt;li&gt;Create a project test and namespace test under it&lt;/li&gt;&#xA;&lt;li&gt;Go to user authentication page&lt;/li&gt;&#xA;&lt;li&gt;Create a stand rancher user test&lt;/li&gt;&#xA;&lt;li&gt;Access Harvester in Rancher&lt;/li&gt;&#xA;&lt;li&gt;Set project owner role of test project to test user&lt;/li&gt;&#xA;&lt;li&gt;Login Rancher with test user&lt;/li&gt;&#xA;&lt;li&gt;Access the virtualization management page&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Now the standard user with project owner role can access harvester in virtualization management page correctly&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/174706597-f98ecc41-b479-4e5b-b163-02f43c1c6138.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Project owner should not see additional alert</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2288-2350-project-owner-should-not-see-alert-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2288-2350-project-owner-should-not-see-alert-copy/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2288&#34;&gt;#2288&lt;/a&gt; [BUG] The project-owner user will see an additional alert&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2350&#34;&gt;#2350&lt;/a&gt; [Backport v1.0] The project-owner user will see an additional alert&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Importing a harvester cluster in a rancher cluster&lt;/li&gt;&#xA;&lt;li&gt;enter the imported harvester cluster from the &lt;code&gt;Virtualization Management&lt;/code&gt; page&lt;/li&gt;&#xA;&lt;li&gt;create a new Project  (test), Create a test namespace in the test project.&lt;/li&gt;&#xA;&lt;li&gt;go to &lt;code&gt;Network&lt;/code&gt; page, add &lt;code&gt;vlan 1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;create a vm，  choose  &lt;code&gt;test namespace&lt;/code&gt;,  choose &lt;code&gt;vlan network&lt;/code&gt;, click save&lt;/li&gt;&#xA;&lt;li&gt;create a new user (test),  choose &lt;code&gt;Standard User&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;go to the &lt;code&gt;project page&lt;/code&gt;, edit &lt;code&gt;test&lt;/code&gt; Project, set &lt;code&gt;test&lt;/code&gt; user to Project Owner。&#xA;&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;&#xA;&lt;li&gt;login again with &lt;code&gt;test user&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;go to the vm page&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use rancher standard user &lt;code&gt;test&lt;/code&gt; with project owner permission to access Harvester.&#xA;Now there is no error alert on the created VM with vlan1 network&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/174733151-c8bcffdd-50e0-404e-a5b6-a9ff2f1a7387.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Promote remaining host when delete one</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2191-promote-remaining-host-when-delete-one/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2191-promote-remaining-host-when-delete-one/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2191&#34;&gt;#2191&lt;/a&gt; [BUG] Promote fail, cluster stays in Provisioning phase&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Host&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a 4-node Harvester cluster.&lt;/li&gt;&#xA;&lt;li&gt;Wait for three nodes to become control plane nodes (role is control-plane,etcd,master).&lt;/li&gt;&#xA;&lt;li&gt;Delete one of the control plane nodes.&lt;/li&gt;&#xA;&lt;li&gt;The remaining worker node should be promoted to a control plane node (role is control-plane,etcd,master).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Four nodes Harvester cluster status, before delete one of the control-plane node&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;n1-221021:/etc # kubectl get nodes&#xA;NAME        STATUS   ROLES                       AGE     VERSION&#xA;n1-221021   Ready    control-plane,etcd,master   17h     v1.24.7+rke2r1&#xA;n2-221021   Ready    control-plane,etcd,master   16h     v1.24.7+rke2r1&#xA;n3-221021   Ready    control-plane,etcd,master   15h     v1.24.7+rke2r1&#xA;n4-221021   Ready    &amp;lt;none&amp;gt;                      4m10s   v1.24.7+rke2r1&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Delete the third control-plane node, the 4th node can be promoted to control-plane role&lt;/p&gt;</description>
    </item>
    <item>
      <title>rancher-monitoring status when hosting NODE down</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2243-rancher-monitoring-status-when-hosting-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2243-rancher-monitoring-status-when-hosting-node-down/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2243&#34;&gt;#2243&lt;/a&gt; [BUG] rancher-monitoring is unusable when hosting NODE is (accidently) down&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Monitoring&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install a two nodes harvester cluster&lt;/li&gt;&#xA;&lt;li&gt;Check the Initial state of the 2 nodes Harvester cluster&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;harv-node1-0719:~ # kubectl get nodes&#xA;NAME              STATUS   ROLES                       AGE   VERSION&#xA;harv-node1-0719   Ready    control-plane,etcd,master   36m   v1.21.11+rke2r1&#xA;harv-node2-0719   Ready    &amp;lt;none&amp;gt;&#xA;&#xA;harv-node1-0719:~ # kubectl get pods -A | grep monitoring&#xA;cattle-monitoring-system    prometheus-rancher-monitoring-prometheus-0               3/3     Running     0          33m&#xA;cattle-monitoring-system    rancher-monitoring-grafana-d9c56d79b-ckbjc               3/3     Running     0          33m&#xA;&#xA;harv-node1-0719:~ # kubectl get pods prometheus-rancher-monitoring-prometheus-0 -n cattle-monitoring-system -o yaml | grep nodeName&#xA;  nodeName: harv-node1-0719&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Power off both nodes&lt;/p&gt;</description>
    </item>
    <item>
      <title>RBAC Cluster Owner</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2626-local-cluster-0owner/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2626-local-cluster-0owner/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2626&#34;&gt;#2626&lt;/a&gt; [BUG] Access Harvester project/namespace page hangs with no response timeout with local owner role from Rancher&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Authentication&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Import Harvester from Rancher&lt;/li&gt;&#xA;&lt;li&gt;Create a standard user local in Rancher User &amp;amp; Authentication&lt;/li&gt;&#xA;&lt;li&gt;Open Cluster Management page&lt;/li&gt;&#xA;&lt;li&gt;Edit cluster config&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/182781682-5cdd3c6a-517b-4f61-980d-3ee3cab86745.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Expand Member Roles&lt;/li&gt;&#xA;&lt;li&gt;Add local user with Cluster Owner role&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/182781823-b71ba504-6488-4581-b50d-17c333496b8c.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Logout Admin&lt;/li&gt;&#xA;&lt;li&gt;Login with local user&lt;/li&gt;&#xA;&lt;li&gt;Access Harvester from virtualization management&lt;/li&gt;&#xA;&lt;li&gt;Click the Project/Namespace page&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Local owner role user can access and display Harvester project/namespace place correctly without hanging to timeout&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>RBAC Create VM with restricted admin user</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2587-2116-create-vm-with-restricted-admin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2587-2116-create-vm-with-restricted-admin/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/harvester/harvester/issues/2587&#34;&gt;#2587&lt;/a&gt; [BUG] namespace on create VM is wrong when going through Rancher&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/harvester/harvester/issues/2116&#34;&gt;#2116&lt;/a&gt; [BUG] You can see cattle-monitoring-system volumes as restricted admin in Harvester&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Authentication&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Verification Steps&lt;/li&gt;&#xA;&lt;li&gt;Import Harvester into Rancher&lt;/li&gt;&#xA;&lt;li&gt;Create a restricted admin&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Volumes page&lt;/li&gt;&#xA;&lt;li&gt;Verify you only see associated Volumes&lt;/li&gt;&#xA;&lt;li&gt;Log out of admin and log in to restricted admin&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Harvester UI via virtualization management&lt;/li&gt;&#xA;&lt;li&gt;Open virtual machines tab&lt;/li&gt;&#xA;&lt;li&gt;Click create&lt;/li&gt;&#xA;&lt;li&gt;Verified that namespace was default.&lt;/li&gt;&#xA;&lt;li&gt;Create VM&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;VM should create with no errors&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Reinstall agent node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2665-2892-reinstall-agent-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2665-2892-reinstall-agent-node/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2665&#34;&gt;#2665&lt;/a&gt; [BUG] reinstall 1st node&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2892&#34;&gt;#2892&lt;/a&gt; [BUG] rancher-system-agent keeps showing error on a new node in an upgraded cluster&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Host&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;h3 id=&#34;test-plan-1-reinstall-management-node-and-agent-node-in-a-upgraded-cluster&#34;&gt;Test Plan 1: Reinstall management node and agent node in a upgraded cluster&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a 4-node v1.0.3 cluster.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Upgrade the master branch:&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Check the spec content in &lt;code&gt;provisioning.cattle.io/v1/clusters -&amp;gt; fleet-local&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196139161-7b6e6e84-692d-4f4f-a978-62fc50f64f06.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Check the iface content in &lt;code&gt;helm.cattle.io/v1/helmchartconfigs -&amp;gt; rke2-canal&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196141139-e4ff668e-287a-4722-865d-a7c0f145c862.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;   spec:                                                                                                                                                                                      │&#xA;│   valuesContent: |-                                                                                                                                                                        │&#xA;│     flannel:                                                                                                                                                                               │&#xA;│       iface: &amp;#34;&amp;#34; &#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Remove the agent node and 1 management node. Remove agent node (node 4)&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196138324-83b64d50-0236-4110-86c2-551ae046a406.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196138906-e77de8f0-d61d-4a91-82b2-e36cbbb6462a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Remove Pod Scheduling from harvester rke2 and rke1</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2642-remove-pod-scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2642-remove-pod-scheduling/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2642&#34;&gt;#2642&lt;/a&gt; [BUG] Remove Pod Scheduling from harvester rke2 and rke1&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;test-information&#34;&gt;Test Information&lt;/h2&gt;&#xA;&lt;p&gt;Test Environment: 1 node harvester on local kvm machine&#xA;Harvester version: v1.0-44fb5f1a-head (08/10)&#xA;Rancher version: v2.6.7-rc7&lt;/p&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare Harvester master node&lt;/li&gt;&#xA;&lt;li&gt;Prepare Rancher v2.6.7-rc7&lt;/li&gt;&#xA;&lt;li&gt;Import Harvester to Rancher&lt;/li&gt;&#xA;&lt;li&gt;Set ui-offline-preferred: Remote&lt;/li&gt;&#xA;&lt;li&gt;Go to Harvester Support page&lt;/li&gt;&#xA;&lt;li&gt;Download Kubeconfig&lt;/li&gt;&#xA;&lt;li&gt;Copy the content of Kubeconfig&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;h3 id=&#34;rke2-verification-steps&#34;&gt;RKE2 Verification Steps&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Open Harvester Host page then edit host config&lt;/li&gt;&#xA;&lt;li&gt;Add the following key value in the labels page:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;topology.kubernetes.io/zone: zone_bp&lt;/li&gt;&#xA;&lt;li&gt;topology.kubernetes.io/region: region_bp&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/183802450-a790b9a2-3e2c-4559-8f84-b5a768b9c83d.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Open the RKE2 provisioning page&lt;/li&gt;&#xA;&lt;li&gt;Expand the show advanced&lt;/li&gt;&#xA;&lt;li&gt;Click add Node selector in Node scheduling&lt;/li&gt;&#xA;&lt;li&gt;Use default Required priority&lt;/li&gt;&#xA;&lt;li&gt;Click Add Rule&lt;/li&gt;&#xA;&lt;li&gt;Provide the following key/value pairs&lt;/li&gt;&#xA;&lt;li&gt;topology.kubernetes.io/zone: zone_bp&lt;/li&gt;&#xA;&lt;li&gt;topology.kubernetes.io/region: region_bp&lt;/li&gt;&#xA;&lt;li&gt;Provide the following user data&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;password: 123456&#xA;chpasswd: { expire: False }&#xA;ssh_pwauth: True&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create the RKE2 guest cluster&lt;/li&gt;&#xA;&lt;li&gt;Go to Harvester Virtual Machine page&lt;/li&gt;&#xA;&lt;li&gt;Edit yaml of the RKE2 guest cluster&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/183804372-776736cd-d58a-4f16-828f-45903d99af60.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the node affinity label have written into the yaml&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/183804693-da78125c-0be8-4775-8665-37e0302877b9.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the guest cluster VM have no error message&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/183804967-9e043d1a-cdc1-4233-a32e-63ea04b98125.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check can provision RKE2 cluster correctly&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/183805796-956e5ee0-129f-41b8-99e1-03764b6f436e.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;rke1-verification-steps&#34;&gt;RKE1 Verification Steps&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Follow the steps 1 ~ 7 of the RKE2 verification section&lt;/li&gt;&#xA;&lt;li&gt;Go to Rancher Cluster Management page, add the RKE1 node template&lt;/li&gt;&#xA;&lt;li&gt;Click add Node selector in Node scheduling&lt;/li&gt;&#xA;&lt;li&gt;Use default Required priority&lt;/li&gt;&#xA;&lt;li&gt;Click Add Rule&lt;/li&gt;&#xA;&lt;li&gt;Provide the following key/value pairs&#xA;&lt;ul&gt;&#xA;&lt;li&gt;topology.kubernetes.io/zone: zone_bp&lt;/li&gt;&#xA;&lt;li&gt;topology.kubernetes.io/region: region_bp&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/183807452-643faffb-f9f9-46b8-8414-5a8224aa3e66.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create the RKE1 guest cluster&lt;/li&gt;&#xA;&lt;li&gt;Go to Harvester Virtual Machine page&lt;/li&gt;&#xA;&lt;li&gt;Edit yaml of the RKE1 guest cluster&lt;/li&gt;&#xA;&lt;li&gt;Check the node affinity label have written into the yaml&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/183807763-073b5622-5e77-4b23-989c-b610ff4d0586.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the guest cluster VM have no error message&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/183807827-005bd590-5b24-4bf8-a26d-b81a4bbdc3dc.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check can provision RKE1 cluster without error&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/183808305-d64c828f-7a08-4fc5-ac27-cbb879d57f2f.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Pod Scheduling did not appear on the RKE1 template provisioning advanced options&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/183801086-cc6de3d9-e420-42b5-af47-3ce7a60cd53c.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Restart Button Web VNC window</title>
      <link>https://harvester.github.io/tests/manual/_incoming/379-restart-button-web-vnc-window/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/379-restart-button-web-vnc-window/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/379&#34;&gt;#379&lt;/a&gt; [Question] Restart Button Web VNC window&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VM&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a new VM with Ubuntu desktop 20.04&lt;/li&gt;&#xA;&lt;li&gt;Prepare two volume&lt;/li&gt;&#xA;&lt;li&gt;Complete the installation process&lt;/li&gt;&#xA;&lt;li&gt;Open a web browser on Ubuntu desktop&lt;/li&gt;&#xA;&lt;li&gt;Check the shortcut keys combination&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The soft reboot keys can display and reboot correctly on Linux OS VM (Ubuntu desktop 20.04)&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/177100026-e67d0101-0a5b-433c-b9ab-e2b4af1a8d0f.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Restart/Stop VM with in progress Backup</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1702-do-not-allow-restart-or-stop-vm-when-backup-is-in-progress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1702-do-not-allow-restart-or-stop-vm-when-backup-is-in-progress/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1702&#34;&gt;#1702&lt;/a&gt; Don&amp;rsquo;t allow restart/stop vm when backup is in progress&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a VM.&lt;/li&gt;&#xA;&lt;li&gt;Create a VMBackup for it.&lt;/li&gt;&#xA;&lt;li&gt;Before VMBackup is done, stop/restart the VM. Verify VM can&amp;rsquo;t be stopped/restarted.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>restored VM can not be cloned</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2968_restored_vm_can_not_be_cloned/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2968_restored_vm_can_not_be_cloned/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2968&#34;&gt;https://github.com/harvester/harvester/issues/2968&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-information&#34;&gt;Test Information&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Environment: &lt;strong&gt;qemu/KVM 3 nodes&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Harvester Version: &lt;strong&gt;master-f96827b2-head&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ui-source&lt;/strong&gt; Option: &lt;strong&gt;Auto&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Follow &lt;strong&gt;Steps to reproduce&lt;/strong&gt; in &lt;a href=&#34;https://github.com/harvester/harvester/issues/2968#issue-1413026149&#34;&gt;https://github.com/harvester/harvester/issues/2968#issue-1413026149&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Additional regression test cases listed in &lt;a href=&#34;https://github.com/harvester/tests/issues/568#issue-1414534000&#34;&gt;https://github.com/harvester/tests/issues/568#issue-1414534000&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Restored VM name does not support uppercases</title>
      <link>https://harvester.github.io/tests/manual/_incoming/4544_restored_vm_name_does_not_support_uppercases/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/4544_restored_vm_name_does_not_support_uppercases/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/4544&#34;&gt;#4544&lt;/a&gt; [BUG] Unable to restore backup into new VM when the name starts with upper case&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Backup/Restore&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Setup &lt;code&gt;backup-target&lt;/code&gt; in &amp;lsquo;Advanced&amp;rsquo; -&amp;gt; &amp;lsquo;Settings&amp;rsquo;&lt;/li&gt;&#xA;&lt;li&gt;Create an image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create a VM &lt;code&gt;vm1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Take a VM backup &lt;code&gt;vm1b&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Go to &amp;lsquo;Backup &amp;amp; Snapshot&amp;rsquo;, restore &lt;code&gt;vm1b&lt;/code&gt; to new VM&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;positive-cases&#34;&gt;Positive Cases&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Single lower&lt;/li&gt;&#xA;&lt;li&gt;Lowers&lt;/li&gt;&#xA;&lt;li&gt;Lowers contains &amp;lsquo;.&amp;rsquo;&lt;/li&gt;&#xA;&lt;li&gt;Lowers contains &amp;lsquo;-&amp;rsquo;&lt;/li&gt;&#xA;&lt;li&gt;Lowers contains &amp;lsquo;.&amp;rsquo; and &amp;lsquo;-&amp;rsquo;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/2773781/270225975-17fea11e-a266-484d-a9d4-3e3af1624d45.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;negtive-cases&#34;&gt;Negtive Cases&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Upper&#xA;&lt;img src=&#34;https://github.com/harvester/harvester/assets/2773781/b2411e02-e0c1-4fef-b996-997c8c827862&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Restricted admin should not see cattle-monitoring-system volumes</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2116-2351-restricted-admin-no-cattle-monitoring-system-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2116-2351-restricted-admin-no-cattle-monitoring-system-volumes/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2116&#34;&gt;#2116&lt;/a&gt; [BUG] You can see cattle-monitoring-system volumes as restricted admin in Harvester&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2351&#34;&gt;#2351&lt;/a&gt; [Backport v1.0] You can see cattle-monitoring-system volumes as restricted admin in Harvester&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Import Harvester to Rancher&lt;/li&gt;&#xA;&lt;li&gt;Create restricted admin in Rancher&lt;/li&gt;&#xA;&lt;li&gt;Log out of rancher&lt;/li&gt;&#xA;&lt;li&gt;Log in as restricted admin&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Harvester ui in virtualization management&lt;/li&gt;&#xA;&lt;li&gt;Navigate to volumes page&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Login Rancher with restricted admin and access Harvester volume page.&#xA;Now it won&amp;rsquo;t display the cattle-monitoring-system volumes.&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/174289481-00e74f70-c773-47af-847c-9ca6ecd86e1d.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Setup and test local Harvester upgrade responder</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1849-setup-test-local-upgrade-responder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1849-setup-test-local-upgrade-responder/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1849&#34;&gt;#1849&lt;/a&gt; [Task] Improve Harvester upgrade responder&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Upgrade&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;p&gt;Follow the steps in &lt;a href=&#34;https://github.com/harvester/harvester/issues/1849#issuecomment-1180346017&#34;&gt;https://github.com/harvester/harvester/issues/1849#issuecomment-1180346017&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Clone &lt;a href=&#34;https://github.com/longhorn/upgrade-responder&#34;&gt;longhorn/upgrade-responder&lt;/a&gt; and checkout to &lt;a href=&#34;https://github.com/longhorn/upgrade-responder/releases/tag/v0.1.4&#34;&gt;v0.1.4&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Edit &lt;a href=&#34;https://github.com/longhorn/upgrade-responder/blob/master/config/response.json&#34;&gt;response.json&lt;/a&gt; content in config folder&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{&#xA;  &amp;#34;Versions&amp;#34;: [&#xA;    {&#xA;      &amp;#34;Name&amp;#34;: &amp;#34;v1.0.2-master-head&amp;#34;,&#xA;      &amp;#34;ReleaseDate&amp;#34;: &amp;#34;2022-06-15T00:00:00Z&amp;#34;,&#xA;      &amp;#34;Tags&amp;#34;: [&#xA;        &amp;#34;latest&amp;#34;,&#xA;        &amp;#34;test&amp;#34;,&#xA;        &amp;#34;dev&amp;#34;&#xA;      ]&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;Install InfluxDB&lt;/li&gt;&#xA;&lt;li&gt;Run longhorn/upgrade-responder with the command:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;go run main.go --debug start --upgrade-response-config config/response.json --influxdb-url http://localhost:8086 --geodb geodb/GeoLite2-City.mmdb --application-name harvester&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;5&#34;&gt;&#xA;&lt;li&gt;Check the local upgrade responder is running&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -X POST http://localhost:8314/v1/checkupgrade \&#xA;     -d &amp;#39;{ &amp;#34;appVersion&amp;#34;: &amp;#34;v1.0.2&amp;#34;, &amp;#34;extraInfo&amp;#34;: {}}&amp;#39;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;Create a new folder &lt;code&gt;v1.0.2-master-head&lt;/code&gt; for the http server&lt;/li&gt;&#xA;&lt;li&gt;Download the latest master head installation files&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://releases.rancher.com/harvester/master/harvester-master-amd64.iso&#34;&gt;https://releases.rancher.com/harvester/master/harvester-master-amd64.iso&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://releases.rancher.com/harvester/master/harvester-master-initrd-amd64&#34;&gt;https://releases.rancher.com/harvester/master/harvester-master-initrd-amd64&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://releases.rancher.com/harvester/master/harvester-master-rootfs-amd64.squashfs&#34;&gt;https://releases.rancher.com/harvester/master/harvester-master-rootfs-amd64.squashfs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://releases.rancher.com/harvester/master/harvester-master-vmlinuz-amd64&#34;&gt;https://releases.rancher.com/harvester/master/harvester-master-vmlinuz-amd64&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://releases.rancher.com/harvester/master/harvester-master-amd64.sha256&#34;&gt;https://releases.rancher.com/harvester/master/harvester-master-amd64.sha256&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol start=&#34;7&#34;&gt;&#xA;&lt;li&gt;Launch a python http server&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;python3 -m http.server&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;8&#34;&gt;&#xA;&lt;li&gt;Create a &lt;code&gt;version.yaml&lt;/code&gt; with the following content&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: harvesterhci.io/v1beta1&#xA;kind: Version&#xA;metadata:&#xA;  name: v1.0.2-master-head&#xA;  namespace: harvester-system&#xA;spec:&#xA;  isoChecksum: &amp;#39;0d5999471553e767cb0c4d7d1c82b00b884e994e5856d8feb90798ace523b7aa2145a5fc245e1d0073ce7b41c490979950f3f31f60a682c971aba63d562973e5&amp;#39;&#xA;  isoURL: http://192.168.122.224:8000/v1.0.2-master-head/harvester-master-amd64.iso&#xA;  releaseDate: &amp;#39;20220712&amp;#39;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;9&#34;&gt;&#xA;&lt;li&gt;Check the upgrade responder connection in harvester node&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -X POST http://192.168.122.224:8314/v1/checkupgrade      -d &amp;#39;{ &amp;#34;appVersion&amp;#34;: &amp;#34;v1.0.2&amp;#34;, &amp;#34;extraInfo&amp;#34;: {}}&amp;#39;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;10&#34;&gt;&#xA;&lt;li&gt;Check the iso download url connection in harvester node&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -output http://192.168.122.224:8000/harvester-master-amd64.iso&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;11&#34;&gt;&#xA;&lt;li&gt;Open Harvester settings, change upgrade-checker-url setting to our upgrade-responder URL.&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/178421503-bce7d8a2-1c02-403d-ae10-0c073fd2c8b0.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Change the release download url to our http server url&lt;br&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/178421631-2c88cdc5-a138-403b-98b0-d944b01861ef.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;ssh to harvester node, change to root, run &lt;code&gt;k9s&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Run : deployments -&amp;gt; / harvester -&amp;gt; select the harvester node&lt;/li&gt;&#xA;&lt;li&gt;Remove pods in deployment harvester-system/harvester to trigger check new versions.&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/178243762-fafc1c66-a3a2-4553-8aca-fb239da85677.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Wait for 5 - 10 minutes,&lt;/li&gt;&#xA;&lt;li&gt;Check Harvester dashboard and click the upgrade button&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/178248176-8f0a2d80-cb96-43ca-976a-7e47b5f89244.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Select the version and start the upgrade process&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Can select the prompted upgrade button by using the updated version of Harvester upgrade responder &lt;a href=&#34;https://github.com/longhorn/upgrade-responder&#34;&gt;https://github.com/longhorn/upgrade-responder&lt;/a&gt; (v0.1.4) by using the &lt;code&gt;upgrade-checker url&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/178421998-c2ad04e8-d912-46a0-8c35-dfd051aa0e86.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Support configuring a VLAN at the management interface in installer config</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1390_support_configuring_a_vlan_at_the_management_interface_in_installer_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1390_support_configuring_a_vlan_at_the_management_interface_in_installer_config/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1390&#34;&gt;https://github.com/harvester/harvester/issues/1390&lt;/a&gt;, &lt;a href=&#34;https://github.com/harvester/harvester/issues/1647&#34;&gt;https://github.com/harvester/harvester/issues/1647&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/192803102-5062546d-ec36-4ecc-a1f3-4e6ec6c7a620.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes from PXE Boot with configurd vlan with &lt;code&gt;vlan_id&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Harvester should installed successfully&lt;/li&gt;&#xA;&lt;li&gt;Login to console, execute &lt;code&gt;ip a s dev mgmt-br.&amp;lt;vlan_id&amp;gt;&lt;/code&gt; should have IP and accessible&lt;/li&gt;&#xA;&lt;li&gt;Dashboard should be accessible&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Support multiple VLAN physical interfaces</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2259-multiple-vlan-physical-interfaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2259-multiple-vlan-physical-interfaces/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2259&#34;&gt;#2259&lt;/a&gt; [FEATURE] Support multiple VLAN physical interfaces&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Network&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create cluster network &lt;code&gt;cn1&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196580297-57541544-48f5-4492-b3e9-a3450697f490.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a vlanconfig &lt;code&gt;config-n1&lt;/code&gt; on &lt;code&gt;cn1&lt;/code&gt; which applied to node 1 only&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196580491-0572c539-5828-4f2e-a0a6-59b40fcc549b.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Select an available NIC on the Uplink&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196580574-d38d59de-251c-4cf8-885d-655b76a78659.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a vlan, the cluster network &lt;code&gt;cn1&lt;/code&gt; vlanconfig and provide valid vlan id &lt;code&gt;91&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196584602-b663ca69-da9a-42e3-94e0-41e094ff1d0b.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create cluster network &lt;code&gt;cn2&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196580818-2ad7ed6b-db07-45e1-bb68-675f15b3fcf3.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a vlanconfig &lt;code&gt;config-n2&lt;/code&gt; on &lt;code&gt;cn2&lt;/code&gt; which applied to node 2 only&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196583186-566af433-a37e-4d19-a660-879ce8e7f020.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Select an available NIC on the Uplink&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196584299-ba819310-8242-4196-a4d8-bacc7912c41d.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/196584334-2b5ceaf1-3345-4cba-aaac-917dee70099f.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Support private registry for Rancher agent image in Air-gap</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2176-airgap-private-registry-rancher-agent-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2176-airgap-private-registry-rancher-agent-image/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2176&#34;&gt;#2176&lt;/a&gt; [Enhancement] Air-gap operation: Support using a private registry for Rancher agent image&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher Integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;h3 id=&#34;environment-setup&#34;&gt;Environment Setup&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Use vagrant-pxe-harvester to create a harvester cluster.&lt;/li&gt;&#xA;&lt;li&gt;Create another VM &lt;code&gt;myregistry&lt;/code&gt; and set it in the same virtual network.&lt;/li&gt;&#xA;&lt;li&gt;In &lt;code&gt;myregistry&lt;/code&gt; VM:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Install docker.&lt;/li&gt;&#xA;&lt;li&gt;Run following commands:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mkdir auth&#xA;docker run \&#xA;--entrypoint htpasswd \&#xA;httpd:2 -Bbn testuser testpassword &amp;gt; auth/htpasswd&#xA;&#xA;mkdir -p certs&#xA;&#xA;openssl req \&#xA;-newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \&#xA;-addext &amp;#34;subjectAltName = DNS:myregistry.local&amp;#34; \&#xA;-x509 -days 365 -out certs/domain.crt&#xA;&#xA;sudo mkdir -p /etc/docker/certs.d/myregistry.local:5000&#xA;sudo cp certs/domain.crt /etc/docker/certs.d/myregistry.local:5000/domain.crt&#xA;&#xA;docker run -d \&#xA;-p 5000:5000 \&#xA;--restart=always \&#xA;--name registry \&#xA;-v &amp;#34;$(pwd)&amp;#34;/certs:/certs \&#xA;-v &amp;#34;$(pwd)&amp;#34;/registry:/var/lib/registry \&#xA;-e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \&#xA;-e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \&#xA;-v &amp;#34;$(pwd)&amp;#34;/auth:/auth \&#xA;-e &amp;#34;REGISTRY_AUTH=htpasswd&amp;#34; \&#xA;-e &amp;#34;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&amp;#34; \&#xA;-e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \&#xA;registry:2&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Set IP and domain in &lt;code&gt;/etc/hosts&lt;/code&gt; to all VM (harvester and &lt;code&gt;myregistry&lt;/code&gt;). Remember to change the IP.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# vim /etc/hosts&#xA;192.168.0.50 myregistry.local&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login, pull, and push nginx image in &lt;code&gt;myregsitry&lt;/code&gt; VM:&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# username: testuser, password: testpassword&#xA;docker login myregistry.local:5000&#xA;docker pull nginx:latest&#xA;docker tag nginx:latest myregistry.local:5000:/nginx:latest&#xA;docker push myregistry.local:5000/nginx:latest&#xA;docker pull nginx:1.22&#xA;docker tag nginx:latest myregistry.local:5000:/nginx:1.22&#xA;docker push myregistry.local:5000/nginx:1.22&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Copy &lt;code&gt;certs/domain.crt&lt;/code&gt; content in &lt;code&gt;myregistry&lt;/code&gt; VM and paste it to &lt;code&gt;additional-ca&lt;/code&gt; setting.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test-plan-1&#34;&gt;Test Plan 1&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Update the Harvester containerd-registry setting to use private registry&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193785720-970d9f18-76c6-4818-ae6a-e6239baf8ebe.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/193785814-02c7b5b5-89da-43f0-a5f5-f36b8609a2df.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Support Volume Clone</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2293_support_volume_clone/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2293_support_volume_clone/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2293&#34;&gt;https://github.com/harvester/harvester/issues/2293&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Create an Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;code&gt;vm1&lt;/code&gt; with the image and an additional data volume &lt;code&gt;disk-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Volumes&lt;/em&gt;, clone &lt;em&gt;disk-0&lt;/em&gt; and &lt;em&gt;disk-1&lt;/em&gt; which attached to &lt;code&gt;vm1&lt;/code&gt; by clicking &lt;code&gt;Clone Volume&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;code&gt;vm2&lt;/code&gt; with cloned &lt;em&gt;disk-0&lt;/em&gt; and &lt;em&gt;disk-1&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;vm2&lt;/code&gt; should started successfully&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;code&gt;vm1&lt;/code&gt;, execute following commands:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;fdisk /dev/vdb&lt;/code&gt; with new and primary partition&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;mkfs.ext4 /dev/vdb1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;mkdir vdb &amp;amp;&amp;amp; mount -t ext4 /dev/vdb1 vdb&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;ping 127.0.0.1 | tee -a vdb/test&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Volumes, then clone &lt;code&gt;disk-1&lt;/code&gt; of &lt;strong&gt;vm1&lt;/strong&gt; into &lt;strong&gt;vm1-disk-2&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Virtual Machines, then update &lt;code&gt;vm1&lt;/code&gt; to add existing volume &lt;code&gt;vm1-disk-2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;code&gt;vm1&lt;/code&gt; then mount &lt;code&gt;/dev/vdb1&lt;/code&gt;(disk-1) and &lt;code&gt;/dev/vdc1&lt;/code&gt;(disk-2) into &lt;em&gt;vdb&lt;/em&gt; and &lt;em&gt;vdc&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;test file should be appeared in both folders of &lt;em&gt;vdb&lt;/em&gt; and &lt;em&gt;vdc&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;test file should not be empty in both folders of &lt;em&gt;vdb&lt;/em&gt; and &lt;em&gt;vdc&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Support Volume Snapshot</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2294_support_volume_snapshot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2294_support_volume_snapshot/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2294&#34;&gt;https://github.com/harvester/harvester/issues/2294&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Create an Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;code&gt;vm1&lt;/code&gt; with the image and an additional data volume &lt;code&gt;disk-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;code&gt;vm1&lt;/code&gt;, execute following commands:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;fdisk /dev/vdb&lt;/code&gt; with new and primary partition&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;mkfs.ext4 /dev/vdb1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;mkdir vdb &amp;amp;&amp;amp; mount -t ext4 /dev/vdb1 vdb&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;ping 127.0.0.1 | tee -a vdb/test&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Volumes, then click &lt;strong&gt;Take Snapshot&lt;/strong&gt; button on &lt;code&gt;disk-1&lt;/code&gt; of &lt;strong&gt;vm1&lt;/strong&gt; into &lt;strong&gt;vm1-disk-2&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Virtual Machines, then update &lt;code&gt;vm1&lt;/code&gt; to add existing volume &lt;code&gt;vm1-disk-2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;code&gt;vm1&lt;/code&gt; then mount &lt;code&gt;/dev/vdb1&lt;/code&gt;(disk-1) and &lt;code&gt;/dev/vdc1&lt;/code&gt;(disk-2) into &lt;em&gt;vdb&lt;/em&gt; and &lt;em&gt;vdc&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;test file should be appeared in both folders of &lt;em&gt;vdb&lt;/em&gt; and &lt;em&gt;vdc&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;test file should not be empty in both folders of &lt;em&gt;vdb&lt;/em&gt; and &lt;em&gt;vdc&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Sync harvester node&#39;s topology labels to rke2 guest-cluster&#39;s node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1418-sync-topology-labels-to-rke2-guest-cluster-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1418-sync-topology-labels-to-rke2-guest-cluster-node/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1418&#34;&gt;#1418&lt;/a&gt; Support topology aware scheduling of guest cluster workloads&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Add &lt;a href=&#34;https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesioregion&#34;&gt;topology labels&lt;/a&gt;(&lt;code&gt;topology.kubernetes.io/region&lt;/code&gt;, &lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt;) to the Harvester node:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;In Harvester UI, select &lt;code&gt;Hosts&lt;/code&gt; page.&lt;/li&gt;&#xA;&lt;li&gt;Click hosts&amp;rsquo; &lt;code&gt;Edit Config&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Select &lt;code&gt;Labels&lt;/code&gt; page, click &lt;code&gt;Add Labels&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Fill in, eg, Key: &lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt;, Value: &lt;code&gt;zone1&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create harvester guest-cluster from rancher-UI.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Wait for the guest-cluster to be created successfully and check if the guest-cluster node labels are consistent with the harvester nodes.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;In Rancher UI, select guest cluster.&lt;/li&gt;&#xA;&lt;li&gt;Select &lt;code&gt;Nodes&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Click each node, and check if contains labels.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Sync image display name to image labels</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2630-sync-image-display-name-to-image-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2630-sync-image-display-name-to-image-labels/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2630&#34;&gt;#2630&lt;/a&gt; [FEATURE] Sync image display_name to image labels&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Image&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Login harvester dashboard&lt;/li&gt;&#xA;&lt;li&gt;Access the Preference page&lt;/li&gt;&#xA;&lt;li&gt;Enable developer tool&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/187353113-495af11e-a3e5-4f8e-b03b-174b4f0660ea.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create an ubuntu focal image from url  &lt;a href=&#34;https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img&#34;&gt;https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img&lt;/a&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/187353177-52516c6d-8e68-4ac5-8b40-4006f6460773.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;View API of the created image&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/187353338-1f0691f3-b19a-4382-a26f-ab5897842474.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check can found the display name in the image API content&lt;/li&gt;&#xA;&lt;li&gt;Create the same ubuntu focal image from previous url again which would bring the same display name&lt;/li&gt;&#xA;&lt;li&gt;Check would be denied with error message&lt;/li&gt;&#xA;&lt;li&gt;Create a different ubuntu focal image with the same display name&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;In image API content, label &lt;code&gt;harvesterhci.io/imageDisplayName&lt;/code&gt; added to labels, and it&amp;rsquo;s value should be the displayName value&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/187353496-39c20027-f438-43de-a212-4f38b2dfbbae.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Image with the same display name in label would be denied by admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/187354352-ea2f08f3-01a1-4088-899b-d92e25433781.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Image with the same display name but different url would also be denied&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/187355241-845b09b5-953b-4e90-9948-ca8b025a6f5d.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>template with EFI (e2e_fe)</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2577-template-with-efi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2577-template-with-efi/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2577&#34;&gt;#2577&lt;/a&gt; [BUG] Boot in EFI mode not selected when creating multiple VM instances using VM template with EFI mode selected.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Template&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Go to Template, create a VM template with Boot in EFI mode selected.&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/9990804/181196319-d95a4d23-ea31-418c-9fd2-152821d56930.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Go to Virtual Machines, click Create, select Multiple instance, type in a random name prefix, and select the VM template we just created.&#xA;&lt;img src=&#34;image.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Go to Advanced Options, for now this EFI checkbox should be checked without any issue.&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/9990804/181196934-1249902f-47dd-44dc-bced-5911ffcfdf16.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a VM with template&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Check VM setting, the booting in EFI mode is checked&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/182343254-4a421a04-aa3f-471c-a258-930a98cc84d3.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that VM is running with UEFI using&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ubuntu@efi-01:~$ ls /sys/firmware/&#xA;acpi  dmi  efi  memmap&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Terraform import VLAN</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2261-terraform-import-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2261-terraform-import-vlan/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2261&#34;&gt;#2261&lt;/a&gt; [FEATURE] enhance terraform network to not pruge route_cidr and route_gateway&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;** Harvester Version &amp;lt;= &lt;code&gt;v1.0.3&lt;/code&gt;**&lt;/p&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Terraform&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Install terraform-harvester-provider (using master-head for testing)&lt;/li&gt;&#xA;&lt;li&gt;Execute &lt;code&gt;terraform init&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create the file network.tf as following snippets, then execute &lt;code&gt;terraform import harvester_clusternetwork.vlan vlan&lt;/code&gt; to import default vlan settings&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;resource &amp;#34;harvester_clusternetwork&amp;#34; &amp;#34;vlan&amp;#34; {&#xA;  name                 = &amp;#34;vlan&amp;#34;&#xA;  enable               = true&#xA;  default_physical_nic = &amp;#34;harvester-mgmt&amp;#34;&#xA;}&#xA;resource &amp;#34;harvester_network&amp;#34; &amp;#34;vlan1&amp;#34; {&#xA;  name      = &amp;#34;vlan1&amp;#34;&#xA;  namespace = &amp;#34;harvester-public&amp;#34;&#xA;&#xA;  vlan_id = 1&#xA;  route_mode = &amp;#34;auto&amp;#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol&gt;&#xA;&lt;li&gt;execute &lt;code&gt;terraform apply&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to dashboard then navigate to Advanced/Networks, make sure the Route Connectivity becomes Active&lt;/li&gt;&#xA;&lt;li&gt;Execute &lt;code&gt;terraform apply&lt;/code&gt; again and many more times&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Resources should not be changed or added or destroyed.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Terraformer import KUBECONFIG</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2604-terraformer-import-kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2604-terraformer-import-kubeconfig/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2604&#34;&gt;#2604&lt;/a&gt; [BUG] Terraformer imported VLAN always be 0&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Terraformer&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to dashboard, navigate to: Advanced/Settings -&amp;gt; then enabledvlan`&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Advanced/Networks and Create a Network which Vlan ID is not 0&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Support Page and Download KubeConfig file&lt;/li&gt;&#xA;&lt;li&gt;Initialize a terraform environment, download Harvester Terraformer&lt;/li&gt;&#xA;&lt;li&gt;Execute command &lt;code&gt;terraformer import harvester -r network&lt;/code&gt; to generate terraform configuration from the cluster&lt;/li&gt;&#xA;&lt;li&gt;Generated file &lt;code&gt;generated/harvester/network/network.tf&lt;/code&gt; should exists&lt;/li&gt;&#xA;&lt;li&gt;VLAN and other settings should match&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;vlan_id should be the same as the import cluster.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Testing Harvester Storage Tiering</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2147-testing-storage-tiering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2147-testing-storage-tiering/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2147&#34;&gt;#2147&lt;/a&gt; [[FEATURE] Storage Tiering&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Images&lt;/li&gt;&#xA;&lt;li&gt;Volumes&lt;/li&gt;&#xA;&lt;li&gt;VirtualMachines&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;test-setup-steps&#34;&gt;Test Setup Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Have a Harvester Node with 3 Disks in total (one main disk, two additional disks), ideally the two additional disks should be roughly 20/30Gi for testing&lt;/li&gt;&#xA;&lt;li&gt;Add the additional disks to the harvester node (you may first need to be on the node itself and do a &lt;code&gt;sudo gdisk /dev/sda&lt;/code&gt; and then &lt;code&gt;w&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; to write the disk identifier so that Harvester can recogonize the disk, note you shouldn&amp;rsquo;t need to build partitions)&lt;/li&gt;&#xA;&lt;li&gt;Add the disks to the Harvester node via: Hosts -&amp;gt; Edit Config -&amp;gt; Storage -&amp;gt; &amp;ldquo;Add Disk&amp;rdquo; (call-to-action), they should auto populate with available disks that you can add&lt;/li&gt;&#xA;&lt;li&gt;Save&lt;/li&gt;&#xA;&lt;li&gt;Navigate back to Hosts -&amp;gt; Host -&amp;gt; Edit Config -&amp;gt; Storage, then add a Host Tag, and a unique disk tag for every disk (including the main disk/default-disk)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;verification-steps-with-checks&#34;&gt;Verification Steps with Checks&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Navigate to Advanced -&amp;gt; Storage Classes -&amp;gt; Create (Call-To-Action), create a storageClass &amp;ldquo;sc-a&amp;rdquo;, specify nodeSelector (choose host), diskSelector (choose one of the unique disk tags), number of replicas (1-12)&lt;/li&gt;&#xA;&lt;li&gt;Also create a storageClass &amp;ldquo;sc-b&amp;rdquo;, specify nodeSelector (choose host), diskSelector (choose one of the unique disk tags), number of replicas (1-12)&lt;/li&gt;&#xA;&lt;li&gt;Create a new image img-a, specify storageClassName to sc-a&lt;/li&gt;&#xA;&lt;li&gt;Create a new vm vm1 use the image img-a&lt;/li&gt;&#xA;&lt;li&gt;Check the replicas number and location of rootdisk volume in longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Create a new volume volume-a by choose source=image img-a&lt;/li&gt;&#xA;&lt;li&gt;Add the volume volume-a to vm vm1&lt;/li&gt;&#xA;&lt;li&gt;Check the replicas number and location of volume volume-a in longhorn UI:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;volume-a, should also be seen in &lt;code&gt;kubectl get pv --all-namespaces&lt;/code&gt; (where &amp;ldquo;Claim&amp;rdquo; is volume-a) with the appropriate storage class&lt;/li&gt;&#xA;&lt;li&gt;also with something like &lt;code&gt;kubectl describe pv/pvc-your-uuid-from-get-pv-call-with-volume-a --all-namespaces&lt;/code&gt;:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;can audit volume attributes like:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    VolumeAttributes:      diskSelector=second&#xA;                   migratable=true&#xA;                   nodeSelector=node-2&#xA;                   numberOfReplicas=1&#xA;                   share=true&#xA;                   staleReplicaTimeout=30&#xA;                   storage.kubernetes.io/csiProvisionerIdentity=1665780638152-8081-driver.longhorn.io&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol&gt;&#xA;&lt;li&gt;you can also notice the increase of % used &amp;amp; % allocated in longhorn underneath the node and disk like: &lt;code&gt;Path:  /var/lib/harvester/extra-disks/uuid-of-disk&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Export the image volume volume-a to a new image img-b and specify storageClassName sc-b&lt;/li&gt;&#xA;&lt;li&gt;Create a new volume volume-b by choose source=image img-b&lt;/li&gt;&#xA;&lt;li&gt;Add the volume volume-b to vm vm1&lt;/li&gt;&#xA;&lt;li&gt;Check the replicas number and location of volume volume-b in longhorn UI:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;volume-b, should also be seen in &lt;code&gt;kubectl get pv --all-namespaces&lt;/code&gt; (where &amp;ldquo;Claim&amp;rdquo; is volume-b) with the appropriate storage class&lt;/li&gt;&#xA;&lt;li&gt;also with something like &lt;code&gt;kubectl describe pv/pvc-your-uuid-from-get-pv-call-with-volume-b --all-namespaces&lt;/code&gt;:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;can audit volume attributes like:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    VolumeAttributes:      diskSelector=second&#xA;                   migratable=true&#xA;                   nodeSelector=node-2&#xA;                   numberOfReplicas=1&#xA;                   share=true&#xA;                   staleReplicaTimeout=30&#xA;                   storage.kubernetes.io/csiProvisionerIdentity=1665780638152-8081-driver.longhorn.io&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;you can also notice the increase of % used &amp;amp; % allocated in longhorn underneath the node and disk like: &lt;code&gt;Path:  /var/lib/harvester/extra-disks/uuid-of-disk&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create a volume snapshot snapshot-a from volume-a&lt;/li&gt;&#xA;&lt;li&gt;Create a new volume volume-c by choose Source=New and specify storageClassName sc-a&lt;/li&gt;&#xA;&lt;li&gt;Add the volume volume-c to vm vm1&lt;/li&gt;&#xA;&lt;li&gt;Check the replicas number and location of volume volume-c in longhorn UI:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;volume-c, should also be seen in &lt;code&gt;kubectl get pv --all-namespaces&lt;/code&gt; (where &amp;ldquo;Claim&amp;rdquo; is volume-c) with the appropriate storage class&lt;/li&gt;&#xA;&lt;li&gt;also with something like &lt;code&gt;kubectl describe pv/pvc-your-uuid-from-get-pv-call-with-volume-c --all-namespaces&lt;/code&gt;:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;can audit volume attributes like:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    VolumeAttributes:      diskSelector=second&#xA;                   migratable=true&#xA;                   nodeSelector=node-2&#xA;                   numberOfReplicas=1&#xA;                   share=true&#xA;                   staleReplicaTimeout=30&#xA;                   storage.kubernetes.io/csiProvisionerIdentity=1665780638152-8081-driver.longhorn.io&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;you can also notice the increase of % used &amp;amp; % allocated in longhorn underneath the node and disk like: &lt;code&gt;Path:  /var/lib/harvester/extra-disks/uuid-of-disk&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create a volume snapshot snapshot-c from volume-c&lt;/li&gt;&#xA;&lt;li&gt;Restore the volume snapshot snapshot-c to a new volume volume-restore-c and specify storageClassName sc-b, attach volume-restore-c to vm1&lt;/li&gt;&#xA;&lt;li&gt;Check the replicas number and location of volume volume-restore-c in longhorn UI:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;volume-restore-c, should also be seen in &lt;code&gt;kubectl get pv --all-namespaces&lt;/code&gt; (where &amp;ldquo;Claim&amp;rdquo; is volume-restore-c) with the appropriate storage class&lt;/li&gt;&#xA;&lt;li&gt;also with something like &lt;code&gt;kubectl describe pv/pvc-your-uuid-from-get-pv-call-with-volume-restore-c --all-namespaces&lt;/code&gt;:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;can audit volume attributes like:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    VolumeAttributes:      diskSelector=second&#xA;                   migratable=true&#xA;                   nodeSelector=node-2&#xA;                   numberOfReplicas=1&#xA;                   share=true&#xA;                   staleReplicaTimeout=30&#xA;                   storage.kubernetes.io/csiProvisionerIdentity=1665780638152-8081-driver.longhorn.io&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;you can also notice the increase of % used &amp;amp; % allocated in longhorn underneath the node and disk like: &lt;code&gt;Path:  /var/lib/harvester/extra-disks/uuid-of-disk&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Verify you should be able to edit the description of sc-a&lt;/li&gt;&#xA;&lt;li&gt;Verify that other elements should not be able to be edited for sc-a&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>The count of volume snapshots should not include VM&#39;s snapshots</title>
      <link>https://harvester.github.io/tests/manual/_incoming/3004-volume-snaphost-not-include-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/3004-volume-snaphost-not-include-vm/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/3004&#34;&gt;#3004&lt;/a&gt; [BUG] The count of volume snapshots should not include VM&amp;rsquo;s snapshots&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Volume&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a VM &lt;code&gt;vm1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Take a VM snapshot&lt;/li&gt;&#xA;&lt;li&gt;Check the volume snapshot page&lt;/li&gt;&#xA;&lt;li&gt;Check the VM snapshot page&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;When one VM is created&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/197482909-baf7d1f4-4032-4180-bb88-22aac8b9a8bc.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Only VM snap are created&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/197484294-46b89b29-78be-4d28-a33c-77aa525850a8.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;The count of volume snapshots should not include VM&amp;rsquo;s snapshots.&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/197484528-ed4c562b-782b-400e-99ec-fa97e292568d.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Topology aware scheduling of guest cluster workloads</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1418-2383-topology-scheduling-of-guest-cluster-workloads/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1418-2383-topology-scheduling-of-guest-cluster-workloads/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1418&#34;&gt;#1418&lt;/a&gt; [FEATURE] Support topology aware scheduling of guest cluster workloads&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2383&#34;&gt;#2383&lt;/a&gt; [backport v1.0.3] [FEATURE] Support topology aware scheduling of guest cluster workloads&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Environment preparation as above steps&lt;/li&gt;&#xA;&lt;li&gt;Access Harvester node config page&lt;/li&gt;&#xA;&lt;li&gt;Add the following node labels with values&#xA;&lt;ul&gt;&#xA;&lt;li&gt;topology.kubernetes.io/zone&lt;/li&gt;&#xA;&lt;li&gt;topology.kubernetes.io/region&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Provision an RKE2 cluster&lt;/li&gt;&#xA;&lt;li&gt;Wait for the provisioning complete&lt;/li&gt;&#xA;&lt;li&gt;Access RKE2 guest cluster&lt;/li&gt;&#xA;&lt;li&gt;Access the RKE2 cluster in Cluster Management page&lt;/li&gt;&#xA;&lt;li&gt;Click + to add another node&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/177774100-63c1a229-19d4-45f7-bd4e-8d2453c9149f.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Access the RKE2 cluster node page&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/177774234-ed001086-75a2-46e7-9638-0771cc790fad.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Wait until the second node created&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/177774368-0c8b6ac1-15f0-4a64-8945-85551dc85e4f.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Edit yaml of the second node&lt;/li&gt;&#xA;&lt;li&gt;Check the harvester node label have propagated  to the guest cluster node&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/177774559-8f278b2d-fff0-48ec-a62f-ceb3a9da8cc3.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;The topology encoded in the Harvester cluster node labels&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/177771658-1e3a8336-61c7-459d-9d4f-19e626ce9f23.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unable to stop VM which in starting state</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2263_unable_to_stop_vm_which_in_starting_state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2263_unable_to_stop_vm_which_in_starting_state/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2263&#34;&gt;https://github.com/harvester/harvester/issues/2263&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Create an Windows iso image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create the Windows VM by using the iso image&lt;/li&gt;&#xA;&lt;li&gt;When the VM in &lt;strong&gt;Starting&lt;/strong&gt; state, &lt;strong&gt;Stop&lt;/strong&gt; button should able to click and work as expected&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Upgrade guest cluster kubernetes version can also update the cloud provider chart version</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2546-upgrade-guest-k8s-version-upgrade-cloud-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2546-upgrade-guest-k8s-version-upgrade-cloud-provider/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2546&#34;&gt;#2546&lt;/a&gt; [BUG] Harvester Cloud Provider is not able to deploy upgraded container after upgrading the cluster&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rancher integration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare the previous stable Rancher rc version and Harvester&lt;/li&gt;&#xA;&lt;li&gt;Update &lt;code&gt;rke-metadata-config&lt;/code&gt; to &lt;code&gt;{&amp;quot;refresh-interval-minutes&amp;quot;:&amp;quot;1440&amp;quot;,&amp;quot;url&amp;quot;:&amp;quot;https://yufa-dev.s3.ap-east-1.amazonaws.com/data.json&amp;quot;}&lt;/code&gt; in global settings&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180735267-939e92e3-7fd5-4659-8bc8-ab14c95161d8.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Update the ui-dashboard-index to &lt;code&gt;https://releases.rancher.com/dashboard/latest/index.html&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Set &lt;code&gt;ui-offline-preferred&lt;/code&gt; to &lt;code&gt;Remote&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Refresh web page (ctrl + r)&lt;/li&gt;&#xA;&lt;li&gt;Open Create RKE2 cluster page&lt;/li&gt;&#xA;&lt;li&gt;Check the &lt;code&gt;show deprecated kubernetes patched versions&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180736528-feaa9615-ccf9-482b-9354-c2c9a6a4b23b.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Select &lt;code&gt;v1.23.8+rke2r1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Finish the RKE2 cluster provision&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180738516-3f429bba-22ab-4476-bebf-0ac2f87935c3.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the current cloud provider version in workload page&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180738877-56afcd55-e519-48d9-a8b8-3cbed91a1dfb.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Edit RKE2 cluster, upgrade the kubernetes version to &lt;code&gt;1.23.9-rc3+rke2r1&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180739231-e61ef680-5a9d-480b-9ac9-eda7839e17b6.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180739331-611b05d4-0c5d-4835-9da0-8c05b9cca027.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Wait for update finish&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180739876-dc409fa8-a9a6-406b-a614-085cea57121f.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;The cloud provider is upgrading&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180740637-5d1c6ce0-07ed-4a62-a364-f1b5e9fe473f.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;delete the old cloud provider version pod (v0.1.3)&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180740767-e6d5cdc2-c004-4c7a-8298-690775265002.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Wait for newer version cloud provider have been bumped&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180740875-38fa0cc0-c13a-4e39-ba46-5e869eadf087.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/180740998-80e451e5-ad91-4111-8abe-f51395427b9c.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;After upgrading the existing RKE2 guest cluster kubernetes version from older &lt;code&gt;v1.23.8+rke2r1&lt;/code&gt; to &lt;code&gt;1.23.9-rc3+rke2r1&lt;/code&gt;.&#xA;The Harvester cloud provider can successfully updated from &lt;code&gt;v0.1.3&lt;/code&gt; to &lt;code&gt;v0.1.4&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Upgrade Harvester on node that has bonded NICs for management interface</title>
      <link>https://harvester.github.io/tests/manual/_incoming/3045-upgrade-with-bonded-nic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/3045-upgrade-with-bonded-nic/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/3045&#34;&gt;#3045&lt;/a&gt; [BUG] Harvester Upgrade 1.0.3 to 1.1.0 does not handle multiple SLAVE in BOND for management interface&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Upgrade&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;This is to be done on a Harvester cluster where the NICs were configured to be bonded on install for the management interface. This can be done in one of two ways.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Single node virtualized environment&lt;/li&gt;&#xA;&lt;li&gt;Bare metal environment with at least two NICs (this should really be done on 10gig NICs, but can be done on gigabit)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Both NICs should be on the same VLAN/network with the same subnet&lt;/p&gt;</description>
    </item>
    <item>
      <title>Upgrade support of audit and event log</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2750-support-audit-event-log/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2750-support-audit-event-log/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2750&#34;&gt;#2750&lt;/a&gt; [FEATURE] Upgrade support of audit and event log&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Logging Audit&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare v1.0.3 cluster, single-node and multi-node need to be tested separately&lt;/li&gt;&#xA;&lt;li&gt;Upgrade to v1.1.0-rc2 / master-head&lt;/li&gt;&#xA;&lt;li&gt;The upgrade should be successful, if not, check log and POD errors&lt;/li&gt;&#xA;&lt;li&gt;After upgrade, check following PODs and files, there should be no error&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;Check both Single and Multi nodes upgrade of the following:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Check the following files and pods have no error&lt;/p&gt;</description>
    </item>
    <item>
      <title>VLAN Upgrade Test</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2734-vlan-upgrade-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2734-vlan-upgrade-test/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2734&#34;&gt;#2734&lt;/a&gt; [FEATURE] VLAN enhancement upgrading&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Upgrade&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;h3 id=&#34;test-plan-1-harvester-mgmt-vlan1&#34;&gt;Test plan 1: harvester-mgmt vlan1&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare a 3 nodes &lt;code&gt;v1.0.3&lt;/code&gt; Harvester cluster&lt;/li&gt;&#xA;&lt;li&gt;Enable network on &lt;code&gt;harvester-mgmt&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create vlan id &lt;code&gt;1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create two VMs, one set to vlan 1 and another use harvester-mgmt&lt;/li&gt;&#xA;&lt;li&gt;Perform manual upgrade to &lt;code&gt;v1.1.0&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test-plan-2--enps0-nic-with-valid-vlan&#34;&gt;Test plan 2:  enps0 NIC with valid vlan&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare a 3 nodes &lt;code&gt;v1.0.3&lt;/code&gt; Harvester cluster&lt;/li&gt;&#xA;&lt;li&gt;Enable network on another NIC (eg. &lt;code&gt;enp129s0&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Create vlan id &lt;code&gt;91&lt;/code&gt; on &lt;code&gt;enp129s0&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create two VMs, one set to vlan 91 and another use harvester-mgmt&lt;/li&gt;&#xA;&lt;li&gt;Perform manual upgrade to &lt;code&gt;v1.1.0&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test-plan-3-bond-mode-using-harvester-config-file&#34;&gt;Test plan 3: Bond mode using Harvester config file&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Edit the ipxe-example add two additional NICs in Vagrantfile&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;harvester_node.vm.network &amp;#39;private_network&amp;#39;,&#xA;        libvirt__network_name: &amp;#39;harvester&amp;#39;&#xA;    harvester_node.vm.network &amp;#39;private_network&amp;#39;,&#xA;        libvirt__network_name: &amp;#39;harvester&amp;#39;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Add the &lt;code&gt;harvester-vlan&lt;/code&gt; network in /ansible/roles/harvester/templates/config-create.yaml.j2&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;install:&#xA;mode: create&#xA;networks:&#xA;    harvester-mgmt:&#xA;    interfaces:&#xA;    - name: {{ settings[&amp;#39;harvester_network_config&amp;#39;][&amp;#39;cluster&amp;#39;][0][&amp;#39;mgmt_interface&amp;#39;] }}  # The management interface name&#xA;    method: dhcp&#xA;    bond0:&#xA;    interfaces:&#xA;    - name: {{ settings[&amp;#39;harvester_network_config&amp;#39;][&amp;#39;cluster&amp;#39;][0][&amp;#39;vagrant_interface&amp;#39;] }}&#xA;    method: dhcp&#xA;    harvester-vlan:&#xA;    interfaces:&#xA;    - name: ens7&#xA;    - name: ens8&#xA;    method: none&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Prepare a 1 nodes &lt;code&gt;v1.0.3&lt;/code&gt; Harvester cluster using ipex-example&lt;/li&gt;&#xA;&lt;li&gt;Check the &lt;code&gt;harvester-vlan&lt;/code&gt; link device status &lt;code&gt;ip -d l show dev harvester-vlan&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create several vlan based on &lt;code&gt;harvester-vlan&lt;/code&gt; interface&lt;/li&gt;&#xA;&lt;li&gt;Create a VM with &lt;code&gt;vlan 1&lt;/code&gt; network&lt;/li&gt;&#xA;&lt;li&gt;The harvester-vlan config before upgrade&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;harvester-vlan:&#xA;    interfaces:&#xA;    - name: ens7&#xA;        hwaddr: &amp;#34;&amp;#34;&#xA;    - name: ens8&#xA;        hwaddr: &amp;#34;&amp;#34;&#xA;    method: none&#xA;    ip: &amp;#34;&amp;#34;&#xA;    subnetmask: &amp;#34;&amp;#34;&#xA;    gateway: &amp;#34;&amp;#34;&#xA;    defaultroute: false&#xA;    bondoptions: {}&#xA;    mtu: 0&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Can successfully upgrade to v1.1.0&lt;/li&gt;&#xA;&lt;li&gt;Check the network bridge &lt;code&gt;mgmt-br&lt;/code&gt; exists &lt;code&gt;ip a&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check there is &lt;code&gt;vlan 1&lt;/code&gt; created on default cluster network (For Test plan1)&lt;/li&gt;&#xA;&lt;li&gt;Check there is vlan with id created on related cluster network (For Test plan2)&lt;/li&gt;&#xA;&lt;li&gt;Check the cluster networks contains &lt;code&gt;mgmt&lt;/code&gt; and &lt;code&gt;vlan&lt;/code&gt; (For Test plan3)&lt;/li&gt;&#xA;&lt;li&gt;Check the yaml content of vlan1 (For Test plan1)&lt;/li&gt;&#xA;&lt;li&gt;Check the yaml content of available vlan &lt;code&gt;91&lt;/code&gt; (For Test plan2)&lt;/li&gt;&#xA;&lt;li&gt;Check new network feature and UI work on v1.1.0&lt;/li&gt;&#xA;&lt;li&gt;Check new network feature and UI, &lt;code&gt;vlan1&lt;/code&gt; on &lt;code&gt;mgmt&lt;/code&gt;, &lt;code&gt;vlan91&lt;/code&gt; on &lt;code&gt;vlan&lt;/code&gt; (For Test plan2)&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>VM boot stress test</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2906-vm-boot-stress-test-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2906-vm-boot-stress-test-/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2906&#34;&gt;#2906&lt;/a&gt; [BUG] VM can’t boot due to filesystem corruption&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Volume&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create volume (Harvester, Longhorn storage class)&lt;/li&gt;&#xA;&lt;li&gt;Create volume from image&lt;/li&gt;&#xA;&lt;li&gt;Unmount volume from VM&lt;/li&gt;&#xA;&lt;li&gt;Delete volume in use and not in use&lt;/li&gt;&#xA;&lt;li&gt;Export volume to image&lt;/li&gt;&#xA;&lt;li&gt;Create VM from the exported image&lt;/li&gt;&#xA;&lt;li&gt;Edit volume to increase size&lt;/li&gt;&#xA;&lt;li&gt;Delete volume in use&lt;/li&gt;&#xA;&lt;li&gt;Clone volume&lt;/li&gt;&#xA;&lt;li&gt;Take volume snapshot&lt;/li&gt;&#xA;&lt;li&gt;Restore volume snapshot&lt;/li&gt;&#xA;&lt;li&gt;Utilize the E2E test in harvester/test repo to prepare a script to continues run step 1-11 at lease 100 runs&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Pass more than 300 rounds of the I/O write test, &lt;strong&gt;Should Not&lt;/strong&gt; encounter data corruption issue and VM is alive&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;opensuse:~ # xfs_info /dev/vda3&#xA;meta-data=/dev/vda3              isize=512    agcount=13, agsize=653887 blks&#xA;        =                       sectsz=512   attr=2, projid32bit=1&#xA;        =                       crc=1        finobt=1, sparse=0, rmapbt=0&#xA;        =                       reflink=0&#xA;data     =                       bsize=4096   blocks=7858427, imaxpct=25&#xA;        =                       sunit=0      swidth=0 blks&#xA;naming   =version 2              bsize=4096   ascii-ci=0, ftype=1&#xA;log      =internal log           bsize=4096   blocks=2560, version=2&#xA;        =                       sectsz=512   sunit=0 blks, lazy-count=1&#xA;realtime =none                   extsz=4096   blocks=0, rtextents=0&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>VM Import/Migration</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2274-vm-import/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2274-vm-import/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2274&#34;&gt;#2274&lt;/a&gt; [Feature] VM Import/Migration&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Virtual Machine&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;test-information&#34;&gt;Test Information&lt;/h2&gt;&#xA;&lt;p&gt;Test Environment:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1 node harvester on local kvm machine&lt;/li&gt;&#xA;&lt;li&gt;Harvester version: v1.1.0-rc1&lt;/li&gt;&#xA;&lt;li&gt;Vsphere: 7.0&lt;/li&gt;&#xA;&lt;li&gt;Openstack: Simulated using running devstack&lt;/li&gt;&#xA;&lt;li&gt;Download kubeconfig for harvester cluster&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;environment-setup&#34;&gt;Environment Setup&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare Harvester master node&lt;/li&gt;&#xA;&lt;li&gt;Prepare vsphere setup (or use existing setup)&lt;/li&gt;&#xA;&lt;li&gt;Prepare a devstack cluster (Openstack 16.2) (stable/train)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;openstack-setup&#34;&gt;OpenStack Setup&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare a baremetal or virtual machine to host the OpenStack service&lt;/li&gt;&#xA;&lt;li&gt;For automated installation on virtual machine, please refer to the &lt;code&gt;cloud init user data&lt;/code&gt; in&#xA;&lt;a href=&#34;https://github.com/harvester/tests/issues/522#issuecomment-1654646620&#34;&gt;https://github.com/harvester/tests/issues/522#issuecomment-1654646620&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;For manual installation, we can also follow the command in the &lt;code&gt;cloud init user data&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;openstack-troubleshooting&#34;&gt;OpenStack troubleshooting&lt;/h3&gt;&#xA;&lt;p&gt;If you failed create volume with the following error message&#xA;&lt;code&gt;Error: Failed to perform requested operation on instance &amp;quot;opensuse&amp;quot;, the instance has an error status: Please try again later [Error: Build of instance 289d8c95-fd99-42a4-8eab-3a522e891463 aborted: Invalid input received: Invalid image identifier or unable to access requested image. (HTTP 400) (Request-ID: req-248baac7-a2de-4c51-9817-de653a548e3b)].&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>VM IP addresses should be labeled per network interface</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2032-2370-vm-ip-lableled-per-network-interface/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2032-2370-vm-ip-lableled-per-network-interface/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2032&#34;&gt;#2032&lt;/a&gt; [BUG] VM IP addresses should be labeled per network interface&lt;/li&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2370&#34;&gt;#2370&lt;/a&gt; [backport v1.0.3] VM IP addresses should be labeled per network interface&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Virtual Machine&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable network with magement-mgmt interface&lt;/li&gt;&#xA;&lt;li&gt;Create vlan network &lt;code&gt;vlan1&lt;/code&gt; with id &lt;code&gt;1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the IP address on the VM page&lt;/li&gt;&#xA;&lt;li&gt;Create a VM with &lt;code&gt;harvester-mgmt&lt;/code&gt; network&lt;/li&gt;&#xA;&lt;li&gt;Import Harvester in Rancher&lt;/li&gt;&#xA;&lt;li&gt;Provision a RKE2 cluster from Rancher&lt;/li&gt;&#xA;&lt;li&gt;Check the IP address on the VM page&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Now the VM list only show IP which related to user access.&lt;/li&gt;&#xA;&lt;li&gt;And provide hover message on each displayed IP address&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/173749441-06fdad41-147a-4703-b19f-eafb1af9f18d.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/173750324-9f26bcd2-024c-428f-a8bd-2a564c6078f2.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>VM label names consistentency before and after the restore</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2662-vm-label-names-consistentency-after-the-restore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2662-vm-label-names-consistentency-after-the-restore/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2662&#34;&gt;#2662&lt;/a&gt; [BUG] VM label names should be consistent before and after the restore task is done&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Network&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a VM named &lt;code&gt;ubuntu&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the label name in virtual machine yaml content, label marked with &lt;code&gt;harvesterhci.io/vmName&lt;/code&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/188374691-b36db1bc-2e2e-447b-96e1-699aa5e0ffee.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Setup the S3 backup target&lt;/li&gt;&#xA;&lt;li&gt;Take a S3 backup with name&lt;/li&gt;&#xA;&lt;li&gt;After the backup task is done, delete the current VM&lt;/li&gt;&#xA;&lt;li&gt;Restore VM from the backup with the same name &lt;code&gt;ubuntu&lt;/code&gt; (Create New)&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/188378123-9af171af-c992-4e78-bdbb-8627903502ff.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the yaml content after VM fully operated&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;p&gt;The vm lable name is consistent to display &lt;code&gt;harvesterhci.io/vmName&lt;/code&gt; after restore from the backup.&lt;/p&gt;</description>
    </item>
    <item>
      <title>VM Snapshot support</title>
      <link>https://harvester.github.io/tests/manual/_incoming/553_vm_snapshot_support/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/553_vm_snapshot_support/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/553&#34;&gt;https://github.com/harvester/harvester/issues/553&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Create an Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;code&gt;vm1&lt;/code&gt; with the image and an additional data volume &lt;code&gt;disk-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Login to &lt;code&gt;vm1&lt;/code&gt;, execute following commands:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;fdisk /dev/vdb&lt;/code&gt; with new and primary partition&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;mkfs.ext4 /dev/vdb1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;mkdir vdb &amp;amp;&amp;amp; mount -t ext4 /dev/vdb1 vdb&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;ping 127.0.0.1 | tee -a test vdb/test&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Virtual Machines&lt;/em&gt; page, click &lt;strong&gt;Take Snapshot&lt;/strong&gt; button on &lt;code&gt;vm1&lt;/code&gt;&amp;rsquo;s details, named &lt;code&gt;vm1s1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Execute &lt;code&gt;sync&lt;/code&gt; on &lt;code&gt;vm1&lt;/code&gt; and &lt;strong&gt;Take Snapshot&lt;/strong&gt; named &lt;code&gt;vm1s2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Interrupt &lt;code&gt;ping...&lt;/code&gt; command and &lt;code&gt;rm test &amp;amp;&amp;amp; sync&lt;/code&gt;, then &lt;strong&gt;Take Snapshot&lt;/strong&gt; named &lt;code&gt;vm1s3&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Restore 3 snapshots into &lt;strong&gt;New&lt;/strong&gt; VM: &lt;code&gt;vm1s1r&lt;/code&gt;, &lt;code&gt;vm1s2r&lt;/code&gt; and &lt;code&gt;vm1s3r&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Content of &lt;code&gt;test&lt;/code&gt; and &lt;code&gt;vdb/test&lt;/code&gt; should be the same  in VM, and different in other restored VMs.&lt;/li&gt;&#xA;&lt;li&gt;Restore snapshots with &lt;strong&gt;Replace Existing&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Content of &lt;code&gt;test&lt;/code&gt; and &lt;code&gt;vdb/test&lt;/code&gt; in restored &lt;code&gt;vm1&lt;/code&gt; from the snapshot, should be the same as the VM restored with the same snapshot.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>VM template is not working with Node scheduling</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2244_vm_template_is_not_working_with_node_scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2244_vm_template_is_not_working_with_node_scheduling/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2244&#34;&gt;https://github.com/harvester/harvester/issues/2244&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/177742575-31730953-5ffd-4018-b5ce-1b1e487ee14c.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Create an Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create VM with &lt;em&gt;&lt;strong&gt;Multiple Instance&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;Use VM Template&lt;/strong&gt;&lt;/em&gt;, In &lt;strong&gt;Node Scheduling&lt;/strong&gt; tab, select &lt;code&gt;Run VM on specific node(s)&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Created VMs should be scheduled on the specific node&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>VMIs created from VM Template don&#39;t have LiveMigrate evictionStrategy set</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2357_vmis_created_from_vm_template_do_nott_have_livemigrate_evictionstrategy_set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2357_vmis_created_from_vm_template_do_nott_have_livemigrate_evictionstrategy_set/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2357&#34;&gt;https://github.com/harvester/harvester/issues/2357&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with at least 2 nodes&lt;/li&gt;&#xA;&lt;li&gt;Create Image for VM Creation&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Advanced/Templates&lt;/em&gt; and create a template &lt;code&gt;t1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create VM &lt;code&gt;vm1&lt;/code&gt; from template &lt;code&gt;t1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Edit YAML of &lt;code&gt;vm1&lt;/code&gt;, field &lt;code&gt;spec.template.spec.evictionStrategy&lt;/code&gt; should be &lt;code&gt;LiveMigrate&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Enable Maintenance Mode on the host which hosting &lt;code&gt;vm1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;vm1&lt;/code&gt; should start migrating automatically&lt;/li&gt;&#xA;&lt;li&gt;Migration should success&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>VMs can&#39;t start if a node contains more than ~60 VMs</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2722_vms_can_not_start_if_a_node_contains_more_than_60_vms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2722_vms_can_not_start_if_a_node_contains_more_than_60_vms/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2722&#34;&gt;https://github.com/harvester/harvester/issues/2722&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/192251104-7a53a1a9-260d-4e90-aade-1b3e7c11cc52.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Login to console, execute &lt;code&gt;sysctl -a | grep aio&lt;/code&gt;, the value of &lt;code&gt;fs.aio-max-nr&lt;/code&gt; should be &lt;code&gt;1048576&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Update the value by executing:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir -p /usr/local/lib/sysctl.d/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &amp;gt; /usr/local/lib/sysctl.d/harvester.conf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt;EOF&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;fs.aio-max-nr = 61440&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sysctl --system&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;&#xA;&lt;li&gt;Execute &lt;code&gt;sysctl -a | grep aio&lt;/code&gt;, the value of &lt;code&gt;fs.aio-max-nr&lt;/code&gt; should be &lt;code&gt;61440&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Reboot the node then execute &lt;code&gt;sysctl -a | grep aio&lt;/code&gt;, the value of &lt;code&gt;fs.aio-max-nr&lt;/code&gt; should still be &lt;code&gt;61440&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create an image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create 60 VMs and schedule on the node which updated &lt;code&gt;fs.aio-max-nr&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Update &lt;code&gt;fs.aio-max-nr&lt;/code&gt; to &lt;code&gt;1048576&lt;/code&gt; in &lt;code&gt;/usr/local/lib/sysctl.d/harvester.conf&lt;/code&gt; and execute &lt;code&gt;sysctl --system&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;VMs should started successfully or Stopping with error message &lt;code&gt;Too many pods&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>VolumeSnapshot Management</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2296_volumesnapshot_management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2296_volumesnapshot_management/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2296&#34;&gt;https://github.com/harvester/harvester/issues/2296&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester with any nodes&lt;/li&gt;&#xA;&lt;li&gt;Create an Image for VM creation&lt;/li&gt;&#xA;&lt;li&gt;Create vm &lt;code&gt;vm1&lt;/code&gt; and start it&lt;/li&gt;&#xA;&lt;li&gt;*&lt;em&gt;Take Snapshot&lt;/em&gt; on &lt;code&gt;vm1&lt;/code&gt; named &lt;code&gt;vm1s1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Volumes&lt;/em&gt;, click disks of &lt;code&gt;vm1&lt;/code&gt; then move to &lt;strong&gt;Snapshots&lt;/strong&gt; tab, volume of snapshot &lt;code&gt;vm1s1&lt;/code&gt; should not displayed&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Advanced/Volume Snapshots&lt;/em&gt;, volumes of snapshot &lt;code&gt;vm1s1&lt;/code&gt; should not displayed&lt;/li&gt;&#xA;&lt;li&gt;Navigate to &lt;em&gt;Advanced/VM Snapshots&lt;/em&gt;, snapshot &lt;code&gt;vm1s1&lt;/code&gt; should displayed&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Wrong mgmt bond MTU size during initial ISO installation</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2437_wrong_mgmt_bond_mtu_size_during_initial_iso_installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/2437_wrong_mgmt_bond_mtu_size_during_initial_iso_installation/</guid>
      <description>&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/harvester/harvester/issues/2437&#34;&gt;https://github.com/harvester/harvester/issues/2437&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/192757588-73484301-07e7-4a37-9d1e-cbcada9b5774.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/5169694/192758868-422887df-557c-4d8c-9ee8-2ab0f863f97a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;verify-steps&#34;&gt;Verify Steps:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Harvester via ISO and configure &lt;strong&gt;IPv4 Method&lt;/strong&gt; with &lt;em&gt;static&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Inputbox &lt;code&gt;MTU (Optional)&lt;/code&gt; should be available and optional&lt;/li&gt;&#xA;&lt;li&gt;Configured MTU should reflect to the port&amp;rsquo;s MTU after installation&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Zero downtime upgrade</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1707-zero-downtime-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://harvester.github.io/tests/manual/_incoming/1707-zero-downtime-upgrade/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Related issues: &lt;a href=&#34;https://github.com/harvester/harvester/issues/1707&#34;&gt;#1707&lt;/a&gt; [BUG] Zero downtime upgrade stuck in &amp;ldquo;Waiting for VM live-migration or shutdown&amp;hellip;&amp;rdquo;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;category&#34;&gt;Category:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Upgrade&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;verification-steps&#34;&gt;Verification Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a ubuntu image from URL&lt;/li&gt;&#xA;&lt;li&gt;Enable Network with management-mgmt&lt;/li&gt;&#xA;&lt;li&gt;Create a virtual network vlan1 with id 1&lt;/li&gt;&#xA;&lt;li&gt;Setup backup target&lt;/li&gt;&#xA;&lt;li&gt;Create a VM backup&lt;/li&gt;&#xA;&lt;li&gt;Follow the &lt;a href=&#34;https://github.com/harvester/docs/blob/main/docs/upgrade/automatic.md&#34;&gt;guide&lt;/a&gt; to do upgrade test&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/166428121-391f5321-ec8e-46ce-9a96-ea92f04b3907.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/166429966-b08cea0e-c457-41b2-a647-b6d3ac00aa58.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;expected-results&#34;&gt;Expected Results&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Can upgrade correctly with all VMs remain in running&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/166430303-376d9e30-bf92-49eb-b3e2-8eeeb2375702.png&#34; alt=&#34;image&#34;&gt;&#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/29251855/166430680-bb9e14fe-7da5-4b73-9ec8-47a780b4914c.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
