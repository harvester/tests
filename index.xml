<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Harvester Test Cases on Harvester manual test cases</title>
    <link>https://harvester.github.io/tests/</link>
    <description>Recent content in Harvester Test Cases on Harvester manual test cases</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://harvester.github.io/tests/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://harvester.github.io/tests/manual/deployment/1218-http-proxy-setting-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/1218-http-proxy-setting-harvester/</guid>
      <description>Related issue: #1218 Missing http proxy settings on rke2 and rancher pod Environment setup Setup an airgapped harvester
Clone ipxe example repository https://github.com/harvester/ipxe-examples Edit the setting.xml file under vagrant ipxe example Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Verification Steps Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,192.168.0.0/16,cattle-system.svc,.svc,.cluster.local,&amp;lt;internal domain&amp;gt; Create image from URL (change folder date to latest) https://cloud-images.ubuntu.com/focal/20211122/focal-server-cloudimg-amd64.img Create a virtual machine Prepare an S3 account with Bucket, Bucket region, Access Key ID and Secret Access Key Setup backup target in settings Edit virtual machine and take backup ssh to server node with user rancher Run kubectl create deployment nginx --image=nginx:latest on Harvester cluster Run kubectl get pods Expected Results At Step 2, Can download and create image from URL without error At step 6, Can backup running VM to external S3 storage correctly At step 6, Can delete backup from external S3 correctly At step 9, Can pull image from internet and deploy nginx pod in running status harvester-node-0:/home/rancher # kubectl create deployment nginx --image=nginx:latest deployment.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1330-rancher-import-harvester-enhacement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1330-rancher-import-harvester-enhacement/</guid>
      <description> Related issues: #1330 Http proxy setting download image Environment setup Install the latest rancher from docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6-head Create an one node harvester cluster Both harvester and rancher have internet connection Verification Steps Access rancher dashboard Open Virtualization Management page Import existing harvester Copy the registration url Create image from URL (change folder date to latest) https://cloud-images.ubuntu.com/focal/20211122/focal-server-cloudimg-amd64.img Access harvester dashboard Edit cluster-registration-url in settings Paste the registration url and save Back to rancher and wait for harvester imported in Rancher Expected Results Harvester can be imported in rancher dashboard with running status Can access harvester in virtual machine page Can create harvester cloud credential Can load harvester cloud credential while creating harvester </description>
    </item>
    
    <item>
      <title></title>
      <link>https://harvester.github.io/tests/manual/live-migration/1401-support-volume-hot-unplug-live-migrate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/1401-support-volume-hot-unplug-live-migrate/</guid>
      <description> Related issues: #1401 Http proxy setting download image Environment setup Setup an airgapped harvester
Create an 3 nodes harvester cluster with large size disks Verification Steps Scenario2: Live migrate VM not have hot-plugged volume before, do hot-plugged the unplugged. Create a virtual machine Create several volumes (without image) Add volume, hot-plug volume to virtual machine Open virtual machine, find hot-plugged volume Click Detach volume Add volume again Migrate VM from one node to another Detach volume Add unplugged volume again Expected Results Can hot-plug volume without error Can hot-unplug the pluggable volumes without restarting VM The de-attached volume can also be hot-plug and mount back to VM </description>
    </item>
    
    <item>
      <title></title>
      <link>https://harvester.github.io/tests/manual/volumes/1401-support-volume-hot-unplug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/1401-support-volume-hot-unplug/</guid>
      <description> Related issues: #1401 Http proxy setting download image Environment setup Setup an airgapped harvester
Create an 3 nodes harvester cluster with large size disks Scenario1: Live migrate VM already have hot-plugged volume to new node, then detach (hot-unplug) it Verification Steps Create a virtual machine Create several volumes (without image) Add volume, hot-plug volume to virtual machine Open virtual machine, find hot-plugged volume Click de-attach volume Add volume again Expected Results Can hot-plug volume without error Can hot-unplug the pluggable volumes without restarting VM The de-attached volume can also be hot-plug and mount back to VM </description>
    </item>
    
    <item>
      <title>01-Import existing Harvester clusters in Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/01-import-existing-harvester-in-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/01-import-existing-harvester-in-rancher/</guid>
      <description>This feature have been deprecated which already enhanced and merge to setup from harvester settings Please refer to 02-Integrate to Rancher from Harvester settings to test this feature
Login rancher dashboard Navigate to Virtual Management Page Click import existing Copy the curl command SSH to harvester master node (user: rancher) Execute the curl command to import harvester to rancher curl --insecure -sfL https://192.168.50.82/v3/import/{identifier}.yaml | kubectl apply -f - Run sudo chmod 775 /etc/rancher/rke2/rke2.</description>
    </item>
    
    <item>
      <title>02-Integrate to Rancher from Harvester settings (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/02-integrate-rancher-from-harvester-settings/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/02-integrate-rancher-from-harvester-settings/</guid>
      <description>Environment setup Install the latest rancher from docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6-head Create an one node harvester cluster Both harvester and rancher have internet connection Verification Steps Access rancher dashboard Open Virtualization Management page Import existing harvester Copy the registration url Create image from URL (change folder date to latest) https://cloud-images.ubuntu.com/focal/20211122/focal-server-cloudimg-amd64.img Access harvester dashboard Edit cluster-registration-url in settings Paste the registration url and save Back to rancher and wait for harvester imported in Rancher Expected Results Harvester can be imported in rancher dashboard with running status Can access harvester in virtual machine page Can create harvester cloud credential Can load harvester cloud credential while creating harvester </description>
    </item>
    
    <item>
      <title>03-Manage VM in Downstream Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/03-manage-vm-downstream-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/03-manage-vm-downstream-harvester/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard
Open harvester from Virtualization Management page Open Virtual Machine page Create a single instance virtual machine in Virtual Machines page Create multiple 3 instances virtual machines in Virtual Machines page Access and check virtual machine details Edit cpu, memory and network of one virtual machine Try Stop, Restart and Migrate virtual machine Try Clone virtual machine Try Delete virtual machine Expected Results Can create a single instance vm correctly Can create multiple instances vm correctly Can diaply all virtual machine information Can change cpu, memory and network and retart vm correctly Can Stop, Restart and Migrate virtual machine correctly Can Clone virtual machine correctly Can Delete virtual machine correctly </description>
    </item>
    
    <item>
      <title>04-Manage Node in Downstream Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/04-manage-host-downstream-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/04-manage-host-downstream-harvester/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard
Open harvester from Virtualization Management page Open Host page Access and check node details Edit node config, change network and add disk Try to Cordon and decordon node Enable and disable Maintenance mode Expected Results Can diaply all node&amp;rsquo;s information Can add disk to node correctly Can change network of node correctly Can Cordon and decordon node correctly Can enable and disable Maintenance mode </description>
    </item>
    
    <item>
      <title>05-Manage Image in Downstream Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/05-manage-image-volume-downstream-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/05-manage-image-volume-downstream-harvester/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard
Open harvester from Virtualization Management page Open Images page Create an image from URL Create an image from file Delete created images Expected Results Can create an image from URL Can create an image from file Can create an image from file Can delete created images correctly </description>
    </item>
    
    <item>
      <title>06-Manage Network in Downstream Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/06-manage-network-in-downstream-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/06-manage-network-in-downstream-harvester/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard
Open harvester from Virtualization Management page Open Network page Create an new virtual network Create a new virtual machine using the new virtual network Delete a virtual network Expected Results Can create an new virtual network Create create a new virtual machine using the new virtual network Virtual machine can retrieve ip address Can delete a virtual network </description>
    </item>
    
    <item>
      <title>07-Add and grant project-owner user to harvester (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/07-rbac-add-grant-project-owner-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/07-rbac-add-grant-project-owner-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-owner and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project Search project-owner user Assign Owner role to it Logout current user from Rancher Login with project-owner Open harvester from Virtualization Management page Expected Results Can create project-owner and set password Can assign Owner role to project-owner in default Can login correctly with project-owner Can manage all default project resources including host, virtual machines, volumes, VM and network </description>
    </item>
    
    <item>
      <title>08-Add and grant project-readonly user to harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/08-rbac-add-grant-project-readonly-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/08-rbac-add-grant-project-readonly-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-readonly and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project Search project-readonly user Assign Read Only role to it Logout current user from Rancher Login with project-readonly Open harvester from Virtualization Management page Expected Results Can create project-readonly and set password Can assign Read Only role to project-readonly in default Can login correctly with project-readonly Can&amp;rsquo;t see Host page in harvester Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip; </description>
    </item>
    
    <item>
      <title>09-Add and grant project-member user to harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/09-rbac-add-grant-project-member-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/09-rbac-add-grant-project-member-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-member and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project Search project-member user Assign Member role to it Logout current user from Rancher Login with project-member Open harvester from Virtualization Management page Expected Results Can create project-member and set password Can assign Member role to project-member in default Can login correctly with project-member Can&amp;rsquo;t see Host page in harvester Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip; </description>
    </item>
    
    <item>
      <title>10-Add and grant project-custom user to harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/10--rbacadd-grant-project-custom-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/10--rbacadd-grant-project-custom-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-custom and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project Search project-custom user Assign Custom role to it Set Create Namespace, Manage Volumes and View Volumes Logout current user from Rancher Login with project-custom Open harvester from Virtualization Management page Expected Results Can create project-custom and set password Can assign Custom role to project-custom in default Can login correctly with project-custom Can do Create Namespace, Manage Volumes and View Volumes in default project </description>
    </item>
    
    <item>
      <title>11-Create New Project in Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/11-create-project-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/11-create-project-harvester/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Click Create Project Set CPU and Memory limit in Resource Quotas Change view to testProject only Create some images Create some volumes Create a virtual machine Expected Results Can creat project correctly in Projects/Namespaces page Can create images correctly Can create volumes correctly Can create virtual machine correctly </description>
    </item>
    
    <item>
      <title>12-Create New Namespace in Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/12-create-namespace-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/12-create-namespace-harvester/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Select the new project created in previous test case Click Create Namespace Set CPU and Memory limit in Container Resource Limit Expected Results Can creat new namepasce in correctly in create new project </description>
    </item>
    
    <item>
      <title>13-Add and grant project-owner user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/13-rbac-add-grant-project-owner-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/13-rbac-add-grant-project-owner-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-owner user Assign Owner role to it Logout current user from Rancher Login with project-owner Open harvester from Virtualization Management page Change view to testProject only Expected Results Can assign Owner role to project-owner in testProject project Can manage all testProject project resources including host, virtual machines, volumes, VM and network </description>
    </item>
    
    <item>
      <title>14-Add and grant project-readonly user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/14-rbac-add-grant-project-readonly-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/14-rbac-add-grant-project-readonly-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-readonly user Assign Read Only role to it Logout current user from Rancher Login with project-readonly Open harvester from Virtualization Management page Change view to testProject only Expected Results Can assign Read Only role to in testProject project Can login correctly with project-readonly Can&amp;rsquo;t see Host page in testProject only view Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip; in testProject only view </description>
    </item>
    
    <item>
      <title>15-Add and grant project-member user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/15-rbac-add-grant-project-member-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/15-rbac-add-grant-project-member-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-member user Assign Member role to it Logout current user from Rancher Login with project-member Open harvester from Virtualization Management page Change view to testProject only Expected Results Can assign Member role to project-member in testProject project Can login correctly with project-member Can&amp;rsquo;t see Host page in testProject project Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip; in testProject project </description>
    </item>
    
    <item>
      <title>16-Add and grant project-custom user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/16-rbac-add-grant-project-custom-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/16-rbac-add-grant-project-custom-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-custom user Assign Custom role to it Set Create Namespace, Manage Volumes and View Volumes Logout current user from Rancher Login with project-custom Open harvester from Virtualization Management page Change view to testProject only Expected Results Can assign Custom role to project-custom in testProject project Can login correctly with project-custom Can do Create Namespace, Manage Volumes and View Volumes in testProject project </description>
    </item>
    
    <item>
      <title>17-Delete Imported Harvester Cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/17-delete-imported-harvester-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/17-delete-imported-harvester-cluster/</guid>
      <description> Finish 01-Import existing Harvester clusters in Rancher Open Virtualization Management page Delete already imported harvester Expected Results Can delete imported harvester correctly </description>
    </item>
    
    <item>
      <title>18-Delete Failed Imported Harvester Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/18-delete-failed-imported-harvester-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/18-delete-failed-imported-harvester-cluster/</guid>
      <description> Make failure in 01-Import existing Harvester clusters in Rancher Open Virtualization Management page Delete already imported harvester Expected Results Can delete imported harvester correctly </description>
    </item>
    
    <item>
      <title>19-Enable Harvester Node Driver</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/19-enable-harvester-node-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/19-enable-harvester-node-driver/</guid>
      <description>From Rancher v2.6.3-rc1, harvester node driver has already builtin racher We don&amp;rsquo;t need manual activate it
Open Cluster Management Click Drivers page and navigate to Node Drivers tab Search harvester Check Harvester node driver is activated and mark as builtin Expected Results Status displayed Activated </description>
    </item>
    
    <item>
      <title>20-Create RKE1 Kubernetes Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/20-create-rke1-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/20-create-rke1-kubernetes-cluster/</guid>
      <description>Click Cluster Management Click Cloud Credentials Click createa and select Harvester Input credential name Select existing cluster in the Imported Cluster list Click Create Expand RKE1 Configuration Add Template in Node template Select Harvester Select created cloud credential created Select default namespace Select ubuntu image Select network: vlan1 Provide SSH User: ubuntu Provide template name, click create Open Cluster page, click Create
Toggle RKE1
Provide cluster name
Provide Name Prefix</description>
    </item>
    
    <item>
      <title>21-Delete RKE1 Kubernetes Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/21-delete-rke1-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/21-delete-rke1-kubernetes-cluster/</guid>
      <description> Open Cluster Management Check provisioned RKE1 cluster Click Delete from menu Expected Results Can remove RKE1 Cluster and disapper on Cluster page RKE1 Cluster will be removed from rancher menu under explore cluster RKE1 virtual machine should be also be removed from Harvester </description>
    </item>
    
    <item>
      <title>22-Create RKE2 Kubernetes Cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/22-create-rke2-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/22-create-rke2-kubernetes-cluster/</guid>
      <description> Click Cluster Management Click Cloud Credentials Click create and select Harvester Input credential name Select existing cluster in the Imported Cluster list Click Create Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Click Create Wait for RKE2 cluster provisioning complete (~20min) Expected Results Provision RKE2 cluster successfully with Running status Can acccess RKE2 cluster to check all resources and services </description>
    </item>
    
    <item>
      <title>23-Delete RKE2 Kubernetes Cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/23-delete-rke2-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/23-delete-rke2-kubernetes-cluster/</guid>
      <description> Open Cluster Management Check provisioned RKE2 cluster Click Delete from menu Expected Results Can remove RKE2 Cluster and disapper on Cluster page RKE2 Cluster will be removed from rancher menu under explore cluster RKE2 virtual machine should be also be removed from Harvester </description>
    </item>
    
    <item>
      <title>24-Delete RKE1 Kubernetes Cluster in Provisioning</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/24-delete-rke1-kubernetes-cluster-provisioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/24-delete-rke1-kubernetes-cluster-provisioning/</guid>
      <description> Provision RKE1 Cluster Management When RKE1 cluster show Provisioning Click Delete from menu Expected Results Can remove RKE1 Cluster and disapper on Cluster page RKE1 Cluster will be removed from rancher menu under explore cluster RKE1 virtual machine should be also be removed from Harvester </description>
    </item>
    
    <item>
      <title>25-Delete RKE1 Kubernetes Cluster in Failure</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/25-delete-rke1-kubernetes-cluster-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/25-delete-rke1-kubernetes-cluster-failure/</guid>
      <description> Provision RKE1 Cluster Management When RKE1 cluster displayed in Failure Click Delete from menu Expected Results Can remove RKE1 Cluster and disapper on Cluster page RKE1 Cluster will be removed from rancher menu under explore cluster RKE1 virtual machine should be also be removed from Harvester </description>
    </item>
    
    <item>
      <title>26-Delete RKE2 Kubernetes Cluster in Provisioning</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/26-delete-rke2-kubernetes-cluster-provisioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/26-delete-rke2-kubernetes-cluster-provisioning/</guid>
      <description> Provision RKE2 Cluster Management When RKE2 cluster show Provisioning Click Delete from menu Expected Results Can remove RKE2 Cluster and disapper on Cluster page RKE2 Cluster will be removed from rancher menu under explore cluster RKE2 virtual machine should be also be removed from Harvester </description>
    </item>
    
    <item>
      <title>27-Delete RKE2 Kubernetes Cluster in Failure</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/27-delete-rke2-kubernetes-cluster-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/27-delete-rke2-kubernetes-cluster-failure/</guid>
      <description> Provision RKE2 Cluster Management When RKE2 cluster displayed in Failure Click Delete from menu Expected Results Can remove RKE2 Cluster and disapper on Cluster page RKE2 Cluster will be removed from rancher menu under explore cluster RKE2 virtual machine should be also be removed from Harvester </description>
    </item>
    
    <item>
      <title>28-Deploy Harvester cloud provider to RKE1 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/28-deploy-harvester-cloud-provider-to-rke1-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/28-deploy-harvester-cloud-provider-to-rke1-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher Environment Setup Docker install rancher v2.6.3 Create one node harvester with enough resource Verify steps Environment preparation as above steps Import harvester to rancher from harvester settings Create cloud credential Create RKE1 node template Provision a RKE1 cluster, check the Harvester as cloud provider Access RKE1 cluster Open charts in Apps &amp;amp; Market page Install harvester cloud provider Make sure cloud provider installed complete NAME: harvester-cloud-provider LAST DEPLOYED: Thu Dec 16 03:57:26 2021 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None --------------------------------------------------------------------- SUCCESS: helm install --namespace=kube-system --timeout=10m0s --values=/home/shell/helm/values-harvester-cloud-provider-100.</description>
    </item>
    
    <item>
      <title>29-Deploy Harvester cloud provider to RKE2 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/29-deploy-harvester-cloud-provider-to-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/29-deploy-harvester-cloud-provider-to-rke2-cluster/</guid>
      <description> Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Check alread set Harvester as cloud provider Click Create Wait for RKE2 cluster provisioning complete (~20min) Expected Results Provision RKE2 cluster successfully with Running status Can acccess RKE2 cluster to check all resources and services Check cloud provider installed and configured on RKE2 cluster </description>
    </item>
    
    <item>
      <title>30-Configure Harvester LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/30-configure-harvester-loadbalancer-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/30-configure-harvester-loadbalancer-service/</guid>
      <description>Prerequisite: Already provision RKE1/RKE2 cluster in previous test case
Open Global Settings in hamburger menu Replace ui-dashboard-index to https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Change ui-offline-preferred to Remote Refresh the current page (ctrl + r) Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Click Create Select Load Balancer Given service name to make the load balancer name composed of the cluster name, namespace, svc name, and suffix(8 characters) more than 63 characters Provide Listening port and Target port Click Add-on Config Select Health Check port Select dhcp as IPAM mode Provide Health Check Threshold Provide Health Check Failure Threshold Provide Health Check Period Provide Health Check Timeout Click Create button Create another load balancer service with the name characters.</description>
    </item>
    
    <item>
      <title>31-Specify &#34;pool&#34; IPAM mode in LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/31-specify-pool-ipam-mode-loadbalancer-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/31-specify-pool-ipam-mode-loadbalancer-service/</guid>
      <description>Prerequisite: Already provision RKE1/RKE2 cluster in previous test case
Open Global Settings in hamburger menu Replace ui-dashboard-index to https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Change ui-offline-preferred to Remote Refresh the current page (ctrl + r) Access Harvester dashboard UI Go to Settings Create a vip-pool in Harvester settings. Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Click Create Select Load Balancer Given service name Provide Listending port and Target port Click Add-on Config Provide Health Check port Select pool as IPAM mode Provide Health Check Threshold Provide Health Check Failure Threshold Provide Health Check Period Provide Health Check Timeout Click Create button Expected Results Can create load balance service correctly Can operate and route to deployed service correctly </description>
    </item>
    
    <item>
      <title>32-Deploy Harvester CSI provider to RKE 1 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/32-deploy-harvester-csi-provider-to-rke1-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/32-deploy-harvester-csi-provider-to-rke1-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher Environment Setup Docker install rancher v2.6.3 Create one node harvester with enough resource Verify steps Environment preparation as above steps Import harvester to rancher from harvester settings Create cloud credential Create RKE1 node template Provision a RKE1 cluster, check the Harvester as cloud provider Access RKE1 cluster Open charts in Apps &amp;amp; Market page Install Harvester CSI driver Make sure CSI driver installed complete NAME: harvester-csi-driver LAST DEPLOYED: Thu Dec 16 03:59:54 2021 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Successfully deployed Harvester CSI driver to the kube-system namespace.</description>
    </item>
    
    <item>
      <title>33-Deploy Harvester CSI provider to RKE 2 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/33-deploy-harvester-csi-provider-to-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/33-deploy-harvester-csi-provider-to-rke2-cluster/</guid>
      <description> Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Check alread set Harvester as cloud provider Click Create Wait for RKE2 cluster provisioning complete (~20min) Expected Results Provision RKE2 cluster successfully with Running status Can acccess RKE2 cluster to check all resources and services Check CSI driver installed and configured on RKE2 cluster </description>
    </item>
    
    <item>
      <title>34-Hot plug and unplug volumes in RKE1 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/34-hotplug-unplug-volumes-in-rke1-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/34-hotplug-unplug-volumes-in-rke1-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher Environment Setup Docker install rancher v2.6.3 Create one node harvester with enough resource Verify Steps Environment preparation as above steps
Import harvester to rancher from harvester settings
Create cloud credential
Create RKE1 node template Provision a RKE1 cluster, check the Harvester as cloud provider Access RKE1 cluster
Open charts in Apps &amp;amp; Market page
Install harvester cloud provider and CSI driver</description>
    </item>
    
    <item>
      <title>35-Hot plug and unplug volumes in RKE2 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/35-hotplug-unplug-volumes-in-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/35-hotplug-unplug-volumes-in-rke2-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher Environment Setup Docker install rancher v2.6.3 Create one node harvester with enough resource Verify Steps Environment preparation as above steps
Import harvester to rancher from harvester settings
Create cloud credential
Create RKE2 cluster as test case #34
Access RKE2 cluster
Open charts in Apps &amp;amp; Market page
Install harvester cloud provider and CSI driver
Make sure cloud provider installed complete</description>
    </item>
    
    <item>
      <title>36-Remove Harvester LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/36-remove-harvester-loadbalancer-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/36-remove-harvester-loadbalancer-service/</guid>
      <description> Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Delete previous created load balancer service Expected Results Can remove load balance service correctly Service will be removed from assigned Apps </description>
    </item>
    
    <item>
      <title>37-Import Online Harvester From the Airgapped Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher-copy/</guid>
      <description>Environment Setup Setup the online harvester
Use ipxe vagrant example to setup a 3 nodes cluster https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create ubuntu cloud image from URL Create virtual machine with name vlan1 and id: 1 Create virtual machine and assign vlan network, confirm can get ip address Setup squid HTTP proxy server
Move to vagrant pxe harvester folder Execute vagrant ssh pxe_server Run apt-get install squid Edit /etc/squid/squid.</description>
    </item>
    
    <item>
      <title>37-Import Online Harvester From the Airgapped Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher/</guid>
      <description>Environment Setup Setup the online harvester
Use ipxe vagrant example to setup a 3 nodes cluster https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create ubuntu cloud image from URL Create virtual machine with name vlan1 and id: 1 Create virtual machine and assign vlan network, confirm can get ip address Setup squid HTTP proxy server
Move to vagrant pxe harvester folder Execute vagrant ssh pxe_server Run apt-get install squid Edit /etc/squid/squid.</description>
    </item>
    
    <item>
      <title>38-Import Airgapped Harvester From the Airgapped Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/38-import-airgapped-harvester-from-airgapped-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/38-import-airgapped-harvester-from-airgapped-rancher/</guid>
      <description>Related task: #1052 Test Air gap with Rancher integration Environment Setup Setup the airgapped harvester
Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create virtual machine with name vlan1 and id: 1 Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.</description>
    </item>
    
    <item>
      <title>39-Standard user no Harvester Access</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/39-rbac-standard-user-no-access/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/39-rbac-standard-user-no-access/</guid>
      <description> As admin import/register a harvester cluster in Rancher As admin, Enable Harvester node driver As a standard user User1, login to rancher Verify User1 has no access to harvester cluster in Virtualization management page Create harvester cloud credential as User1 Verify User1 can use this cloud credential to create a node template and a node driver cluster 3 and can CRUD each resource </description>
    </item>
    
    <item>
      <title>40-RBAC Add restricted admin User Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/40-rbac-add-restricted-admin-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/40-rbac-add-restricted-admin-user-harvester/</guid>
      <description> As admin import/register a harvester cluster in Rancher create restricted admin user rstradm verify rstradm has access to to Virturalization management page and the harvester cluster is listed Verify rstradm has access to Harvester UI through rancher by selecting it from the list in step 3 and can CRUD each resource </description>
    </item>
    
    <item>
      <title>41-Import Harvester into nested Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/41-rancher-nested-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/41-rancher-nested-harvester/</guid>
      <description>Prerequisite: External network on VLAN
Install Rancher in a VM using Docker method on Harvester cluster using the external VLAN Login rancher dashboard Navigate to Virtual Management Page Click import existing Copy the curl command SSH to harvester master node (user: rancher) Execute the curl command to import harvester to rancher curl --insecure -sfL https://192.168.50.82/v3/import/{identifier}.yaml | kubectl apply -f - Run sudo chmod 775 /etc/rancher/rke2/rke2.yaml to solve the permission denied error Run curl command again, you should see the following successful import message namespace/cattle-system configured serviceaccount/cattle created clusterrolebinding.</description>
    </item>
    
    <item>
      <title>42-Add cloud credential KUBECONFIG</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/42-add-cloud-credential-kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/42-add-cloud-credential-kubeconfig/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester
Click Cluster Management Click Cloud Credentials Click createa and select Harvester Input credential name Select external cluster Input KUBECONFIG from Harvester Click Create </description>
    </item>
    
    <item>
      <title>43-Scale up node driver RKE1</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/43-node-driver-scale-up-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/43-node-driver-scale-up-rke1/</guid>
      <description>Prerequisite: RKE1 cluster in Harvester with at least 2 worker nodes
provision a multinode cluster using harvester node driver with at least 2 worker nodes scale up a node in the cluster </description>
    </item>
    
    <item>
      <title>44-Scale up node driver RKE2</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/44-node-driver-scale-up-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/44-node-driver-scale-up-rke2/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester
provision a multinode cluster using harvester node driver with at least 2 worker nodes scale up a node in the cluster </description>
    </item>
    
    <item>
      <title>45-Scale down node driver RKE1</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/45-node-driver-scale-down-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/45-node-driver-scale-down-rke1/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester
provision a multinode cluster using harvester node driver with at least 2 worker nodes scale down a node in the cluster </description>
    </item>
    
    <item>
      <title>46-Scale down node driver RKE2</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/46-node-driver-scale-down-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/46-node-driver-scale-down-rke2/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester
provision a multinode cluster using harvester node driver with at least 2 worker nodes scale down a node in the cluster </description>
    </item>
    
    <item>
      <title>47-Verify Backup and restore on same server</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/47-verify-backup-restore-same-server/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/47-verify-backup-restore-same-server/</guid>
      <description></description>
    </item>
    
    <item>
      <title>48-Verify Backup and restore on server migration</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/48-verify-backup-restore-server-migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/48-verify-backup-restore-server-migration/</guid>
      <description> Create a harvester cluster Deploy a harvester node driver cluster preupgrade checks on both Take a backup Restore on a new rancher server from backup taken ins step 4 Run post-upgrade checks for both clusters Verify virtualization management â†’ harvester is accessible Run p0 use case </description>
    </item>
    
    <item>
      <title>49-Overprovision Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/49-overprovision-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/49-overprovision-harvester/</guid>
      <description> import harvester into rancher over-provision the connected harvester cluster (i.e. deploy large number of nodes) note: the number will depend on the resources available in the harvester cluster you&amp;rsquo;ve imported. i.e. a harvester setup with 24 cores, 64 GB of ram, you could try provisioning a 3cp, 2cp, 2w cluster of size 4 vCPU 8GB ram to over-provision CPU i.e. a harvester setup with 24 cores, 64 GB of ram, you could try provisioning a 3cp, 2cp, 2w cluster of size 2 vCPU 10GB ram to over-provision CPU </description>
    </item>
    
    <item>
      <title>50-Use fleet when a harvester cluster is imported to rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/50-fleet-with-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/50-fleet-with-harvester/</guid>
      <description> deploy rancher with harvester enabled docker: &amp;ndash;features=harvester=enabled helm: &amp;ndash;set &amp;rsquo;extraEnv[0].name=CATTLE_FEATURES&amp;rsquo; &amp;ndash;set &amp;rsquo;extraEnv[0].value=harvester=enabled import a harvester setup go to fleet â†’ repos -&amp;gt; create validate that that the harvester cluster is NOT in the dropdown for cluster deployments validate that selecting the &amp;lsquo;all clusters&amp;rsquo; option for deployment does NOT deploy to the harvester cluster </description>
    </item>
    
    <item>
      <title>51-Use harvester cloud provider to provision an LB - rke1</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/51-harvester-cloud-provider-loadbalancer-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/51-harvester-cloud-provider-loadbalancer-rke1/</guid>
      <description> Provision cluster using rke1 with harvester as the node driver enable the cloud driver for harvester while provisioning the cluster run jenkins v3 validation checks once cluster comes to active </description>
    </item>
    
    <item>
      <title>52-Use harvester cloud provider to provision an LB - rke2</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/52-harvester-cloud-provider-loadbalancer-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/52-harvester-cloud-provider-loadbalancer-rke2/</guid>
      <description> Provision cluster using rke1 with harvester as the node driver enable the cloud driver for harvester while provisioning the cluster run jenkins v3 validation checks once cluster comes to active </description>
    </item>
    
    <item>
      <title>53-Disable Harvester flag with Harvester cluster added</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/53-disable-harvester-flag/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/53-disable-harvester-flag/</guid>
      <description>Pre-requisites: Rancher with Harvester imported
Disable Harvester feature flag on Rancher Expected Results Harvester should show up in cluster management Virtualization management tab should be hidden. </description>
    </item>
    
    <item>
      <title>54-Import Airgapped Harvester From the Online Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/54-import-airgapped-harvester-from-online-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/54-import-airgapped-harvester-from-online-rancher/</guid>
      <description>Environment Setup Setup the airgapped harvester
Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,192.168.0.0/16,cattle-system.svc,.svc,.cluster.local,&amp;lt;internal domain&amp;gt; Create ubuntu cloud image from URL Create virtual machine with name vlan1 and id: 1 Create virtual machine and assign vlan network, confirm can get ip address Setup squid HTTP proxy server</description>
    </item>
    
    <item>
      <title>55-Import Harvester to Rancher in airgapped different subnet</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/55-import-harvester-rancher-airgapped-different-subnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/55-import-harvester-rancher-airgapped-different-subnet/</guid>
      <description>Environment Setup Note: Harvester and Rancher are under different subnet, can access to each other
Setup the airgapped harvester
Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Enable vlan on harvester-mgmt Create virtual machine with name vlan1 and id: 1 Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,192.168.0.0/16,cattle-system.svc,.svc,.cluster.local,&amp;lt;internal domain&amp;gt; Create ubuntu cloud image from URL Create virtual machine and assign vlan network, confirm can get ip address Setup squid HTTP proxy server</description>
    </item>
    
    <item>
      <title>56-Import Harvester to Rancher in airgapped different subnet</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/56-import-harvester-rancher-online-different-subnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/56-import-harvester-rancher-online-different-subnet/</guid>
      <description>Environment Setup Note: Harvester and Rancher are under different subnet, can access to each other
Setup the online harvester
Iso or vagrant ipxe install harvester on network with internet connection Enable vlan on harvester-mgmt Create virtual machine with name vlan1 and id: 1 Create ubuntu cloud image from URL Create virtual machine and assign vlan network, confirm can get ip address Setup the online rancher
Install rancher on network with internet connection throug docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.</description>
    </item>
    
    <item>
      <title>57-Import airgapped harvester from airgapped rancher with Proxy</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/57-import-airgapped-harvester-from-airgapped-rancher-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/57-import-airgapped-harvester-from-airgapped-rancher-proxy/</guid>
      <description>Related task: #1052 Test Air gap with Rancher integration Environment Setup Setup the airgapped harvester
Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create virtual machine with name vlan1 and id: 1 Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.</description>
    </item>
    
    <item>
      <title>58-Negative-Fully power cycle harvester node machine should recover RKE2 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/58-negative-fully-power-cycle-harvester-node-machine-should-recover-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/58-negative-fully-power-cycle-harvester-node-machine-should-recover-rke2-cluster/</guid>
      <description>Related issue: #1561 Fully shutdown then power on harvester node machine can&amp;rsquo;t get provisioned RKE2 cluster back to work
Related issue: #1428 rke2-coredns-rke2-coredns-autoscaler timeout
Environment Setup The network environment must have vlan network configured and also have DHCP server prepared on your testing vlan Verification Step Prepare a 3 nodes harvester cluster (provo bare machine) Enable virtual network with harvester-mgmt Create vlan1 with id 1 Import harvester from rancher and create cloud credential Provision a RKE2 cluster with vlan 1 Wait for build up ready Shutdown harvester node 3 Shutdown harvester node 2 Shutdown harvester node 1 Wait for 20 minutes Power on node 1, wait 10 seconds Power on node 2, wait 10 seconds Power on node 3 Wait for harvester startup complete Wait for RKE2 cluster back to work Check node and VIP accessibility Check the rke2-coredns pod status kubectl get pods --all-namespaces | grep rke2-coredns Expected Results RKE2 cluster on harvester can recover to Active status</description>
    </item>
    
    <item>
      <title>59-Create K3s Kubernetes Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/59-create-k3s-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/59-create-k3s-kubernetes-cluster/</guid>
      <description>Click Cluster Management Click Cloud Credentials Click create and select Harvester Input credential name Select existing cluster in the Imported Cluster list Click Create Click Clusters
Click Create
Toggle RKE2/K3s
Select Harvester
Input Cluster Name
Select default namespace
Select ubuntu image
Select network vlan1
Input SSH User: ubuntu Click Show Advanced
Add the following user data:
password: 123456 chpasswd: { expire: false } ssh_pwauth: true Click the drop down Kubernetes version list</description>
    </item>
    
    <item>
      <title>60-Delete K3s Kubernetes Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/60-delete-k3s-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/60-delete-k3s-kubernetes-cluster/</guid>
      <description> Open Cluster Management Check provisioned K3s cluster Click Delete from menu Expected Results Can remove K3s Cluster and disapper on Cluster page K3s Cluster will be removed from rancher menu under explore cluster K3s virtual machine should be also be removed from Harvester </description>
    </item>
    
    <item>
      <title>61-Deploy Harvester cloud provider to k3s Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/61-deploy-harvester-cloud-provider-to-k3s-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/61-deploy-harvester-cloud-provider-to-k3s-cluster/</guid>
      <description>Related task: #1812 K3s cloud provider and csi driver support Environment Setup Docker install rancher v2.6.4 Create one node harvester with enough resource Verify steps Follow step 1~13 in tets plan 59-Create K3s Kubernetes Cluster
Click the Edit yaml button Set disable-cloud-provider: true to disable default k3s cloud provider. Add cloud-provider=external to use harvester cloud provider. Create K3s cluster Download the Generate addon configuration for cloud provider Download Harvester kubeconfig and add into your local ~/.</description>
    </item>
    
    <item>
      <title>62-Configure the K3s &#34;DHCP&#34; LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/62-configure-k3s-dhcp-loadbalancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/62-configure-k3s-dhcp-loadbalancer/</guid>
      <description>Prerequisite: Already provision K3s cluster and cloud provider on test plan
59-Create K3s Kubernetes Cluster 61-Deploy Harvester cloud provider to k3s Cluster Create Nginx workload for testing Create a test-nginx deployment with image nginx:latest. Add pod label test: test. Create a DHCP LoadBalancer Open Kubectl shell. Create test-dhcp-lb.yaml file. apiVersion: v1 kind: Service metadata: annotations: cloudprovider.harvesterhci.io/ipam: dhcp name: test-dhcp-lb namespace: default spec: ports: - name: http nodePort: 30172 port: 8080 protocol: TCP targetPort: 80 selector: test: test sessionAffinity: None type: LoadBalancer Run k apply -f test-dhcp-lb.</description>
    </item>
    
    <item>
      <title>62-Configure the K3s &#34;DHCP&#34; LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/64-configure-k3s-dhcp-lb-healcheck/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/64-configure-k3s-dhcp-lb-healcheck/</guid>
      <description>Prerequisite: Already provision K3s cluster and cloud provider on test plan
59-Create K3s Kubernetes Cluster 61-Deploy Harvester cloud provider to k3s Cluster 62-Configure the K3s &amp;ldquo;DHCP&amp;rdquo; LoadBalancer service A Working DHCP load balancer service created on K3s cluster Edit Load balancer config Check the &amp;ldquo;Add-on Config&amp;rdquo; tabs Configure port, IPAM and health check related setting on Add-on Config page Expected Results Can create load balance service correctly Can route workload to nginx deployment </description>
    </item>
    
    <item>
      <title>63-Configure the K3s &#34;Pool&#34; LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/63-configure-k3s-pool-loadbalancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/63-configure-k3s-pool-loadbalancer/</guid>
      <description>Prerequisite: Already provision K3s cluster and cloud provider on test plan
59-Create K3s Kubernetes Cluster 61-Deploy Harvester cloud provider to k3s Cluster Create Nginx workload for testing Create a test-nginx deployment with image nginx:latest. Add pod label test: test. Create a Pool LoadBalancer Modify vip-pool in Harvester settings. Open Kubectl shell.
Create test-pool-lb.yaml file.
apiVersion: v1 kind: Service metadata: annotations: cloudprovider.harvesterhci.io/ipam: pool name: test-pool-lb namespace: default spec: ports: - name: http nodePort: 32155 port: 8080 protocol: TCP targetPort: 80 selector: test: test sessionAffinity: None type: LoadBalancer Run k apply -f test-pool-lb.</description>
    </item>
    
    <item>
      <title>65-Configure the K3s &#34;Pool&#34; LoadBalancer health check</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/65-configure-k3s-pool-lb-healthcheck/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/65-configure-k3s-pool-lb-healthcheck/</guid>
      <description>Prerequisite: Already provision K3s cluster and cloud provider on test plan
59-Create K3s Kubernetes Cluster 61-Deploy Harvester cloud provider to k3s Cluster 63-Configure the K3s &amp;ldquo;Pool&amp;rdquo; LoadBalancer service A Working DHCP load balancer service created on K3s cluster Edit Load balancer config Check the &amp;ldquo;Add-on Config&amp;rdquo; tabs Configure port, IPAM and health check related setting on Add-on Config page Expected Results Can create load balance service correctly Can route workload to nginx deployment </description>
    </item>
    
    <item>
      <title>66-Deploy Harvester csi driver to k3s Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/66-deploy-harvester-csi-driver-to-k3s-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/66-deploy-harvester-csi-driver-to-k3s-cluster/</guid>
      <description>Related task: #1812 K3s cloud provider and csi driver support Environment Setup Docker install rancher v2.6.4 Create one node harvester with enough resource Verify steps Follow step 1~13 in tets plan 59-Create K3s Kubernetes Cluster
Download the Generate addon configuration for csi driver
ssh to harvester node 1
Execute cat /etc/rancher/rke2/rke2.yaml
change the server value from https://127.0.0.1:6443 to your node1 IP
Copy the kubeconfig and add into your local ~/.</description>
    </item>
    
    <item>
      <title>67-Harvester persistent volume on k3s Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/67-harvester-persistent-volume-on-k3s-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/67-harvester-persistent-volume-on-k3s-cluster/</guid>
      <description>Related task: #1812 K3s cloud provider and csi driver support Environment Setup Docker install rancher v2.6.4 Create one node harvester with enough resource Verify steps Follow step 1~6 in tets plan 66-Deploy Harvester csi driver to k3s Cluster
Create Nginx workload for testing Create a nginx-csi deployment with image nginx:latest. Create a new PVC in storage tab: Complete the nginx deployment, create related PV in Harvester volume Click Execute shell to access Nginx container.</description>
    </item>
    
    <item>
      <title>68-Fully airgapped rancher integrate with harvester with no proxy</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/68-fully-airgapped-rancher-integrate-harvester-no-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/68-fully-airgapped-rancher-integrate-harvester-no-proxy/</guid>
      <description>Related task: #1808 RKE2 provisioning fails when Rancher has no internet access (air-gapped)
Note1: in fully air gapped environment, you have to setup private docker hub registry and pull all rancher related offline image
Note2: Please use SUSE SLES JeOS image, it have qemu-guest-agent already installed, thus the guest VM can get IP correctly
Environment Setup Setup the airgapped harvester
Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.</description>
    </item>
    
    <item>
      <title>69-DHCP Harvester LoadBalancer service no health check</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/69-dhcp-loadbalancer-service-no-health-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/69-dhcp-loadbalancer-service-no-health-check/</guid>
      <description>Prerequisite: Already provision RKE1/RKE2 cluster in previous test case
Open Global Settings in hamburger menu Replace ui-dashboard-index to https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Change ui-offline-preferred to Remote Refresh the current page (ctrl + r) Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Click Create Select Load Balancer Given service name to make the load balancer name composed of the cluster name, namespace, svc name, and suffix(8 characters) more than 63 characters Provide Listening port and Target port Click Add-on Config</description>
    </item>
    
    <item>
      <title>70-Pool LoadBalancer service no health check</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/70-pool-loadbalancer-service-no-health-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/70-pool-loadbalancer-service-no-health-check/</guid>
      <description>Prerequisite: Already provision RKE1/RKE2 cluster in previous test case
Open Global Settings in hamburger menu
Replace ui-dashboard-index to https://releases.rancher.com/harvester-ui/dashboard/latest/index.html
Change ui-offline-preferred to Remote
Refresh the current page (ctrl + r)
Access Harvester dashboard UI
Go to Settings
Create a vip-pool in Harvester settings. Open provisioned RKE2 cluster from hamburger menu
Drop down Service Discovery
Click Services
Click Create
Select Load Balancer Given service name
Provide Listening port and Target port Click Add-on Config</description>
    </item>
    
    <item>
      <title>Add a custom &#34;Docker Install URL&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-docker-install-url/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-docker-install-url/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Add a custom &#34;Insecure Registries&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-insecure-registries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-insecure-registries/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Go to node, execute docker info, check the &amp;ldquo;Insecure Registries&amp;rdquo; setting is &amp;ldquo;harbor.wujing.site&amp;rdquo; Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Add a custom &#34;Registry Mirrors&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-registry-mirrors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-registry-mirrors/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Go to node, execute &amp;ldquo;docker info&amp;rdquo;, check the &amp;ldquo;Registry Mirrors&amp;rdquo; setting is &amp;ldquo;https://s06nkgus.mirror.aliyuncs.com&amp;rdquo; Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Add a custom &#34;Storage Driver&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-storage-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-storage-driver/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Go to node, execute &amp;ldquo;docker info&amp;rdquo;, check the Storage Driver setting is overlay Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Add a network to an existing VM with only 1 network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-only-1-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-only-1-network/</guid>
      <description> Add a network to the VM Save the VM Wait for it to start/restart Expected Results the VM should start successfully The already existing network connectivity should still work The new connectivity should also work </description>
    </item>
    
    <item>
      <title>Add a network to an existing VM with two networks</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-two-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-two-networks/</guid>
      <description> Add a network to the VM Save the VM Wait for it to start/restart Expected Results the VM should start successfully The already existing network connectivity should still work The new connectivity should also work </description>
    </item>
    
    <item>
      <title>Add a node to existing cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/add-node-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/add-node-cluster/</guid>
      <description> Start with harvester installer and selectÂ &amp;lsquo;Join an existing Harvester cluster&amp;rsquo; Provide the management ip and cluster token Expected Results On completion, Harvester should show the same management url as of existing node and status as ready. Check the host section, the joined node must appear </description>
    </item>
    
    <item>
      <title>Add cluster driver</title>
      <link>https://harvester.github.io/tests/manual/node-driver/add-cluster-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/add-cluster-driver/</guid>
      <description> Cluster Management &amp;gt; Drivers &amp;gt; Node Drivers Click &amp;ldquo;Add Node driver&amp;rdquo; Add the correct configuration and save Expected Results Created successfully, status is active </description>
    </item>
    
    <item>
      <title>Add extra disks by using raw disks</title>
      <link>https://harvester.github.io/tests/manual/_incoming/extra-disk-using-raw-disk/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/extra-disk-using-raw-disk/</guid>
      <description>Prepare a disk (with WWN) and attach it to the node. Navigate to &amp;ldquo;Host&amp;rdquo; &amp;gt; &amp;ldquo;Edit Config&amp;rdquo; &amp;gt; &amp;ldquo;Disks&amp;rdquo; and open the dropdown menu &amp;ldquo;Add disks&amp;rdquo;. Choose a disk to add, e.g. /dev/sda but not /dev/sda1. Expected Results The raw disk shall be schedulable as a longhorn disk as a whole (without any partition). Ths raw disk shall be in provisioned phase. Reboot the host and the disk shall be reattached and added back as a longhorn disk.</description>
    </item>
    
    <item>
      <title>Add Labels (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/add-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/add-labels/</guid>
      <description> Add multiple labels to the images. Click save Expected Results Labels should be added successfully </description>
    </item>
    
    <item>
      <title>Add multiple Networks via form</title>
      <link>https://harvester.github.io/tests/manual/network/add-multiple-networks-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/add-multiple-networks-form/</guid>
      <description> Create a new VM via the web form Add both a management network and an external VLAN network Validate both interfaces exist in the VM ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should create You should see three interfaces listed in VM You should get responses from pinging the VM You should get responses from pinging the VM </description>
    </item>
    
    <item>
      <title>Add multiple Networks via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/add-multiple-networks-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/add-multiple-networks-yaml/</guid>
      <description> Create a new VM via YAML Add both a management network and an external VLAN network Validate both interfaces exist in the VM ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should create You should see three interfaces listed in VM You should get responses from pinging the VM You should get responses from pinging the VM </description>
    </item>
    
    <item>
      <title>Add network reachability detection from host for the VLAN network</title>
      <link>https://harvester.github.io/tests/manual/network/add-network-reachability-detection-from-host-for-vlan-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/add-network-reachability-detection-from-host-for-vlan-network/</guid>
      <description>Related issue: #1476 Add network reachability detection from host for the VLAN network Category: Network Environment Setup The network environment must have vlan network configured and also have DHCP server prepared on your testing vlan Verification Steps Enable virtual network with harvester-mgmt in harvester Create VLAN 806 with id 806 and set to default auto mode Import harvester to rancher 1 .Create cloud credential Provision a rke2 cluster to harvester Deploy a nginx server workload Open Service Discover -&amp;gt; Services</description>
    </item>
    
    <item>
      <title>Add the different roles to the cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/q-cluster-different-roles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/q-cluster-different-roles/</guid>
      <description> Create three users user1, user2, user3 Give the roles of Cluster Owner to user1, Create Project to user2 and Cluster Member to user3 respectively. Login with these three roles Expected Results </description>
    </item>
    
    <item>
      <title>Add VLAN network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/add-vlan-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/add-vlan-network/</guid>
      <description>Environment setup This should be done on a Harvester setup with at least 2 NICs and at least 2 nodes. This is easily tested in Vagrant
Verification Steps Open settings on a harvester cluster Navigate to the VLAN settings page Click Enabled Check dropdown for NICs and verify that percentage is showing 100% Add the NIC Click Save Validate that it has updated in settings Expected Results You should be able to add the VLAN network device You should see in the settings list that it has your new default NIC </description>
    </item>
    
    <item>
      <title>Add/remove a node in the created harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-add-remove-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-add-remove-node/</guid>
      <description> add/remove a node in the created harvester cluster Expected Results rancher on the cluster modified successfully harvester corresponding VM node added/removed successfully </description>
    </item>
    
    <item>
      <title>Add/remove disk to Host config</title>
      <link>https://harvester.github.io/tests/manual/hosts/1623-add-disk-to-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/1623-add-disk-to-host/</guid>
      <description> Related issues: #1623 Unable to add additional disks to host config Environment setup Add Disk that isn&amp;rsquo;t assigned to host Verification Steps Head to &amp;ldquo;Hosts&amp;rdquo; page Click &amp;ldquo;Edit Config&amp;rdquo; on a node and switch to &amp;ldquo;Disks&amp;rdquo; tab Validate: Open dropdown and see no disks Attach a disk on that node Validate: Open dropdown and see some disks Verify that host shows new disk as available storage and Longhorn is showing new schedulable space Detach a disk on that node Validate: Open dropdown and see no disks Verify that host shows new disk as available storage and Longhorn is showing new schedulable space Expected Results Disk space should show appropriately </description>
    </item>
    
    <item>
      <title>Add/remove disk to Host config</title>
      <link>https://harvester.github.io/tests/manual/volumes/1623-add-disk-to-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/1623-add-disk-to-host/</guid>
      <description> Related issues: #1623 Unable to add additional disks to host config Environment setup Add Disk that isn&amp;rsquo;t assigned to host Verification Steps Head to &amp;ldquo;Hosts&amp;rdquo; page Click &amp;ldquo;Edit Config&amp;rdquo; on a node and switch to &amp;ldquo;Disks&amp;rdquo; tab Validate: Open dropdown and see no disks Attach a disk on that node Validate: Open dropdown and see some disks Verify that host shows new disk as available storage and Longhorn is showing new schedulable space Detach a disk on that node Validate: Open dropdown and see no disks Verify that host shows new disk as available storage and Longhorn is showing new schedulable space Expected Results Disk space should show appropriately </description>
    </item>
    
    <item>
      <title>Additional trusted CA configure-ability</title>
      <link>https://harvester.github.io/tests/manual/deployment/additional-trusted-ca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/additional-trusted-ca/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1260
Verify Items Image download with self-signed additional-ca VM backup with self-signed additional-ca Case: Image downlaod Install Harvester with ipxe-example which includes https://github.com/harvester/ipxe-examples/pull/36 Upload any valid iso to pxe-server&amp;rsquo;s /var/www/ Use Browser to access https://&amp;lt;pxe-server-ip&amp;gt;/&amp;lt;iso-file&amp;gt; should be valid Add self-signed cert to Harvester Navigate to Harvester Advanced Settings, edit additional-ca cert content can be retrieved in pxe-server /etc/ssl/certs/nginx-selfsigned.crt Create Image with the same URL https://&amp;lt;pxe-server-ip&amp;gt;/&amp;lt;iso-file&amp;gt; Image should be downloaded Case: VM backup Install Harvester with ipxe-example setup Minio in pxe-server follow instruction to download binary and start the service login to UI console then add region and create bucket follow instruction to generate self-signed cert with IP SANs restart service with self-signed cert Add self-signed cert to Harvester Add local Minio info as S3 into backup-target Backup-Target Should not pop up any Error Message Create Image for VM creation Create VM with any resource Perform VM backup VM&amp;rsquo;s data Should be backup into Minio&amp;rsquo;s folder </description>
    </item>
    
    <item>
      <title>Agent Node should not rely on specific master Node</title>
      <link>https://harvester.github.io/tests/manual/hosts/agent_node_connectivity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/agent_node_connectivity/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1521
Verify Items Agent Node should keep connection when any master Node is down Case: Agent Node&amp;rsquo;s connecting status Install Harvester with 4 nodes which joining node MUST join by VIP (point server-url to use VIP) Make sure all nodes are ready Login to dashboard, check host state become Active SSH to the 1st node, run command kubectl get node to check all STATUS should be Ready SSH to agent nodes which ROLES IS &amp;lt;none&amp;gt; in Step 2i&amp;rsquo;s output Output should contains VIP in the server URL, by run command cat /etc/rancher/rke2/config.</description>
    </item>
    
    <item>
      <title>allow users to create cloud-config template on the VM creating page</title>
      <link>https://harvester.github.io/tests/manual/templates/allow-users-to-create-cloud-config-template-on-vm-creating-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/templates/allow-users-to-create-cloud-config-template-on-vm-creating-page/</guid>
      <description> Related issues: #1433 allow users to create cloud-config template on the VM creating page Category: Virtual Machine Verification Steps Create a new virtual machine Click advanced options Drop down user data template -&amp;gt; create new Drop down network data template -&amp;gt; create new Expected Results User can create user and network data template when create virtual machine Created cloud-init template template can be saved and auto selected to the latest one </description>
    </item>
    
    <item>
      <title>Attach unpartitioned NVMe disks to host</title>
      <link>https://harvester.github.io/tests/manual/hosts/attach-unpartitioned-nvme-disks-to-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/attach-unpartitioned-nvme-disks-to-host/</guid>
      <description>Related issues: #1414 Adding unpartitioned NVMe disks fails Category: Storage Verification Steps Use qemu-img create -f qcow2 command to create three disk image locally Shutdown target node VM machine Directly edit VM xml content in virt manager page Add to the first line Add the following line before the end of quote &amp;lt;qemu:commandline&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme301.img,if=none,id=D22&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-device&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;nvme,drive=D22,serial=1234&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme302.img,if=none,id=D23&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-device&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;nvme,drive=D23,serial=1235&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme303.</description>
    </item>
    
    <item>
      <title>Authentication Validation</title>
      <link>https://harvester.github.io/tests/manual/authentication/general-authentication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/general-authentication/</guid>
      <description>Enable Access Control . Choose â€œAllow any valid Userâ€ as â€œSite Accessâ€. Make sure any user is able to access the site. Enable Access Control . Choose â€œRestrict to Specific Userâ€ and add few users. Make sure only the specified users have access to the server. Others should get authentication error. Enable Access Control . Choose â€œRestrict to Specific Userâ€ and add a group. Make sure only all users belonging to the group have access to the server Others should get authentication error.</description>
    </item>
    
    <item>
      <title>Auto provision lots of extra disks</title>
      <link>https://harvester.github.io/tests/manual/_incoming/large-amount-of-extra-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/large-amount-of-extra-disks/</guid>
      <description>:warning: This is a heuristic test plan since real world race condition is hard to reproduce. If you find any better alternative, feel free to update.
This test is better to perform under QEMU/libvirt environment.
Related issues: #1718 [BUG] Automatic disk provisioning result in unusable ghost disks on NVMe drives Category: Storage Verification Steps Create a harvester cluster and attach 10 or more extra disks (needs WWN so that they can be identified uniquely).</description>
    </item>
    
    <item>
      <title>Automatically get VIP during PXE installation</title>
      <link>https://harvester.github.io/tests/manual/deployment/1410-pxe-installation-automatically-get-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/1410-pxe-installation-automatically-get-vip/</guid>
      <description> Related issues: #1410 Support getting VIP automatically during PXE boot installation Verification Steps Comment vip and vip_hw_addr in ipxe-examples/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2 Start vagrant-pxe-harvester Run kubectl get cm -n harvester-system vip Check whether we can get ip and hwAddress in it Run ip a show harvester-mgmt Check whether there are two IPs in it and one is the vip. Expected Results VIP should automatically be assigned </description>
    </item>
    
    <item>
      <title>Automatically get VIP during PXE installation</title>
      <link>https://harvester.github.io/tests/manual/hosts/1410-pxe-installation-automatically-get-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/1410-pxe-installation-automatically-get-vip/</guid>
      <description> Related issues: #1410 Support getting VIP automatically during PXE boot installation Verification Steps Comment vip and vip_hw_addr in ipxe-examples/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2 Start vagrant-pxe-harvester Run kubectl get cm -n harvester-system vip Check whether we can get ip and hwAddress in it Run ip a show harvester-mgmt Check whether there are two IPs in it and one is the vip. Expected Results VIP should automatically be assigned </description>
    </item>
    
    <item>
      <title>Backup and restore of harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/q-cluster-backup-restore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/q-cluster-backup-restore/</guid>
      <description> create a deployment in harvester cluster Go to the rancher&amp;rsquo;s cluster list and make a backup of the harvester cluster After the backup is complete, delete the deployment created in the harvester cluster go to the list of clusters in the rancher and restore the harvester cluster Expected Results </description>
    </item>
    
    <item>
      <title>Backup S3 reduce permissions</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup_s3_permission/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup_s3_permission/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1339
Verify Items Backup target connect to S3 should only require the permission to access the specific bucket Case: S3 Backup with single-bucket-user Install Harvester with any nodes Setup Minio then follow the instruction to create a single-bucket-user. Create specific bucket for the user Create other buckets setup backup-target with the single-bucket-user permission When assign the dedicated bucket (for the user), connection should success. When assign other buckets, connection should failed with AccessDenied error message </description>
    </item>
    
    <item>
      <title>Backup Single VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm/</guid>
      <description> Click take backup in virtual machine list Expected Results Backup should be created Backup should be listed in backups list Backup should be available on remote storage (S3/NFS) </description>
    </item>
    
    <item>
      <title>Backup Single VM that has been live migrated before (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-that-has-been-live-migrated/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-that-has-been-live-migrated/</guid>
      <description> Click take backup in virtual machine list Expected Results Backup should be created Backup should be listed in backups list Backup should be available on remote storage (S3/NFS) </description>
    </item>
    
    <item>
      <title>Backup single VM with node off</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-node-off/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-node-off/</guid>
      <description>On multi-node setup bring down node that is hosting VM Click take backup in virtual machine list Expected Results The backup should complete successfully Comments We do allow taking backup even if the VM is down, as you can take backup when the VM is off, this is because the volume still exists with longhorn&amp;rsquo;s multi replicas, but weneed to check the data integrity.
Known Bugs https://github.com/harvester/harvester/issues/1483</description>
    </item>
    
    <item>
      <title>Backup Target error message</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup_target_errmsg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup_target_errmsg/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1051
Verify Items Backup target should check input before Click Save Error message should displayed on edit page when input is wrong Case: Connect to invalid Backup Target Install Harvester with any node Login to dashboard, then navigate to Advanced Settings Edit backup-target,then input invalid data for NFS/S3 and click Save The Page should not be redirect to Advanced Settings Error Message should displayed under Save button </description>
    </item>
    
    <item>
      <title>Basic functional verification of Harvester cluster after creation</title>
      <link>https://harvester.github.io/tests/manual/node-driver/verify-cluster-functionality/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/verify-cluster-functionality/</guid>
      <description> create the project. deploy deployment Expected Results The project is created successfully Deployment successfully deployed </description>
    </item>
    
    <item>
      <title>Better Load Balancer Config of Harvester cloud provider</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/better-load-balancer-config-rke2-cloud-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/better-load-balancer-config-rke2-cloud-provider/</guid>
      <description>Related issue: #1435 better loadblancer config of Harvester cloud provider Category: Rancher Integration Environment setup Install rancher 2.6.3 by docker docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Import harvester to rancher virtualization management Create a harvester cluster by harvester driver Access the new harvester cluster from rancher cluster management Create a load balancer from service discovery -&amp;gt; services Re login rancher Open create load-balance page Click ctrl+R to refresh page Check the &amp;ldquo;Add-on Config&amp;rdquo; tabs Expected Results User can configure port, IPAM and health check related setting on Add-on Config page Can create load balancer correctly with health check setting</description>
    </item>
    
    <item>
      <title>Boot installer under Legacy BIOS and UEFI</title>
      <link>https://harvester.github.io/tests/manual/_incoming/2023-boot-installer-legacy-and-uefi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/2023-boot-installer-legacy-and-uefi/</guid>
      <description>Related issues #2023 Legacy Iso for older servers Verification Steps BIOS Test Build harvester-installer Boot build artifact using BIOS Legacy mode: qemu-system-x86_64 -m 2048 -cdrom ../dist/artifacts/harvester-master-amd64 Verify that the installer boot process reaches the screen that says &amp;ldquo;Create New Cluster&amp;rdquo; or &amp;ldquo;Join existing cluster&amp;rdquo; UEFI Test Build harvester-installer (or use the same one from the BIOS Test, it&amp;rsquo;s a hybrid ISO) Boot build artifact using UEFI mode: qemu-system-x86_64 -m 2048 -cdrom .</description>
    </item>
    
    <item>
      <title>Button of `Download KubeConfig`</title>
      <link>https://harvester.github.io/tests/manual/misc/download_kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/misc/download_kubeconfig/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1349
Verify Items Download KubeConfig should not exist in general views Download Kubeconfig should exist in Support page Downloaded file should be named with suffix .yaml Case: Download KubeConfig navigate to every pages to make sure download kubeconfig icon will not appear in header section navigate to support page to check Download KubeConfig is work normally </description>
    </item>
    
    <item>
      <title>Chain VM templates and images</title>
      <link>https://harvester.github.io/tests/manual/templates/760-chained-vm-templates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/templates/760-chained-vm-templates/</guid>
      <description> Related issues: #760 cloud config byte limit Verification Steps Create a vm and add userData or networkData, test if it works Run VM health checks create a vm template and add userData create a new vm and use the template Run VM health checks use the existing vm to generate a template, then use the template to create a new vm Run VM health Checks Expected Results All VM&amp;rsquo;s should create All VM Health Checks should pass </description>
    </item>
    
    <item>
      <title>Chain VM templates and images</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/760-chained-vm-templates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/760-chained-vm-templates/</guid>
      <description> Related issues: #760 cloud config byte limit Verification Steps Create a vm and add userData or networkData, test if it works Run VM health checks create a vm template and add userData create a new vm and use the template Run VM health checks use the existing vm to generate a template, then use the template to create a new vm Run VM health Checks Expected Results All VM&amp;rsquo;s should create All VM Health Checks should pass </description>
    </item>
    
    <item>
      <title>Change api-ui-source bundled</title>
      <link>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-bundled/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-bundled/</guid>
      <description> Log in as admin Navigate to advanced settings Change api-ui-source to bundled Save Refresh page Check page source for dashboard loading location Expected Results Log in should complete Settings should save dashboard location should be loading from /dashboard/_nuxt/ (verify it in browser&amp;rsquo;s developers tools) </description>
    </item>
    
    <item>
      <title>Change api-ui-source external</title>
      <link>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-external/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-external/</guid>
      <description> Log in as admin Navigate to advanced settings Change api-ui-source to external Save Refresh page Check page source for dashboard loading location Expected Results Log in should complete Settings should save dashboard location should be loading from https://releases.rancher.com/harvester-ui/latest (verify it in browser&amp;rsquo;s developers tools) </description>
    </item>
    
    <item>
      <title>Change DNS servers while installing</title>
      <link>https://harvester.github.io/tests/manual/deployment/1590-change-dns-server-for-install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/1590-change-dns-server-for-install/</guid>
      <description>Related issues: #1590 Harvester installer can&amp;rsquo;t resolve hostnames Known Issues When supplying multiple ip=&amp;hellip; kernel cmdline arguments, only one of them will be configured by dracut, therefore only the configured interface would have ifcfg generated. So for now, we can&amp;rsquo;t support multiple ip=&amp;hellip; kernel cmdline arguments
Verification Steps Because configuring the network of the installation environment only works with PXE installation, you could use ipxe-examples/vagrant-pxe-harvester/ to set it up. Be sure you can run setup_harvester.</description>
    </item>
    
    <item>
      <title>Change log level debug</title>
      <link>https://harvester.github.io/tests/manual/advanced/change-log-level-debug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/change-log-level-debug/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on log-level Choose Debug Save Create two VMs Reboot both VMs Download Logs Expected Results Login should complete Settings should save VMs should create VMs should reboot sucessfully Logs should show Debug level output </description>
    </item>
    
    <item>
      <title>Change log level Info</title>
      <link>https://harvester.github.io/tests/manual/advanced/change-log-level-info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/change-log-level-info/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on log-level Choose Info Save Create two VMs Reboot both VMs Download Logs Expected Results Login should complete Settings should save VMs should create VMs should reboot sucessfully Logs should show Info level output </description>
    </item>
    
    <item>
      <title>Change log level Trace</title>
      <link>https://harvester.github.io/tests/manual/advanced/change-log-level-trace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/change-log-level-trace/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on log-level Choose Trace Save Create two VMs Reboot both VMs Download Logs Expected Results Login should complete Settings should save VMs should create VMs should reboot sucessfully Logs should show Trace level output </description>
    </item>
    
    <item>
      <title>Change user passowrd</title>
      <link>https://harvester.github.io/tests/manual/authentication/1409-change-password/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/1409-change-password/</guid>
      <description> Related issues: #1409 There&amp;rsquo;s no way to change user password in single cluster UI Verification Steps Logged in with user Changed password Logged out Logged back in with new password Verified old password didn&amp;rsquo;t work Expected Results Password should change and be accepted on new login Old password shouldn&amp;rsquo;t work </description>
    </item>
    
    <item>
      <title>Check can apply the resource quota limit to project and namespace</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/check-can-apply-the-resource-quota-limit-to-project-and-namespace-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/check-can-apply-the-resource-quota-limit-to-project-and-namespace-/</guid>
      <description>Related issues: #1454 Incorrect memory unit conversion in namespace resource quota Category: Rancher Integration Environment setup Install the latest rancher from docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Access Rancher dashboard Open Cluster management -&amp;gt; Explore the active cluster Create a new project test-1454-proj in Projects/Namespaces Set resource quota for the project Memory Limit: Project Limit: 512 Namespace default limit: 256 Memory Reservation: Project Limit: 256 Namespace default limit: 128 Click create namespace test-1454-ns under project test-1454-proj Click Kubectl Shell and run the following command kubectl get ns kubectl get quota -n test-1454-ns Check the output Click Workload -&amp;gt; Deployments -&amp;gt; Create Given the Name, Namespace and Container image Click Create Expected Results Based on configured project resource limit and namespace default limit,</description>
    </item>
    
    <item>
      <title>Check conditions when stop/pause VM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1987-failure-message-in-stopping-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1987-failure-message-in-stopping-vm/</guid>
      <description>Related issues: #1987 Verification Steps Stop Request should not have failure message
Create a VM with runStrategy: RunStrategyAlways. Stop the VM. Check there is no Failure attempting to delete VMI: &amp;lt;nil&amp;gt; in VM status. UI should not show pause message
Create a VM. Pause the VM. Although the message The status of pod readliness gate &amp;quot;kubevirt.io/virtual-machine-unpaused&amp;quot; is not &amp;quot;True&amp;quot;, but False is in the VM condition, UI should not show it.</description>
    </item>
    
    <item>
      <title>Check crash dump when there&#39;s a kernel panic</title>
      <link>https://harvester.github.io/tests/manual/hosts/1357-kernel-panic-check-crash-dump/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/1357-kernel-panic-check-crash-dump/</guid>
      <description> Related issues: #1357 Crash dump not written when kernel panic occurs Verification Steps Created new single node cluster with 16GB RAM Booted into debug mode from GRUB entry Created several VMs triggered kernel panic with echo c &amp;gt;/proc/sysrq-trigger Waited for reboot Verified that dump was saved in /var/crash Expected Results dump should be saved in /var/crash </description>
    </item>
    
    <item>
      <title>Check default and customized project and namespace details page</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/check-default-customized-project-and-namespace-details-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/check-default-customized-project-and-namespace-details-page/</guid>
      <description> Related issue: #1574 Multi-cluster projectNamespace details page error Category: Rancher Integration Environment setup Install rancher 2.6.3 by docker docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Import harvester from rancher dashboard Access harvester from virtualization management page Create several new projects Create several new namespaces under each new projects Access all default and self created namespace Check can display namespace details Check all new namespaces can display correctly under each projects Expected Results Access harvester from rancher virtualization management page Click any namespace in the Projects/Namespace can display details correctly with no page error Default namespace Customized namespace Newly created namespace will display under project list </description>
    </item>
    
    <item>
      <title>check detailed network status in host page</title>
      <link>https://harvester.github.io/tests/manual/hosts/check-detailed-network-status-in-host-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/check-detailed-network-status-in-host-page/</guid>
      <description> Related issues: #531 Better error messages when misconfiguring multiple nics Category: Host Verification Steps Enable vlan cluster network setting and set a default network interface Wait a while for the setting take effect on all harvester nodes Click nodes on host page Check the network tab Expected Results On the Host view page, now we can see detailed network status including Name, Type, IP Address, Status etc.. Check all network interface can display Check the Name, Type, IP Address, Status display correct values </description>
    </item>
    
    <item>
      <title>Check DNS on install with Github SSH keys</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1903-dns-github-ssh-keys/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1903-dns-github-ssh-keys/</guid>
      <description>Related issues: #1903 DNS server not available during install Verification Steps Without PXE Start a new install Set DNS as 8.8.8.8 Add in github SSH keys Finish install SSH into node with SSH keys from github (rancher@hostname) Verify login was successful With PXE Got vagrant setup from https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Changed settings.yml DHCP config and added dns: 8.8.8.8 dhcp_server: ip: 192.168.0.254 subnet: 192.168.0.0 netmask: 255.255.255.0 range: 192.168.0.50 192.168.0.130 dns: 8.8.8.8 https: false Also changed ssh_authorized_keys and commented out default SSH key and added username for github</description>
    </item>
    
    <item>
      <title>Check favicon and title on pages</title>
      <link>https://harvester.github.io/tests/manual/misc/1520-check-title-and-favicon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/misc/1520-check-title-and-favicon/</guid>
      <description> Related issues: #1520 incorrect title and favicon Verification Steps Log into Harvester Check page title and favicon on each of these pages dashboard main page settings support Volumes SSH Keys Host info Expected Results Harvester favicon and title should show on each page </description>
    </item>
    
    <item>
      <title>Check IPAM configuration with IPAM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1697-ipam-load-balancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1697-ipam-load-balancer/</guid>
      <description> Related issues: #1697 Optimization for the Harvester load balancer Verification Steps Install the latest rancher and import a Harvester cluster Create a cluster by Harvester node driver Navigate to the workload Page, create a workload Click &amp;ldquo;Add ports&amp;rdquo;, select type as LB, protocol as TCP Check IPAM selector Navigate to the service page, create a LB Click &amp;ldquo;Add-on config&amp;rdquo; tab and check IPAM and port </description>
    </item>
    
    <item>
      <title>Check Longhorn volume mount point</title>
      <link>https://harvester.github.io/tests/manual/hosts/1667-check-longhorn-volume-mount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/1667-check-longhorn-volume-mount/</guid>
      <description> Related issues: #1667 data partition is not mounted to the LH path properly Verification Steps Install Harvester node in VM from ISO Check partitions with lsblk -f Verify mount point of /var/lib/longhorn Expected Results Mount point should show /var/lib/longhorn </description>
    </item>
    
    <item>
      <title>Check Longhorn volume mount point</title>
      <link>https://harvester.github.io/tests/manual/volumes/1667-check-longhorn-volume-mount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/1667-check-longhorn-volume-mount/</guid>
      <description> Related issues: #1667 data partition is not mounted to the LH path properly Verification Steps Install Harvester node in VM from ISO Check partitions with lsblk -f Verify mount point of /var/lib/longhorn Expected Results Mount point should show /var/lib/longhorn </description>
    </item>
    
    <item>
      <title>Check redirect for editing server URL setting</title>
      <link>https://harvester.github.io/tests/manual/hosts/1489-redirect-for-server-url-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/1489-redirect-for-server-url-setting/</guid>
      <description> Related issues: #1489 Edit Advanced Setting option server-url will redirect to inappropriate page Verification Steps Install harvester Access harvester Edit server-url form settings Check server-url save, cancel, and back. Additional context: Expected Results URL should stay the same when navigating </description>
    </item>
    
    <item>
      <title>Check that you can communicate with the Harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/terraformer/harvester-cluster-communicate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/harvester-cluster-communicate/</guid>
      <description>Set the KUBECONFIG env variable with the path of your kubeconfig file Try to import any resource to test the connectivity with the Harvester cluster For instance, try to import ssh-key with: terraformer import harvester -r ssh_key Expected Results You should see:
terraformer import harvester -r ssh_key 2021/08/04 15:18:59 harvester importing... ssh_key 2021/08/04 15:18:59 harvester done importing ssh_key ... And the generated files should appear in ./generated/harvester/ssh_key/</description>
    </item>
    
    <item>
      <title>Check VM creation required-fields</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/1283-vm-creation-required-fields/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/1283-vm-creation-required-fields/</guid>
      <description> Related issues: #1283 Fix required fields on VM creation page Verification Steps Create VM without image name and size Create VM without size Create VM wihout image name Create VM without hostname Expected Results You should get an error trying to create VM without image name and size You should get an error trying to create VM without image name You should get an error trying to create VM without size You should not get an error trying to create a VM without hostname </description>
    </item>
    
    <item>
      <title>Clone VM and don&#39;t select start after creation</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-and-dont-select-start-after-creation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-and-dont-select-start-after-creation/</guid>
      <description> Clone VM from Virtual Machine list Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console </description>
    </item>
    
    <item>
      <title>Clone VM that is turned off</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/</guid>
      <description> Clone VM from Virtual Machine list that is turned off Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console </description>
    </item>
    
    <item>
      <title>Clone VM that is turned on</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/</guid>
      <description> Clone VM from Virtual Machine list that is turned on Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console </description>
    </item>
    
    <item>
      <title>Clone VM that was created from image</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-image/</guid>
      <description> Clone VM from Virtual Machine list Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console </description>
    </item>
    
    <item>
      <title>Clone VM that was created from template</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-template/</guid>
      <description> Clone VM from Virtual Machine list Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console </description>
    </item>
    
    <item>
      <title>Clone VM that was not created from image</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-not-created-from-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-not-created-from-image/</guid>
      <description> Clone VM from Virtual Machine list Expected Results Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console </description>
    </item>
    
    <item>
      <title>Cluster add labs</title>
      <link>https://harvester.github.io/tests/manual/node-driver/create-add-labs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/create-add-labs/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results Use the command &amp;ldquo;kubectl get node &amp;ndash;show-labels&amp;rdquo; to see the success of the added tabs Go to the node details page of UI, click the &amp;ldquo;Edit Node&amp;rdquo; button, and check Labels Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Cluster add Taints</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-add-taints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-add-taints/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results Use the command kubectl describe node test-tain5 | grep Taint to see if Taint was added successfully. Go to the node details page of UI, click the &amp;ldquo;Edit Node&amp;rdquo; button, and check Taint Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Cluster TLS customization</title>
      <link>https://harvester.github.io/tests/manual/advanced/tls_customize/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/tls_customize/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1046
Verify Items Cluster&amp;rsquo;s SSL/TLS parameters could be configured in install option Cluster&amp;rsquo;s SSL/TLS parameters could be updated in dashboard Case: Configure TLS parameters in dashboard Install Harvester with any nodes Navigate to Advanced Settings, then edit ssl-parameters Select Protocols TLSv1.3, then save execute command echo QUIT | openssl s_client -connect &amp;lt;VIP&amp;gt;:443 -tls1_2 | grep &amp;quot;Cipher is&amp;quot; Output should contain error...SSL routines... and Cipher is (NONE) execute command echo QUIT | openssl s_client -connect &amp;lt;VIP&amp;gt;:443 -tls1_3 | grep &amp;quot;Cipher is&amp;quot; Output should contain Cipher is &amp;lt;one_of_TLS1_3_Ciphers&amp;gt;1 and should not contain error.</description>
    </item>
    
    <item>
      <title>CPU overcommit on VM</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/cpu_overcommit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/cpu_overcommit/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1429
Verify Items Overcommit can be edit on Dashboard VM can allocate exceed CPU on the host Node VM can chage allocated CPU after created Case: Update Overcommit configuration Install Harvester with any Node Login to Dashboard, then navigate to Advanced Settings Edit overcommit-config The field of CPU should be editable Created VM can allocate maximum CPU should be &amp;lt;HostCPUs&amp;gt; * [&amp;lt;overcommit-CPU&amp;gt;/100] - &amp;lt;Host Reserved&amp;gt; Case: VM can allocate CPUs more than Host have Install Harvester with any Node Create a cloud image for VM Creation Create a VM with &amp;lt;HostCPUs&amp;gt; * 5 CPUs VM should start successfully lscpu in VM should display allocated CPUs Page of Virtual Machines should display allocated CPUs correctly Case: Update VM allocated CPUs Install Harvester with any Node Create a cloud image for VM Creation Create a VM with &amp;lt;HostCPUs&amp;gt; * 5 CPUs VM should start successfully Increase/Reduce VM allocated CPUs to minimum/maximum VM should start successfully lscpu in VM should display allocated CPUs Page of Virtual Machines should display allocated CPUs correctly </description>
    </item>
    
    <item>
      <title>Create a 3 nodes harvester cluster with RKE1 (only with mandatory info, other values stays with default)</title>
      <link>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke1/</guid>
      <description> From the Rancher home page, click on Create Select RKE1 on the right and click on Harvester Enter a cluster name Give a prefix name for the VMs Increase count to 3 nodes Check etcd, Control Plane and Worker boxes Select or create a node template if needed Click on Add node template Create credentials by selecting your harvester cluster Fill the instance option fields, pay attention to correctly write the default ssh user of the chosen image in the SSH user field Give a name to the rancher template and click on Create Click on create to spin the cluster up Expected Results The status of the created cluster shows active The status of the corresponding vm on harvester active The 3 nodes should be with the active status </description>
    </item>
    
    <item>
      <title>Create a 3 nodes harvester cluster with RKE2 (only with mandatory info, other values stays with default)</title>
      <link>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke2/</guid>
      <description> From the Rancher home page, click on Create Select RKE2 on the right and click on Harvester Create the credential to talk with the harvester provider Select your harvester cluster (external or internal) Enter a cluster name Increase machine count to 3 Fill the mandatory fields Namespace Image Network SSH User (default ssh user of the chosen image) Click on create to spin the cluster up Expected Results The status of the created cluster shows active The status of the corresponding vm on harvester active The 3 nodes should be with the active status </description>
    </item>
    
    <item>
      <title>Create a harvester cluster and add Taint to a node</title>
      <link>https://harvester.github.io/tests/manual/node-driver/q-cluster-add-taint/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/q-cluster-add-taint/</guid>
      <description>Expected Results </description>
    </item>
    
    <item>
      <title>Create a harvester cluster with 3 master nodes</title>
      <link>https://harvester.github.io/tests/manual/node-driver/add-3-master-nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/add-3-master-nodes/</guid>
      <description> add a harvester node template Create harvester cluster count set to 3 Expected Results The status of the created cluster shows active show the 3 created node status running in harvester&amp;rsquo;s vm list the information displayed on rancher and harvester matches the template configuration </description>
    </item>
    
    <item>
      <title>Create a harvester cluster with a non-default version of k8s</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-k8s/</guid>
      <description> Verify versions 1.19.10, 1.18.18, 1.17.17, 1.16.15 respectively Expected Results k8s displayed on the UI is consistent with the created version (cluster list, host list) Use kubectl version to see that the version information is the same as the created version </description>
    </item>
    
    <item>
      <title>Create a harvester cluster with different images</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-different-images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-different-images/</guid>
      <description>d a harvester node template Set the image, it should be a drop-down list, refer to &amp;ldquo;Test Data&amp;rdquo; for other values ubuntu-18.04-server-cloudimg-amd64.img focal-server-cloudimg-amd64-disk-kvm.img Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active The status of the corresponding vm on harvester active The information displayed on rancher and harvester matches the template configuration The drop-down list of images in the harvester node template corresponds to the list of images in the harvester Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Create a harvester cluster, template drop-down list validation</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-template-dropdown-multi-user/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-template-dropdown-multi-user/</guid>
      <description> Create multiple harvester Node Templates with different users Add harvester cluster and set Template Expected Results pop up a template list pop-up box Show the templates you created and the templates created by other users </description>
    </item>
    
    <item>
      <title>Create a new VM and add Enable USB tablet option	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-enable-usb-tablet-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-enable-usb-tablet-option/</guid>
      <description> Add Enable usb tablet Option Save/Create VM Expected Results Machine starts successfully Enable usb tablet shows In YAML In Form </description>
    </item>
    
    <item>
      <title>Create a new VM and add Install guest agent option (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-install-guest-agent-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-install-guest-agent-option/</guid>
      <description> Add install Guest Agent Option Save/Create VM Validate that qemu-guest-agent was installed You can do this on ubuntu with the command dpkg -l | grep qemu Expected Results Machine starts successfully Guest Agent Option shows In YAML In Form Guest Agent is installed </description>
    </item>
    
    <item>
      <title>Create a new VM with Network Data from the form</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-the-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-the-form/</guid>
      <description> Add Network Data to the VM Here is an example of Network Data config to add DHCP to the physical interface eth0 network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp Save/Create the VM Expected Results Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0 </description>
    </item>
    
    <item>
      <title>Create a new VM with Network Data from YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-yaml/</guid>
      <description> Add Network Data to the VM via YAML Here is an example of Network Data config to add DHCP to the physical interface eth0 network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp Save/Create the VM Expected Results Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0 </description>
    </item>
    
    <item>
      <title>Create a new VM with User Data from the form</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-user-data-from-the-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-user-data-from-the-form/</guid>
      <description> Add User data to the VM Here is an example of user data config to add a password #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Save/Create the VM Expected Results Machine starts succesfully User data should exist In YAML In Form Machine should have user password set </description>
    </item>
    
    <item>
      <title>Create a VM on a VLAN with an existing machine and then change the existing machine&#39;s VLAN</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-on-a-vlan-with-an-existing-machine-and-then-change-the-existing-machines-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-on-a-vlan-with-an-existing-machine-and-then-change-the-existing-machines-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should NOT be able to connect on network verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server </description>
    </item>
    
    <item>
      <title>Create a VM through the Rancher dashboard</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1613-create-vm-through-rancher-dashboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1613-create-vm-through-rancher-dashboard/</guid>
      <description> Related issues: #1613 VM memory shows NaN Gi Verification Steps import harvester into rancher&amp;rsquo;s virtualization management Load Harvester dashboard by going to virtualization management then clicking on harvester cluster Create a new VM on Harvester Validate the following in the VM list page, the form, and YAML&amp;gt; Memory CPU Disk space Expected Results VM should create VM should start All specifications should show correctly </description>
    </item>
    
    <item>
      <title>Create a VM with 2 networks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-2-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-2-networks/</guid>
      <description>Add a network to the VM Save the VM Wait for it to start/restart Expected Results the VM should start successfully The already existing network connectivity should still work The new connectivity should also work Comments one default management network and one VLAN</description>
    </item>
    
    <item>
      <title>Create a vm with all the default values (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/</guid>
      <description> Create a VM with all default values Save Expected Results VM should save VM should start if start after creation checkbox is checked Config should show In Form In YAML </description>
    </item>
    
    <item>
      <title>Create a VM with Start VM on Creation checked (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-checked/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-checked/</guid>
      <description> Create VM Expected Results VM should start Checkbox for start virtual machine on creation should show as appropriate while editing machine after creation </description>
    </item>
    
    <item>
      <title>Create a VM with start VM on creation unchecked (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/</guid>
      <description> Create VM Expected Results VM should start or not start as appropriate Checkbox for start virtual machine on creation should show as appropriate while editing machine after creation </description>
    </item>
    
    <item>
      <title>Create Backup Target (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/create-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/create-backup-target/</guid>
      <description> Open up Backup-target in settings Input server info Save Expected Results Backup Target should show in settings </description>
    </item>
    
    <item>
      <title>Create harvester cluster using non-default CPUs, Memory, Disk</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-resources/</guid>
      <description>add a harvester node template The set CPUs, Memory, and Disk values, refer to &amp;ldquo;Test Data&amp;rdquo; for other values Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:4 Memorys:8 Disk:50 Bus:Virtlo Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Create harvester clusters with different Bus</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-different-bus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-different-bus/</guid>
      <description>add a harvester node template Set the â€œNetwork Nameâ€, it should be a drop-down list, refer to &amp;ldquo;Test Data&amp;rdquo; for other values VirtIO SATA SCSI Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration The drop-down list of &amp;ldquo;BUS&amp;rdquo; in the harvester node template corresponds to the list of â€œBUSâ€ in the harvester Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Create harvester clusters with different Networks</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-different-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-different-networks/</guid>
      <description>add a harvester node template Set the â€œNetwork Nameâ€, it should be a drop-down list, refer to &amp;ldquo;Test Data&amp;rdquo; for other values vlan1 vlan2 Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration The drop-down list of &amp;ldquo;Network Name&amp;rdquo; in the harvester node template corresponds to the list of â€œNetwork Nameâ€ in the harvester Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Create image from Volume</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-image-from-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/create-image-from-volume/</guid>
      <description>Create new VM Add SSH key Run through iterations for 1, 2, and 3 for attached bash script Export volume to image from volumes page Create new VM from image Run md5sum -c file2.md5 file1-2.md5 file2-2.md5 file3.md5 Expected Results image should upload/complete in images page New VM should create SSH key should work on new VM file2.md5 should fail and the other three md5 checks should pass Comments #!/bin/bash # first file if [ $1 = 1 ] then dd if=/dev/urandom of=file1.</description>
    </item>
    
    <item>
      <title>Create Images with valid image URL (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/create-images-with-valid-image-url/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/create-images-with-valid-image-url/</guid>
      <description>Create image with cloud image available for openSUSE. http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 Expected Results Image should show state as Active. Check the backing image in Longhorn Known Bugs https://github.com/harvester/harvester/issues/1269</description>
    </item>
    
    <item>
      <title>Create multiple instances of the vm with ISO image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-iso-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-iso-image/</guid>
      <description>Create images using the external path for ISO image. In user data mention the below to access the vm. Create the 3 vms and wait for vm to start Expected Results 3 vm should come up and start with same config. Observe the time taken for the system to start the vms. Observe the pattern of the vms get allocated on the nodes. Like how many vm on each nodes are created.</description>
    </item>
    
    <item>
      <title>Create multiple instances of the vm with raw image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-raw-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-raw-image/</guid>
      <description>Create images using the external path for cloud image. In user data mention the below to access the vm. Create the 3 vms and wait for vm to start. Expected Results 3 vm should come up and start with same config. Observe the time taken for the system to start the vms. Observe the pattern of the vms get allocated on the nodes. Like how many vm on each nodes are created.</description>
    </item>
    
    <item>
      <title>Create multiple instances of the vm with Windows Image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-windows-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-windows-image/</guid>
      <description>Create images using the external path for ISO image. In user data mention the below to access the vm. Create the 3 vms and wait for vm to start. Expected Results 3 vm should come up and start with same config. Observe the time taken for the system to start the vms. Observe the pattern of the vms get allocated on the nodes. Like how many vm on each nodes are created.</description>
    </item>
    
    <item>
      <title>Create new network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/create-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/create-network/</guid>
      <description> Navigate to the networks page in harvester Click Create Add a name Add a VLAN ID Click Create Expected Results You should be able to add the VLAN You should see the VLAN show up in the networks page </description>
    </item>
    
    <item>
      <title>Create new VM with a machine type of PC (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-pc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-pc/</guid>
      <description> Set up the VM with the appropriate machine type Save/create Expected Results Machine should start sucessfully Machine should show the new machine type in the config and in the YAML </description>
    </item>
    
    <item>
      <title>Create new VM with a machine type of q35 (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-q35/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-q35/</guid>
      <description> Set up the VM with the appropriate machine type Save/create Expected Results Machine should start sucessfully Machine should show the new machine type in the config and in the YAML </description>
    </item>
    
    <item>
      <title>Create one VM on a VLAN and then move another VM to that VLAN</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-and-then-move-another-vm-to-that-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-and-then-move-another-vm-to-that-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should be able to connect on network This can be verified with a ping over the IP, or via other options if ICMP is disabled </description>
    </item>
    
    <item>
      <title>Create one VM on a VLAN that has other VMs then change it to a different VLAN</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-that-has-other-vms-then-change-it-to-a-different-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-that-has-other-vms-then-change-it-to-a-different-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should NOT be able to connect on network verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server </description>
    </item>
    
    <item>
      <title>Create RKE2 cluster with no cloud provider</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1577-create-rke2-cluster-no-cloud-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1577-create-rke2-cluster-no-cloud-provider/</guid>
      <description> Related issues: #1577 Option to disable load balancer feature in cloud provider Verification Steps Click Cluster Management Click Cloud Credentials Click createa and select Harvester Input credential name Select existing cluster in the Imprted Cluster list Click Create Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Select None for cloud provider Click Create Wait for RKE2 cluster provisioning complete (~20min) Expected Results Provision RKE2 cluster successfully with Running status Can acccess RKE2 cluster to check all resources and services by clicking manage </description>
    </item>
    
    <item>
      <title>Create Single instances of the vm with ISO image</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-pc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-pc/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the vm and wait for vm to start. Expected Results VM should come up and start with same config. </description>
    </item>
    
    <item>
      <title>Create Single instances of the vm with ISO image	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the vm and wait for vm to start. Expected Results VM should come up and start with same config. </description>
    </item>
    
    <item>
      <title>Create Single instances of the vm with ISO image with machine type pc</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-q35/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-q35/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the vm and wait for vm to start. Expected Results VM should come up and start with same config. </description>
    </item>
    
    <item>
      <title>Create Single instances of the vm with raw image	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-raw-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-raw-image/</guid>
      <description> Create vm using the external path for cloud image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the vm and wait for vm to start. Expected Results VM should come up and start with same config. </description>
    </item>
    
    <item>
      <title>Create Single instances of the vm with Windows Image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-windows-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-windows-image/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm. #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Create the vm and wait for vm to start. Expected Results VM should come up and start with same config. </description>
    </item>
    
    <item>
      <title>Create SSH key from templates page</title>
      <link>https://harvester.github.io/tests/manual/authentication/1619-create-ssh-key-from-templates-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/1619-create-ssh-key-from-templates-page/</guid>
      <description> Related issues: #1619 User is unable to create ssh key through the templates page Verification Steps on a harvester deployment, navigate to advanced -&amp;gt; templates and click create Click create new under SSH section enter valid credentials and save Expected Results SSH key should be created and show in the SSH key section </description>
    </item>
    
    <item>
      <title>Create SSH key from templates page</title>
      <link>https://harvester.github.io/tests/manual/templates/1619-create-ssh-key-from-templates-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/templates/1619-create-ssh-key-from-templates-page/</guid>
      <description> Related issues: #1619 User is unable to create ssh key through the templates page Verification Steps on a harvester deployment, navigate to advanced -&amp;gt; templates and click create Click create new under SSH section enter valid credentials and save Expected Results SSH key should be created and show in the SSH key section </description>
    </item>
    
    <item>
      <title>Create support bundle in multi-node Harvester cluster with one node off</title>
      <link>https://harvester.github.io/tests/manual/misc/1524-create-support-bundle-with-one-node-off/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/misc/1524-create-support-bundle-with-one-node-off/</guid>
      <description> Related issues: #1524 Can&amp;rsquo;t create support bundle if one node is off Verification Steps On a multi-node harvester cluster power off one node Navigate to support create support bundle Expected Results Support bundle should create and be downloaded YOu should be able to extract and examine support bundle </description>
    </item>
    
    <item>
      <title>Create two VMs in the same VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-in-the-same-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-in-the-same-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should be able to connect on network This can be verified with a ping over the IP, or via other options if ICMP is disabled </description>
    </item>
    
    <item>
      <title>Create two VMs on separate VLANs</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-separate-vlans/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-separate-vlans/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should NOT be able to connect on network verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server </description>
    </item>
    
    <item>
      <title>Create two VMs on the same VLAN and change one</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-the-same-vlan-and-change-one/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-the-same-vlan-and-change-one/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate Expected Results VM should create successfully Appropriate VLAN should show In config in YAML VMs should NOT be able to connect on network verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server </description>
    </item>
    
    <item>
      <title>Create VM and add SSH key (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-and-add-ssh-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-and-add-ssh-key/</guid>
      <description> Create VM Add SSH key if not already in VM Logon with SSH Expected Results You should be prompted for SSH key passphrase if appropriate You should connect You should be able to execute shell commands The SSH Key should show in the SSH key list </description>
    </item>
    
    <item>
      <title>Create vm using a template of default version</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version/</guid>
      <description> Create a new VM with a template of default version Expected Results After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected </description>
    </item>
    
    <item>
      <title>Create vm using a template of default version with machine type pc</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-pc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-pc/</guid>
      <description> Create a new VM with a template of default version Expected Results After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected </description>
    </item>
    
    <item>
      <title>Create vm using a template of default version with machine type q35</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-q35/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-q35/</guid>
      <description> Create a new VM with a template of default version Expected Results After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected </description>
    </item>
    
    <item>
      <title>Create vm using a template of non-default version</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-non-default-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-non-default-version/</guid>
      <description> Create a new VM with a template of non-default version Expected Results After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected </description>
    </item>
    
    <item>
      <title>Create vm with both CPU and Memory not in cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-both-cpu-and-memory-not-in-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-both-cpu-and-memory-not-in-cluster/</guid>
      <description> Attempt to create a VM with the appropriate resources Expected Results You should get errors for each resource you over provisioned The VM should not create until errors are resolved </description>
    </item>
    
    <item>
      <title>Create vm with CPU not in cluster. (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-cpu-not-in-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-cpu-not-in-cluster/</guid>
      <description> Attempt to create a VM with the appropriate resources Expected Results You should get errors for each resource you over provisioned The VM should not create until errors are resolved </description>
    </item>
    
    <item>
      <title>Create VM with existing Volume (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/</guid>
      <description> Create VM with an existing volume Expected Results VM should create and start You should be able to open the console for the VM and see it boot Volume should show in volumes list VM should appear to the &amp;ldquo;Attached VM&amp;rdquo; column of the existing volume </description>
    </item>
    
    <item>
      <title>Create vm with Memory not in cluster. (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-memory-not-in-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-memory-not-in-cluster/</guid>
      <description> Attempt to create a VM with the appropriate resources Expected Results You should get errors for each resource you over provisioned The VM should not create until errors are resolved </description>
    </item>
    
    <item>
      <title>Create VM with resources that are only on one node in cluster CPU</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</guid>
      <description> Edit a VM with resources that are only available on one node in cluster. Expected Results VM should save VM should be reassigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    
    <item>
      <title>Create VM with resources that are only on one node in cluster CPU (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</guid>
      <description> Create a VM with resources that are only available on one node in cluster Expected Results VM should create VM should be assigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    
    <item>
      <title>Create VM with resources that are only on one node in cluster CPU and Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</guid>
      <description> Create a VM with resources that are only available on one node in cluster Expected Results VM should create VM should be assigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    
    <item>
      <title>Create VM with resources that are only on one node in cluster Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</guid>
      <description> Edit a VM with resources that are only available on one node in cluster. Expected Results VM should save VM should be reassigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    
    <item>
      <title>Create VM with resources that are only on one node in cluster Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</guid>
      <description> Create a VM with resources that are only available on one node in cluster Expected Results VM should create VM should be assigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    
    <item>
      <title>Create VM with saved SSH key (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-saved-ssh-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-saved-ssh-key/</guid>
      <description> Create VM Add SSH key if not already in VM Logon with SSH Expected Results You should be prompted for SSH key passphrase if appropriate You should connect You should be able to execute shell commands The SSH Key should show in the SSH key list </description>
    </item>
    
    <item>
      <title>Create VM with the default network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-the-default-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-the-default-network/</guid>
      <description> Create a VM with the default network Let VM boot up after creation Expected Results VM should start VM should be able to ping other machines in the VLAN VM should be able to ping servers on the internet if the VLAN has external access </description>
    </item>
    
    <item>
      <title>Create VM with two disk volumes (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/</guid>
      <description> Create a VM with the appropriate number of volumes Expected Results Verify after creation that the appropriate volumes are in the config for the VM Verify that the volumes are created and listed in the volumes section </description>
    </item>
    
    <item>
      <title>Create VM without memory provided</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-without-memory-provided/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-without-memory-provided/</guid>
      <description>Related issues: #1477 intimidating error message when missing mandatory field Category: Virtual Machine Verification Steps Create some image and volume Create virtual machine Fill out all mandatory field but leave memory blank. Click create Expected Results Leave empty memory field empty when create virtual machine will show &amp;ldquo;Memory is required&amp;rdquo; error message</description>
    </item>
    
    <item>
      <title>Create Volume root disk blank Form with label</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-blank-form-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-blank-form-label/</guid>
      <description> Navigate to volumes page Click Create Don&amp;rsquo;t select an image Input a size Click Create Expected Results Page should load Volume should create successfully and go to succeeded in the list The label can be seen when you edit the volume config </description>
    </item>
    
    <item>
      <title>Create volume root disk VM Image Form</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form/</guid>
      <description> Navigate to volumes page Click Create Select an image Input a size Click Create Expected Results VM should create VM should pass health checks </description>
    </item>
    
    <item>
      <title>Create volume root disk VM Image Form with label (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form-label/</guid>
      <description> Navigate to volumes page Click Create Select an image Input a size Click Create Expected Results Page should load Volume should create successfully and go to succeeded in the list The label can be seen when you edit the volume config </description>
    </item>
    
    <item>
      <title>Create Windows VM</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-windows-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-windows-vm/</guid>
      <description>Create a VM with the VM template with windows-iso-image-base-temp Config the CPU and Memory to 4 and 8 respectively Select the windows ISO image Click the Volumes tab and update the root disk size to 50GB Click create to launch the windows VM Optional: you can increase the second disk size or add an additional one. Click create to launch the VM (this will take a couple of minutes upon your network speed of download the ISO image) Click the Console to launch a VNC console of the windows server, and you will need to find an evaluation key of the windows server 2012 installation.</description>
    </item>
    
    <item>
      <title>Create with invalid image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/negative-create-with-invalid-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/negative-create-with-invalid-image/</guid>
      <description> Create image with invalid URL. e.g. - https://test.img Expected Results Image state show as Failed </description>
    </item>
    
    <item>
      <title>datavolumes.cdi.kubevirt.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/datavolumes.cdi.kubevirt.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/datavolumes.cdi.kubevirt.io/</guid>
      <description>GUI Create a VM in GUI and wait until it&amp;rsquo;s running. Assume its name is test. kube-api Try to delete its datavolume: $ kubectl get vms NAME AGE STATUS READY test 5m16s Running True There should be an datavolume bound to that VM $ kubectl get dvs NAME PHASE PROGRESS RESTARTS AGE test-disk-0-klrft Succeeded 100.0% 5m18s The user should not be able to delete the datavolume $ kubectl delete dv test-disk-0-klrft The request is invalid: : can not delete the volume test-disk-0-klrft which is currently attached to VMs: default/test `` ## Expected Results ### kube-api The deletion of its datavolume should fail.</description>
    </item>
    
    <item>
      <title>Deactivate/activate/delete Harvester Node Driver</title>
      <link>https://harvester.github.io/tests/manual/node-driver/deactivate-activate-deletenode-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/deactivate-activate-deletenode-driver/</guid>
      <description>With Rancher &amp;lt; 2.6:
Tools-&amp;gt;Driver Managementâ†’Node Driver Deactivate/activate/delete Harvester Node Driver With Rancher 2.6: Cluster Management &amp;gt; Drivers &amp;gt; Node Drivers Deactivate/activate/delete Harvester Node Driver Expected Results Harvester icon is not visible when creating a cluster / Harvester icon is visible when creating a cluster /Harvester icon is not visible when creating a cluster </description>
    </item>
    
    <item>
      <title>Delete 3 node RKE2 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1311-delete-3-node-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1311-delete-3-node-rke2-cluster/</guid>
      <description> Related issues: #1311 Deleting a cluster in rancher dashboard doesn&amp;rsquo;t fully remove Verification Steps Create 3 node RKE2 cluster on Harvester through node driver with Rancher Wait fo the nodes to create, but not fully provision Delete the cluster Wait for them to be removed from Harvester Check Rancher cluster management Expected Results Cluster should be removed from Rancher VMs should be removed from Harvester </description>
    </item>
    
    <item>
      <title>Delete backup from backups list (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-single-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-single-backup/</guid>
      <description> Delete backup from backups list Expected Results Backup should be removed from list Backup should be removed from remote storage </description>
    </item>
    
    <item>
      <title>Delete Cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-delete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-delete/</guid>
      <description> Delete Cluster Expected Results successful cluster deletion in rancher the corresponding VM node in harvester is deleted successfully </description>
    </item>
    
    <item>
      <title>Delete external VLAN network via form</title>
      <link>https://harvester.github.io/tests/manual/network/delete-vlan-network-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/delete-vlan-network-form/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the external VLAN via the web form Validate interface was removed with ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the external VLAN You should get responses from the VM </description>
    </item>
    
    <item>
      <title>Delete external VLAN network via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/delete-vlan-network-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/delete-vlan-network-yaml/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the external VLAN via YAML Validate interface was removed with ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the external VLAN You should get responses from the VM </description>
    </item>
    
    <item>
      <title>Delete first backup in chained backup (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-first-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-first-backup-chained-backup/</guid>
      <description> Create a new VM Create a file named 1 and add text Create a backup Edit text in file 1 create file 2 Create Backup Edit file 2 text Create file 3 and add text Create backup Delete backup 1 Validate file 2 and 3 are the same as they were Restore to backup 2 Validate that md5sum -c file1-2.md5 file2.md5 file3.md5 file 1 is in second format file 2 is in first format file 3 doesn&amp;rsquo;t exist Expected Results Vm should create All file operations should create Backup should run All file operations should create Backup should run All file operations should create files should be as expected </description>
    </item>
    
    <item>
      <title>Delete Host (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/delete-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/delete-host/</guid>
      <description> Navigate to the Hosts page and select the node Click Delete Expected Results SSH to the node and check the nodes has components deleted. </description>
    </item>
    
    <item>
      <title>Delete host that has VMs on it</title>
      <link>https://harvester.github.io/tests/manual/hosts/delete-host-with-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/delete-host-with-vm/</guid>
      <description>Navigate to the Hosts page and select the node Click Delete Expected Results An alert message should appear. If VM exists it should stop user to delete the node or move VM to other node. If VM is getting moved to another node and there is no space, it should stop user to delete the node. Existing bugs https://github.com/harvester/harvester/issues/1004</description>
    </item>
    
    <item>
      <title>Delete last backup in chained backup (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-last-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-last-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup delete backup 3 Validate that files didn&amp;rsquo;t change Restore to backup 2 Validate that md5sum -c file1-2.</description>
    </item>
    
    <item>
      <title>Delete management network via form</title>
      <link>https://harvester.github.io/tests/manual/network/delete-management-network-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/delete-management-network-form/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the management VLAN via the web form Validate interface was removed with ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the management VLAN You should get responses from the VM </description>
    </item>
    
    <item>
      <title>Delete management network via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/delete-management-network-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/delete-management-network-yaml/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the management network via YAML Validate interface was removed with ip link list Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine Expected Results The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the management network You should get responses from the VM </description>
    </item>
    
    <item>
      <title>Delete middle backup in chained backup (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-middle-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-middle-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Delete backup 2 Validate file 2 and 3 are the same as they were Restore to backup 1 Validate that md5sum -c file1.</description>
    </item>
    
    <item>
      <title>Delete multiple backups</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-multiple-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-multiple-backups/</guid>
      <description> Select multiple Backups from Backups list Click Delete Expected Results Backups should be removed from list Backups should be removed from remote storage </description>
    </item>
    
    <item>
      <title>Delete multiple VMs with disks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-with-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-with-disks/</guid>
      <description> Delete VM Select whether you want to delete disks Expected Results You should check amount of used space on Server before you delete the VM Machine should delete It should not show up in the Virtual Machine list Disks should be listed/or not in Volumes list as appropriate Verify the cleaned up the space on the disk on the node. </description>
    </item>
    
    <item>
      <title>Delete multiple VMs without disks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-without-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-without-disks/</guid>
      <description> Delete VM Select whether you want to delete disks Expected Results You should check amount of used space on Server before you delete the VM Machine should delete It should not show up in the Virtual Machine list Disks should be listed/or not in Volumes list as appropriate Verify the cleaned up the space on the disk on the node. </description>
    </item>
    
    <item>
      <title>Delete single vm all disks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/delete-single-vm-all-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/delete-single-vm-all-disks/</guid>
      <description> Delete VM Select whether you want to delete disks Expected Results You should check amount of used space on Server before you delete the VM Machine should delete It should not show up in the Virtual Machine list Disks should be listed/or not in Volumes list as appropriate Verify the cleaned up the space on the disk on the node. </description>
    </item>
    
    <item>
      <title>Delete the image	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/delete-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/delete-image/</guid>
      <description> Select an image with state active. Delete the image. Create another image with same name. Delete the newly created image. Delete an image with failed state Expected Results The image should be deleted successfully. Check the CRDS VirtualMachineImage. User should be able to create a new image with same name. Check the backing image in Longhorn. </description>
    </item>
    
    <item>
      <title>Delete VM Negative (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-delete-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-delete-vm/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Delete VM and all disks Expected Results You should not be able to delete the VM </description>
    </item>
    
    <item>
      <title>Delete VM with exported image</title>
      <link>https://harvester.github.io/tests/manual/images/1602-delete-vm-with-exported-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/1602-delete-vm-with-exported-image/</guid>
      <description> Related issues: #1602 exported image can&amp;rsquo;t be deleted after vm removed Verification Steps create vm &amp;ldquo;vm-1&amp;rdquo; create a image &amp;ldquo;img-1&amp;rdquo; by export the volume used by vm &amp;ldquo;vm-1&amp;rdquo; delete vm &amp;ldquo;vm-1&amp;rdquo; delete image &amp;ldquo;img-1&amp;rdquo; Expected Results image &amp;ldquo;img-1&amp;rdquo; will be deleted </description>
    </item>
    
    <item>
      <title>Delete VM with exported image</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/1602-delete-vm-with-exported-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/1602-delete-vm-with-exported-image/</guid>
      <description> Related issues: #1602 exported image can&amp;rsquo;t be deleted after vm removed Verification Steps create vm &amp;ldquo;vm-1&amp;rdquo; create a image &amp;ldquo;img-1&amp;rdquo; by export the volume used by vm &amp;ldquo;vm-1&amp;rdquo; delete vm &amp;ldquo;vm-1&amp;rdquo; delete image &amp;ldquo;img-1&amp;rdquo; Expected Results image &amp;ldquo;img-1&amp;rdquo; will be deleted </description>
    </item>
    
    <item>
      <title>Delete volume that is not attached to a VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/delete-volume-that-is-not-attached-to-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/delete-volume-that-is-not-attached-to-vm/</guid>
      <description> Create volume Validate that it created Check the volume crd. Delete the volume Verify that volume is removed from list Check the volume object doesn&amp;rsquo;t exist anymore. Expected Results Volume should create It should show in volume list Volume crd should have correct info. Volume should delete. Volume should be removed from list </description>
    </item>
    
    <item>
      <title>Delete volume that was attached to VM but now is not (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/delete-volume-that-was-attached-to-vm-but-is-not-now/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/delete-volume-that-was-attached-to-vm-but-is-not-now/</guid>
      <description> Create a VM with a root volume Write 10Gi data into it. Delete the VM but not the volume Verify Volume still exists Check disk space on node Delete the volume Verify that volume is removed from list Check disk space on node Expected Results VM should create 10Gi space should be consumed on the disk. VM should delete Volume should still show in Volume list Disk space should show 10Gi + Volume should delete Volume should be removed from list Space should be less than before </description>
    </item>
    
    <item>
      <title>Detach volume from virtual machine</title>
      <link>https://harvester.github.io/tests/manual/volumes/detach-volume-from-virtual-machine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/detach-volume-from-virtual-machine/</guid>
      <description>Related issues: #1708 After click &amp;ldquo;Detach volume&amp;rdquo; button, nothing happend Category: Volume Verification Steps Create several new volume in volumes page Create a virtual machine Click the config button on the selected virtual machine Click Add volume and add at least two new volume Click the Detach volume button on the attached volume Repeat above steps several times Expected Results Currently when click the Detach volume button, attached volume can be detach successfully.</description>
    </item>
    
    <item>
      <title>Disable and enable vlan cluster network</title>
      <link>https://harvester.github.io/tests/manual/network/disable-and-enable-vlan-cluster-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/disable-and-enable-vlan-cluster-network/</guid>
      <description> Related issues: #1529 Failed to enable vlan cluster network after disable and enable again, display &amp;ldquo;Network Error&amp;rdquo; Category: Network Verification Steps Open settings and config vlan network Enable network and set default harvester-mgmt Disable network Enable network again Check Host, Network and harvester dashboard Repeat above steps several times Expected Results User can disable and enable network with default harvester-mgmt. Harvester dashboard and network work as expected </description>
    </item>
    
    <item>
      <title>Disk can only be added once on UI</title>
      <link>https://harvester.github.io/tests/manual/hosts/add_disk_on_ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/add_disk_on_ui/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1608
Verify Items NVMe disk can only be added once on UI Case: add new NVMe disk on dashboard UI Install Harvester with 2 nodes Power off 2nd node Update VM&amp;rsquo;s xml definition (by using virsh edit or virt-manager) Create nvme.img block: dd if=/dev/zero of=/var/lib/libvirt/images/nvme.img bs=1M count=4096 change owner chown qemu:qemu /var/lib/libvirt/images/nvme.img update &amp;lt;domain type=&amp;quot;kvm&amp;quot;&amp;gt; to &amp;lt;domain type=&amp;quot;kvm&amp;quot; xmlns:qemu=&amp;quot;http://libvirt.org/schemas/domain/qemu/1.0&amp;quot;&amp;gt; append xml node into domain as below: &amp;lt;qemu:commandline&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;file=/var/lib/libvirt/images/nvme.</description>
    </item>
    
    <item>
      <title>Disk devices used for VM storage should be globally configurable</title>
      <link>https://harvester.github.io/tests/manual/hosts/disk-devices-used-for-vm-storage-globally-configurable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/disk-devices-used-for-vm-storage-globally-configurable/</guid>
      <description>Related issue: #1241 Disk devices used for VM storage should be globally configurable
Related issue: #1382 Exclude OS root disk and partitions on forced GPT partition
Related issue: #1599 Extra disk auto provision from installation may cause NDM can&amp;rsquo;t find a valid longhorn node to provision
Category: Storage Test Scenarios (Checked means verification PASS)
BIOS firmware + No MBR (Default) + Auto disk` provisioning config BIOS firmware + MBR + Auto disk provisioning config UEFI firmware + GPT (Default) + Auto disk provisioning config BIOS firmware + GPT (Default) +Auto Provisioning on harvester-config Environment setup Scenario 1: Node type: Create</description>
    </item>
    
    <item>
      <title>Download host YAML</title>
      <link>https://harvester.github.io/tests/manual/hosts/download-host-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/download-host-yaml/</guid>
      <description> Navigate to the Hosts page and select the node Click Download Yaml Expected Results The Yaml should get downloaded. </description>
    </item>
    
    <item>
      <title>Download kubeconfig after shutting down harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/misc/download-kubeconfig-after-shutting-down-harvester-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/misc/download-kubeconfig-after-shutting-down-harvester-cluster/</guid>
      <description>Related issues: #1475 After shutting down the cluster the kubeconfig becomes invalid Category: Host Verification Steps Shutdown harvester node 3, wait for fully power off
Shutdown harvester node 2, wait for fully power off
Shutdown harvester node 1, wait for fully power off
Wait for more than hours or over night
Power on node 1 to console page until you see management url Power on node 2 to console page until you see management url</description>
    </item>
    
    <item>
      <title>Edit a VM and add install Enable usb tablet option (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-enable-usb-tablet-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-enable-usb-tablet-option/</guid>
      <description> Add Enable usb tablet Option Save/Create VM Expected Results Machine starts successfully Enable usb tablet shows In YAML In Form </description>
    </item>
    
    <item>
      <title>Edit a VM and add install guest agent option (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-guest-agent-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-guest-agent-option/</guid>
      <description> Add install Guest Agent Option Save/Create VM Expected Results Machine starts successfully Guest Agent Option shows In YAML In Form Guest Agent is installed </description>
    </item>
    
    <item>
      <title>Edit a VM from the form to add Network Data</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-network-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-network-data/</guid>
      <description> Add Network Data to the VM Here is an example of Network Data config to add DHCP to the physical interface eth0 network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp Save/Create the VM Expected Results Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0 </description>
    </item>
    
    <item>
      <title>Edit a VM from the form to add user data</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-user-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-user-data/</guid>
      <description> Add User data to the VM Here is an example of user data config to add a password `` #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Save/Create the VM Expected Results Machine starts succesfully User data should In YAML In Form Machine should have user password set </description>
    </item>
    
    <item>
      <title>Edit a VM from the YAML to add Network Data (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-network-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-network-data/</guid>
      <description> Add Network Data to the VM Here is an example of Network Data config to add DHCP to the physical interface eth0 network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp Save/Create the VM Expected Results Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0 </description>
    </item>
    
    <item>
      <title>Edit a VM from the YAML to add user data (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-user-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-user-data/</guid>
      <description> Add User data to the VM Here is an example of user data config to add a password `` #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Save/Create the VM Expected Results Machine starts succesfully User data should In YAML In Form Machine should have user password set </description>
    </item>
    
    <item>
      <title>Edit an existing VM to another machine type (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-an-existing-vm-to-another-machine-type/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-an-existing-vm-to-another-machine-type/</guid>
      <description> Set up the VM with the appropriate machine type Save/create Expected Results Machine should start sucessfully Machine should show the new machine type in the config and in the YAML </description>
    </item>
    
    <item>
      <title>Edit backup read YAML from file</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-read-yaml-from-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-read-yaml-from-file/</guid>
      <description> Edit YAML for backup Read from File Show Diff Save Expected Results Diff should show changes Backup should be updated </description>
    </item>
    
    <item>
      <title>Edit backup via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-yaml/</guid>
      <description> Edit YAML for backup Show Diff Save Expected Results Diff should show changes Backup should be updated </description>
    </item>
    
    <item>
      <title>Edit Config (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/edit-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/edit-config/</guid>
      <description> Navigate to the Hosts page and select the node Click edit config. Add description and other details Try to modify the network config Expected Results The edited values should be saved and reflected on the page. </description>
    </item>
    
    <item>
      <title>Edit Config YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/edit-config-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/edit-config-yaml/</guid>
      <description> Navigate to the Hosts page and select the node Click edit config through YAML. Add description and other details Try to modify the network config Expected Results The edited values should be saved and reflected on the page. </description>
    </item>
    
    <item>
      <title>Edit images (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/edit-images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/edit-images/</guid>
      <description> Edit image. Try to edit the description Try to edit the URL Try to edit the Labels Expected Results User should be able to edit the description and Labels User should not be able to edit the URL </description>
    </item>
    
    <item>
      <title>Edit network via form change external VLAN to management network</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-form-change-vlan-to-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/edit-network-form-change-vlan-to-management/</guid>
      <description> Edit VM and change external VLAN to management network with bridge type via the web form Ping VM Attempt to SSH to VM Expected Results VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM </description>
    </item>
    
    <item>
      <title>Edit network via form change management network to external VLAN</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/</guid>
      <description> Edit VM and change management network to external VLAN with bridge type via the web form Ping VM Attempt to SSH to VM Expected Results VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM </description>
    </item>
    
    <item>
      <title>Edit network via YAML change external VLAN to management network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-vlan-to-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-vlan-to-management/</guid>
      <description> Edit VM and change external VLAN to management network with bridge type via YAML Ping VM Attempt to SSH to VM Expected Results VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM </description>
    </item>
    
    <item>
      <title>Edit network via YAML change management network to external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-management-to-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-management-to-vlan/</guid>
      <description> Edit VM and change management network to external VLAN with bridge type via YAML Ping VM Attempt to SSH to VM Expected Results VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM </description>
    </item>
    
    <item>
      <title>Edit vm and insert ssh and check the ssh key is accepted for the login (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-and-insert-ssh-and-check-the-ssh-key-is-accepted-for-the-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-and-insert-ssh-and-check-the-ssh-key-is-accepted-for-the-login/</guid>
      <description> Edit VM and add SSH Key Save VM Expected Results You should be able to ssh in with correct SSH private key You should not be able to SSH in with incorrect SSH private key </description>
    </item>
    
    <item>
      <title>Edit VM Form Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-edit-vm-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-edit-vm-form/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Edit the VM via form Save the VM Expected Results You should not be able to save the edited Form You should get an error </description>
    </item>
    
    <item>
      <title>Edit vm network and verify the network is working as per configuration (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-network-and-verify-the-network-is-working-as-per-configuration-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-network-and-verify-the-network-is-working-as-per-configuration-/</guid>
      <description> Edit VM network Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML Network should function as desired </description>
    </item>
    
    <item>
      <title>Edit VM via form with CPU</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    
    <item>
      <title>Edit VM via form with CPU and Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu-and-memory/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    
    <item>
      <title>Edit VM via form with Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-memory/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    
    <item>
      <title>Edit VM via YAML with CPU (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    
    <item>
      <title>Edit VM via YAML with CPU and Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu-and-memory/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    
    <item>
      <title>Edit VM via YAML with Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-memory/</guid>
      <description> Edit VM Save Expected Results VM should save VM should restart if restart checkbox is checked Changes should show In Form In YAML In VM list </description>
    </item>
    
    <item>
      <title>Edit VM with resources that are only on one node in cluster CPU and Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</guid>
      <description> Edit a VM with resources that are only available on one node in cluster. Expected Results VM should save VM should be reassigned to node that has available resources VM should boot VM should pass health checks </description>
    </item>
    
    <item>
      <title>Edit VM YAML Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/q-negative-edit-vm-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/q-negative-edit-vm-yaml/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Edit the VM via YAML Save the VM Expected Results SSH to the node and check the nodes has components deleted. </description>
    </item>
    
    <item>
      <title>Edit volume decrease size via YAML</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-decrase-size-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-decrase-size-yaml/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume as YAML Decrease size Click Save Connect to VM via console Check size of root disk Expected Results VM should stop VM should reboot after saving Disk should be resized </description>
    </item>
    
    <item>
      <title>Edit volume decrease size via YAML</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-decrease-size-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-decrease-size-form/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume as YAML Decrease size Click Save Connect to VM via console Check size of root disk Expected Results VM should stop VM should reboot after saving Disk should be resized </description>
    </item>
    
    <item>
      <title>Edit Volume Form add label</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-form-add-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-form-add-label/</guid>
      <description> Navigate to volumes page Edit Volume with Form Click Labels Add label Click Save Open VM again and click the config tab Verify that label was saved Expected Results Volume should save Label should add Label should show when re-opened </description>
    </item>
    
    <item>
      <title>Edit volume increase size via form</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-form/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume via form Increase size Click Save Connect to VM via console Check size of root disk Expected Results VM should stop VM should reboot after saving Disk should be resized </description>
    </item>
    
    <item>
      <title>Edit volume increase size via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-yaml/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume as YAML Increase size Click Save Connect to VM via console Check size of root disk Expected Results VM should stop VM should reboot after saving Disk should be resized </description>
    </item>
    
    <item>
      <title>Edit Volume YAML add label (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-yaml-add-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-yaml-add-label/</guid>
      <description> Navigate to volumes page Edit Volume as YAML Add label to config Click Save Open VM again and click the config tab Verify that label was saved Expected Results Volume should save Label should add Label should show when re-opened </description>
    </item>
    
    <item>
      <title>Enabling vlan on a bonded NIC on vagrant install</title>
      <link>https://harvester.github.io/tests/manual/network/enabling-vlan-on-bonded-nic-vagrant-install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/enabling-vlan-on-bonded-nic-vagrant-install/</guid>
      <description>Related issues: #1541 Enabling vlan on a bonded NIC breaks the Harvester setup Category: Network Verification Steps Pull ipxe example from https://github.com/harvester/ipxe-examples Vagrant pxe install 3 nodes harvester Access harvester settings page Open settings -&amp;gt; vlan Enable virtual network and set with bond0 Navigate to every page to check harvester is working Create a vlan based on bon0 Expected Results Enable virtual network with bond0 will not make harvester service out of work.</description>
    </item>
    
    <item>
      <title>Filter backups</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/filter-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/filter-backups/</guid>
      <description> Enter in string in filter input field Columns available for matching: State &amp;ldquo;Ready&amp;rdquo; &amp;ldquo;Progressing&amp;rdquo; Name Target VM With string With matching string Input Clear With non-matching string Input Clear Clear String Expected Results List should filter based on string List should re-populate after clearing string </description>
    </item>
    
    <item>
      <title>First Time Login</title>
      <link>https://harvester.github.io/tests/manual/authentication/first-time-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/first-time-login/</guid>
      <description> After successful installation of Harvester using Iso, on navigating to UI, user should be prompted to change the password. Verify the password rules Expected Results User should be able to login </description>
    </item>
    
    <item>
      <title>Generate Install Support Config Bundle For Single Node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1864-generate-install-support-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1864-generate-install-support-config/</guid>
      <description>Related issue: #1864 Support bundle for a single node (Live/Installed)
Related issue: #272 Generate supportconfig for failed installations
Category: Support Environment setup Setup a single node harvester from ISO install but don&amp;rsquo;t complete the installation
Gain SSH Access to the Single Harvester Node Once Shelled into the Single Harvester Node edit the /usr/sbin/harv-install Using: harvester-installer&amp;rsquo;s harv-install as a reference edit around line #362 adding exit 1: exit 1 trap cleanup exit check_iso save the file.</description>
    </item>
    
    <item>
      <title>Guest CSI Driver</title>
      <link>https://harvester.github.io/tests/manual/node-driver/guest-csi-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/guest-csi-driver/</guid>
      <description>Start rancher using docker in a vm and start harvester in another Import harvester into rancher from &amp;ldquo;Virtualization Management&amp;rdquo; page On rancher, enable harvester node driver at &amp;ldquo;Cluster Management&amp;rdquo; -&amp;gt; &amp;ldquo;Drivers&amp;rdquo; -&amp;gt; &amp;ldquo;Node Driver&amp;rdquo; Go back to &amp;ldquo;Cluster Management&amp;rdquo; and create a rke2 cluster using Harvester Once the created cluster is active on the &amp;ldquo;Cluster Management&amp;rdquo; page, click on the &amp;ldquo;Explore&amp;rdquo; Go to &amp;ldquo;Workload&amp;rdquo; -&amp;gt; &amp;ldquo;Deployment&amp;rdquo; and &amp;ldquo;Create&amp;rdquo; a new deployment, during which in the page of &amp;ldquo;Storage&amp;rdquo;, click on &amp;ldquo;Add Volume&amp;rdquo; and select &amp;ldquo;Create Persistent Volume Claim&amp;rdquo; and select &amp;ldquo;Harvester&amp;rdquo; in the &amp;ldquo;Storage Class&amp;rdquo; Click &amp;ldquo;Create&amp;rdquo; to create the deployment Verify that on the Harvester side, a new volume is created.</description>
    </item>
    
    <item>
      <title>Host list should display the disk error message on failure</title>
      <link>https://harvester.github.io/tests/manual/hosts/host-list-should-display-disk-error-message-on-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/host-list-should-display-disk-error-message-on-failure/</guid>
      <description> Related issue: #1167 Host list should display the disk error message on table Category: Storage Verification Steps Shutdown existing node vm machine Run &amp;ldquo;qemu-img create&amp;rdquo; command to make a nvme.img Edit quem/kvm xml setting to attach the nvme image Start VM Open hostpage and edit your target node config Add the new nvme disk Shutdown VM Remove the attach device setting in Vï¼­ xml file Start VM Open Host page, the targe node will show warning with unready and unscheduable disk exists Expected Results If host encounter disk ready or schedule failure, on host page the &amp;ldquo;disk state&amp;rdquo; will show warning With a hover tip &amp;ldquo;Host have unready or unschedulable disks&amp;rdquo; Can create load balancer correctly with health check setting </description>
    </item>
    
    <item>
      <title>Http proxy setting on harvester</title>
      <link>https://harvester.github.io/tests/manual/deployment/http-proxy-setting-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/http-proxy-setting-harvester/</guid>
      <description>Related issue: #1218 Missing http proxy settings on rke2 and rancher pod
Related issue: #1012 Failed to create image when deployed in private network environment
Category: Network Environment setup Setup an airgapped harvester
Clone ipxe example repository https://github.com/harvester/ipxe-examples Edit the setting.xml file under vagrant ipxe example Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Verification Steps Open Settings, edit http-proxy with the following values HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.</description>
    </item>
    
    <item>
      <title>Import and make changes to clusternetwork resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-clusternetwork/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-clusternetwork/</guid>
      <description>Import clusternetwork resource terraformer import harvester -r clusternetwork Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: default_physical_nic, enable in the clusternetwork.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r clusternetwork 2021/08/04 15:43:25 harvester importing.</description>
    </item>
    
    <item>
      <title>Import and make changes to image resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-image/</guid>
      <description>Import image resource terraformer import harvester -r image Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: description, display_name, name, namespace and url in the image.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r image 2021/08/04 16:14:52 harvester importing.</description>
    </item>
    
    <item>
      <title>Import and make changes to network resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-network/</guid>
      <description>Import network resource terraformer import harvester -r network Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: name, namespace and vlan_id in the network.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r network 2021/08/04 16:14:08 harvester importing.</description>
    </item>
    
    <item>
      <title>Import and make changes to ssh_key resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-ssh-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-ssh-key/</guid>
      <description>Import ssh_key resource terraformer import harvester -r ssh_key Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: name, namespace and public_key in the ssh_key.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r ssh_key 2021/08/04 16:14:36 harvester importing.</description>
    </item>
    
    <item>
      <title>Import and make changes to virtual machine resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-virtual-machine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-virtual-machine/</guid>
      <description>Import virtual machine resource terraformer import harvester -r virtualmachine Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: cpu, memory, name in the virtualmachine.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r virtualmachine 2021/08/04 16:15:08 harvester importing.</description>
    </item>
    
    <item>
      <title>Import and make changes to volume resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-volume/</guid>
      <description>Import volume resource terraformer import harvester -r volume Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: name, namespace in the volume.tf file Check the change through either the UI or the API Expected Results Import output terraformer import harvester -r volume 2021/08/04 16:15:29 harvester importing.</description>
    </item>
    
    <item>
      <title>Import External Harvester</title>
      <link>https://harvester.github.io/tests/manual/node-driver/import-external-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/import-external-harvester/</guid>
      <description>With Rancher &amp;lt; 2.6:
Deploy the rancher and harvester clusters separately In the rancher, add a harvester node template Select &amp;ldquo;External Harvester&amp;rdquo;, and refer to &amp;ldquo;Test Data&amp;rdquo; for other value settings. Use this template to create the corresponding cluster With Rancher 2.6: Home page / Import Existing / Generic Add cluster name and click on Create Follow the registration steps Expected Results The status of the created cluster shows active The status of the corresponding vm on harvester active The information displayed on rancher and harvester matches the template configuration Test Data Harvester Node Template HARVESTER OPTIONS Account Access External Harvester Host: Port: 443 Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Import internal harvester</title>
      <link>https://harvester.github.io/tests/manual/node-driver/import-internal-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/import-internal-harvester/</guid>
      <description>enable harvester&amp;rsquo;s rancher-enabled setting Click the rancher button in the upper right corner to access the internal rancher add a harvester node template Select &amp;ldquo;Internal Harvester&amp;rdquo;, and refer to &amp;ldquo;Test Data&amp;rdquo; for other value settings. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Initiate multiple migrations at one time</title>
      <link>https://harvester.github.io/tests/manual/live-migration/initiate-multple-migrations-same-time/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/initiate-multple-migrations-same-time/</guid>
      <description> Initiate live migration for a vm. While the live migration is in progress, initiate another migration Expected Results Both migration should work fine. The VMs should be accessible after the migration </description>
    </item>
    
    <item>
      <title>Install 2 node Harvester with a Harvester token with multiple words</title>
      <link>https://harvester.github.io/tests/manual/deployment/812-multiple-word-harvester-token/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/812-multiple-word-harvester-token/</guid>
      <description> Related issues: #812 ISO install accepts multiple words for &amp;lsquo;cluster token&amp;rsquo; value resulting in failure to join cluster Verification Steps Start Harvester install from ISO At the &amp;lsquo;Cluster token&amp;rsquo; prompt, enter, here are words Proceed to complete the installation Boot a secondary host from the installation ISO and select the option to join an existing cluster At the &amp;lsquo;Cluster token&amp;rsquo; prompt, enter, here are words Proceed to complete the installation Verify both hosts show in hosts list at VIP Expected Results Install should complete successfully Host should add with no errors Both hosts should show up </description>
    </item>
    
    <item>
      <title>Install Harvester from USB disk</title>
      <link>https://harvester.github.io/tests/manual/deployment/install_via_usb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/install_via_usb/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1200
Verify Items Harvester can be installed via USB stick Case: Install Harvester via USB disk Follow the instruction to create USB disk Harvester should able to be installed via the USB on UEFI-based bare metals </description>
    </item>
    
    <item>
      <title>Install Harvester on a bare Metal node using ISO image</title>
      <link>https://harvester.github.io/tests/manual/deployment/install-bare-metal-iso/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/install-bare-metal-iso/</guid>
      <description>Install using ISO image https://docs.harvesterhci.io/v0.3/install/iso-install/
Expected Results On completion of the installation, Harvester should provide the management url and show status. Harvester and Longhorn components should be up and running in the cluster. Verify the memory, cpu and storage size shown on the Harvester UI </description>
    </item>
    
    <item>
      <title>Install Harvester on a bare Metal node using ISO image</title>
      <link>https://harvester.github.io/tests/manual/deployment/install-nested-virtualization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/install-nested-virtualization/</guid>
      <description>Install using ISO image https://docs.harvesterhci.io/v0.3/install/iso-install/
Expected Results On completion of the installation, Harvester should provide the management url and show status. Harvester and Longhorn components should be up and running in the cluster. Verify the memory, cpu and storage size shown on the Harvester UI </description>
    </item>
    
    <item>
      <title>Install Harvester on a bare Metal node using PXE boot (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/install-bare-metal-pxe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/install-bare-metal-pxe/</guid>
      <description>Install Harvester using PXE boot https://docs.harvesterhci.io/v0.3/install/pxe-boot-install/
Expected Results On completion of the installation, Harvester should provide the management url and show status. Harvester and Longhorn components should be up and running in the cluster. Verify the memory, cpu and storage size shown on the Harvester UI </description>
    </item>
    
    <item>
      <title>Install Harvester on NVMe SSD</title>
      <link>https://harvester.github.io/tests/manual/deployment/install_on_nvme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/install_on_nvme/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1627
Verify Items Harvester can detect NVMe SSD when installing Harvester can be installed on NVMe SSD Case: Install Harvester on NVMe disk Create block image as NVMe disk Run dd if=/dev/zero of=/var/lib/libvirt/images/nvme145.img bs=1M count=148480 Then Change file owner chown qemu:qemu /var/lib/libvirt/images/nvme145.img Create VM via virt-manager Select Manual install, set Generic OS, Memory:9216, CPUs:8, Uncheck enable storage&amp;hellip; and check customize configuration before install Select Firmware to use UEFI x86_64 (use usr/share/qemu/ovmf-x86_64-code.</description>
    </item>
    
    <item>
      <title>Install Option `HwAddr` for Network Interface</title>
      <link>https://harvester.github.io/tests/manual/deployment/hwaddr_configre_option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/hwaddr_configre_option/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1064
Verify Items Configure Option HwAddr is working on install configuration Case: Use HwAddr to install harvester via PXE Install Harvester with PXE installation, set hwAddr instead of name in install.networks Harvester should installed successfully </description>
    </item>
    
    <item>
      <title>Install Option `install.device` support symbolic link</title>
      <link>https://harvester.github.io/tests/manual/deployment/install_symblic_link/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/install_symblic_link/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1462
Verify Items Disk&amp;rsquo;s symbolic link can be used in install configure option install.device Case: Harvester install with configure symbolic link on install.device Install Harvester with any nodes login to console, use ls -l /dev/disk/by-path to get disk&amp;rsquo;s link name Re-install Harvester with configure file, with set the disk&amp;rsquo;s link name instead. Harvester should be install successfully </description>
    </item>
    
    <item>
      <title>Installation of the Harvester terraform provider (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/install-terraform-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/install-terraform-provider/</guid>
      <description>Follow the instruction of the README
Expected Results The provider is initialized and the terraform init command succeeds:
Initializing provider plugins... - Finding harvester/harvester versions matching &amp;#34;~&amp;gt; 0.1.0&amp;#34;... - Installing harvester/harvester v0.1.0... - Installed harvester/harvester v0.1.0 (unauthenticated) ... Terraform has been successfully initialized! </description>
    </item>
    
    <item>
      <title>keypairs.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/keypairs.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/keypairs.harvesterhci.io/</guid>
      <description>GUI Enable VLAN network in settings Create a network with VLAN 5 and assume its name is my-network. C1. reate another network with VLAN 5: it should fails with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: VLAN ID 5 is already allocated Create a VM on VLAN 5, delete network my-network and it should fail with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: network my-network is still used by vm(s): vm-test in a modal.</description>
    </item>
    
    <item>
      <title>Login after password reset</title>
      <link>https://harvester.github.io/tests/manual/authentication/login-after-password-reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/login-after-password-reset/</guid>
      <description> Enter the wrong credential. Enter the correct credential Expected Results Login should fail. Login should pass </description>
    </item>
    
    <item>
      <title>Logout from the UI and login again</title>
      <link>https://harvester.github.io/tests/manual/authentication/logout-then-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/logout-then-login/</guid>
      <description> Logout from the UI and Log in again Expected Results User should be able to logout/login successfully. </description>
    </item>
    
    <item>
      <title>Maintenance mode for host with multiple VMs</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-multiple-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-multiple-vm/</guid>
      <description> Put host in maintenance mode Migrate VMs Wait for VMs to migrate Wait for any vms to migrate off Do health check on VMs Expected Results Host should start to go into maintenance mode Any VMs should migrate off Host should go into maintenance mode </description>
    </item>
    
    <item>
      <title>Maintenance mode for host with one VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-one-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-one-vm/</guid>
      <description> Put host in maintenance mode Migrate VMs Wait for VMs to migrate Wait for any vms to migrate off Do health check on VMs Expected Results Host should start to go into maintenance mode Any VMs should migrate off Host should go into maintenance mode </description>
    </item>
    
    <item>
      <title>Maintenance mode on node with no vms (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-no-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-no-vm/</guid>
      <description> Put host in maintenance mode Wait for host to go from entering maintenance mode to maintenance mode. Expected Results Host should start to go into maintenance mode Host should go into maintenance mode </description>
    </item>
    
    <item>
      <title>Manual upgrade from 0.3.0 to 1.0.0</title>
      <link>https://harvester.github.io/tests/manual/deployment/manual-upgrade-from-0.3.0-to-1.0.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/manual-upgrade-from-0.3.0-to-1.0.0/</guid>
      <description>Related issues: #1644 Harvester pod crashes after upgrading from v0.3.0 to v1.0.0-rc1 (contain vm backup before upgrade)
Related issues: #1588 VM backup cause harvester pod to crash
Notice We recommend using zero downtime upgrade to upgrade harvester. Manual upgrade is for advance usage and purpose.
Category: Manual Upgrade Verification Steps Download harvester v0.3.0 iso and do checksum Download harvester v1.0.0 iso and do checksum Use ISO Install a 4 nodes harvester cluster Create several OS images from URL Create ssh key Enable vlan network with harvester-mgmt Create virtual network vlan1 with id 1 Create 2 virtual machines ubuntu-vm: 2 core, 4GB memory, 30GB disk Setup backup target Take a backup from ubuntu vm Peform manual upgrade steps in the following docudment upgrade process Follow the manual upgrade steps to upgrade from v0.</description>
    </item>
    
    <item>
      <title>Mark some features as experimental</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1671-mark-experimental-features/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1671-mark-experimental-features/</guid>
      <description> Related issues: #1671 Mark external Harvester cluster provisioning support as experimental Verification Steps Verify that external Harvester is marked as experiemental Verify that Cloud Credentials is marked as experimental Verify that external is marked as experimental in add node template Expected Results All external Harvester fields should be marked as experimental </description>
    </item>
    
    <item>
      <title>Memory overcommit on VM</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/memory_overcommit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/memory_overcommit/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1537
Verify Items Overcommit can be edit on Dashboard VM can allocate exceed Memory on the host Node VM can chage allocated Memory after created Case: Update Overcommit configuration Install Harvester with any Node Login to Dashboard, then navigate to Advanced Settings Edit overcommit-config The field of Memory should be editable Created VM can allocate maximum Memory should be &amp;lt;HostMemory&amp;gt; * [&amp;lt;overcommit-Memory&amp;gt;/100] - &amp;lt;Host Reserved&amp;gt; Case: VM can allocate Memory more than Host have Install Harvester with any Node Create a cloud image for VM Creation Create a VM with &amp;lt;HostMemory&amp;gt; * 1.</description>
    </item>
    
    <item>
      <title>Migrate a turned on VM from one host to another</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/</guid>
      <description> Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    
    <item>
      <title>Migrate a VM created with cloud init config data</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-cloud-init/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-cloud-init/</guid>
      <description> Create a new VM with cloud init config data Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    
    <item>
      <title>Migrate a VM created with user data config</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-user-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-user-data/</guid>
      <description> Create a new VM with a password specified by user data config Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    
    <item>
      <title>Migrate a VM that has multiple volumes</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-volumes/</guid>
      <description> Create a new VM with a root disk and a CDROM volume Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    
    <item>
      <title>Migrate a VM that was created from a template</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-template/</guid>
      <description> Create a new VM from a template Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    
    <item>
      <title>Migrate a VM that was created using a restore backup to new VM</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-restore-to-new/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-restore-to-new/</guid>
      <description> Take an existing backup Restore the backup to a new VM Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    
    <item>
      <title>Migrate a VM with 1 backup</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-one-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-one-backup/</guid>
      <description> Create a new VM Create a backup Add a new file to the home directory Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    
    <item>
      <title>Migrate a VM with a saved SSH Key</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-ssh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-ssh/</guid>
      <description> Create a new VM with an SSH key Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    
    <item>
      <title>Migrate a VM with multiple backups</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-backups/</guid>
      <description> Create a new VM Create a backup Add a new file to the home directory Create a new backup Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    
    <item>
      <title>Migrate a VM with multiple networks</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-networks/</guid>
      <description> Create a new VM with one management network in masquerade mode one VLAN network Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again Expected Results File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file </description>
    </item>
    
    <item>
      <title>Migrate back VMs that were on host after taking host out of maintenance mode</title>
      <link>https://harvester.github.io/tests/manual/hosts/q-maintenance-mode-migrate-back-vms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/q-maintenance-mode-migrate-back-vms/</guid>
      <description>Migrate all VMs back to host that were migrated off Expected Results I&amp;rsquo;m not sure about the expected behavior on this. I&amp;rsquo;m checking.</description>
    </item>
    
    <item>
      <title>Migrate to Node without replicaset</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-to-node-without-replicaset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-to-node-without-replicaset/</guid>
      <description> Create a new VM on a 4 node cluster Check which nodes have copies of the replica set Migrate the VM to the host that does not have the volume Expected Results VM should create correctly </description>
    </item>
    
    <item>
      <title>Migrate VM from Restored backup</title>
      <link>https://harvester.github.io/tests/manual/live-migration/restored_vm_migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/restored_vm_migration/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1086
Verify Items VM can be migrate to any node with any times Case: Migrate a restored VM Install Harvester with at least 2 nodes setup backup-target with NFS Create image for VM creation Create VM a Add file with some data in VM a Backup VM a as a-bak Restore backup a-bak into VM b Start VM b then check added file should exist with same content Migrate VM b to another node, then check added file should exist with same content Migrate VM b again, then check added file should exist with same content </description>
    </item>
    
    <item>
      <title>Move Longhorn storage to another partition</title>
      <link>https://harvester.github.io/tests/manual/hosts/move-longhorn-storage-to-another-partition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/move-longhorn-storage-to-another-partition/</guid>
      <description>Related issue: #1316 Move Longhorn storage to another partition Category: Storage Test Scenarios Case 1: UEFI + GPT (Disk &amp;lt; MBR Limit) Case 2: BIOS + No MBR (Disk &amp;lt; MBR Limit) Case 3: BIOS + Force MBR (Disk &amp;lt; MBR Limit) Case 4: BIOS + No MBR (Disk &amp;gt; MBR Limit) Case 5: BIOS + Force MBR (Disk &amp;gt; MBR Limit) Case 6: UEFI + GPT (Disk &amp;gt; MBR Limit) Environment setup Test Environment: 1 node harvester on local kvm machine</description>
    </item>
    
    <item>
      <title>Multi-browser login</title>
      <link>https://harvester.github.io/tests/manual/authentication/multi-browser-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/multi-browser-login/</guid>
      <description> Login via Chrome, firefox, edge, safari etc Expected Results Chrome, firefox, edge, safari etc should have same behavior. </description>
    </item>
    
    <item>
      <title>Multiple Disks Swapping Paths</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1874-extra-disk-swap-path/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1874-extra-disk-swap-path/</guid>
      <description>Related issues: #1874 Multiple Disks Swapping Paths Verification Steps Prepare a harvester cluster (single node is sufficient) Prepare two additional disks and format both of them. Hotplug both disks and add them to the host via Harvester Dashboard (&amp;ldquo;Hosts&amp;rdquo; &amp;gt; &amp;ldquo;Edit Config&amp;rdquo; &amp;gt; &amp;ldquo;Disks&amp;rdquo;) Shutdown the host. Swap the address and slot of the two disks in order to make their dev paths swapped For libvirt environment, you can swap &amp;lt;address&amp;gt; and &amp;lt;target&amp;gt; in the XML of the disk.</description>
    </item>
    
    <item>
      <title>Negative create backup on store that is full (NFS)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-full-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-full-backup-target/</guid>
      <description> Initiate a backup with existing VM where the NFS store is full Expected Results You should get an error </description>
    </item>
    
    <item>
      <title>Negative Create Backup Target</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-create-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-create-backup-target/</guid>
      <description> Open up Backup-target in settings Input Incorrect server info Save Expected Results You should get an error on saving </description>
    </item>
    
    <item>
      <title>Negative delete backup while restore is in progress</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-backup-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-backup-while-restoring/</guid>
      <description> Create a backup of VM which has data more than 10Gi. Add 2Gi data in the same VM. Initiate deletion of the backup. While deletion is in progress, create another backup Expected Results Creation of backup should be prevented as there is a deletion is in progress. Once the deletion is completed, the backup creation should take place </description>
    </item>
    
    <item>
      <title>Negative delete multiple backups</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-multiple-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-multiple-backups/</guid>
      <description> Disconnect Backup Target Select multiple Backups from Backups list Click Delete Expected Results You should get an error </description>
    </item>
    
    <item>
      <title>Negative delete single backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-single-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-single-backup/</guid>
      <description> Take down backup target either by account, or via network blocking Delete backup from backups list Expected Results You should get an error </description>
    </item>
    
    <item>
      <title>Negative delete Volume that is in use (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/negative-delete-volume-that-is-in-use/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/negative-delete-volume-that-is-in-use/</guid>
      <description> Navigate to Volumes page and check for a volume in use by a VM Try to delete volume Click delete on modal Expected Results Page should load You should get an error message on the delete modal </description>
    </item>
    
    <item>
      <title>Negative disrupt backup server while restore is in progress</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-disrupt-backup-target-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-disrupt-backup-target-while-restoring/</guid>
      <description> Initiate a backup restore from NFS server. Disconnect network from NFS server for 5 secs Verify the restore status Expected Results The restore is not be interrupted and should complete. Data should be intact </description>
    </item>
    
    <item>
      <title>Negative edit backup read from file YAML</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-file/</guid>
      <description> Disconnect backup target Edit YAML for backup Read from File Show Diff Save Expected Results You should get an error on saving </description>
    </item>
    
    <item>
      <title>Negative edit backup YAML</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-yaml/</guid>
      <description> Disconnect backup target Edit YAML for backup Show Diff Save Expected Results You should get an error on saving </description>
    </item>
    
    <item>
      <title>Negative initiate a backup while system is taking another backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-while-taking-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-while-taking-backup/</guid>
      <description> Start a VM backup, bk-1 of a VM which has data d1 While the backup is in progress, write some more data d2 in the VM disk and initiate another backup bk-2. Verify the backup 1 and backup 2 Expected Results Backup bk-1 should have only d1 data backup bk-2 should have data d1 and d2 </description>
    </item>
    
    <item>
      <title>Negative migrate a turned on VM from one host to another</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-migrate-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-migrate-vm/</guid>
      <description> Migrate the VM from one host in the cluster to another Turn off/disconnect node while migrating Expected Results Migration should fail You should get an error message in the status </description>
    </item>
    
    <item>
      <title>Negative network comes back up after reboot external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/</guid>
      <description> Start pinging the VM reboot the VM Expected Results The VM should respond The VM should reboot The pings should stop getting responses The pings should start getting responses again </description>
    </item>
    
    <item>
      <title>Negative network comes back up after reboot management network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-management-after-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/negative-management-after-reboot/</guid>
      <description> Start pinging the VM from the management network reboot the VM Expected Results The VM should respond The VM should reboot The pings should stop getting responses The pings should start getting responses again </description>
    </item>
    
    <item>
      <title>Negative network disconnection for a longer time while migration is in progress</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-network-disconnect-while-migrating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-network-disconnect-while-migrating/</guid>
      <description> Initiate VM migration While migration is in progress, disconnect network for 100 sec on the node where the VM is scheduled Expected Results Migration should fail but volume data should be intact The VM should be accessible during the migration and should also be accessible once the migration fails </description>
    </item>
    
    <item>
      <title>Negative network disconnection for a short time while migration is in progress</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-network-disruption-while-migrating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-network-disruption-while-migrating/</guid>
      <description> Initiate VM migration. While migration is in progress, disconnect network for 5 sec on the node where the VM is scheduled Expected Results Migration should resume once the network is up again The VM should be accessible during and after the migration </description>
    </item>
    
    <item>
      <title>Negative node down while migration is in progress</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-node-down-while-migrating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-node-down-while-migrating/</guid>
      <description> Initiate VM migration. While migration is in progress, shut the node where the VM is scheduled. After failure, initiate the migration to another node Expected Results Migration should fail but volume data should be intact The VM should be accessible on older node The migration scheduled for another node should work fine The VM should be accessible during and after the migration </description>
    </item>
    
    <item>
      <title>Negative node un-schedulable during live migration</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-node-unschedulable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-node-unschedulable/</guid>
      <description>Prerequisite: Cluster is of 3 nodes. VM is running on Node-1 Node-2 and Node-3 don&amp;rsquo;t have space to migrate a VM to them. Steps: Create a vm on node-1 Migrate the VM. Expected Results Migration should not be started. Relevant error should be shown on the GUI. The existing VM should be accessible and the health check of the VM should be fine </description>
    </item>
    
    <item>
      <title>Negative Power down the node where the VM is getting replaced by the restore</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring-replace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring-replace/</guid>
      <description> Initiate a restore with existing VM. While the restore is in progress and VM is starting on a node, shut down the node Expected Results You should get an error </description>
    </item>
    
    <item>
      <title>Negative power down the node where the VM is getting restored</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring/</guid>
      <description> Initiate a restore. While the restore is in progress and VM is starting on a node, shut down the node Expected Results The restore should fail </description>
    </item>
    
    <item>
      <title>Negative restore backup replace existing VM</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace/</guid>
      <description> On multi-node setup bring down node that is hosting VM Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Expected Results You should get an error on restoring </description>
    </item>
    
    <item>
      <title>Negative restore backup replace existing VM with backup from same VM that is turned on</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-deleting-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-deleting-backup/</guid>
      <description> Make sure VM is turned on Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Delete backup while restoring Expected Results You should get an error </description>
    </item>
    
    <item>
      <title>Negative restore backup replace existing VM with backup from same VM that is turned on (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-turned-on/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-turned-on/</guid>
      <description> Make sure VM is turned on Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Expected Results You get an error that you have to stop VM before restoring backup </description>
    </item>
    
    <item>
      <title>network-attachment-definitions.k8s.cni.cncf.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/q-network-attachment-definitions.k8s.cni.cncf.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/q-network-attachment-definitions.k8s.cni.cncf.io/</guid>
      <description>GUI Enable VLAN network in settings Create a network with VLAN 5 and assume its name is my-network. Create another network with VLAN 5: it should fails with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: VLAN ID 5 is already allocated Create a VM on VLAN 5, delete network my-network and it should fail with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: network my-network is still used by vm(s): vm-test in a modal Expected Results GUI Unsure of desired behavior.</description>
    </item>
    
    <item>
      <title>Node Labeling for VM scheduling</title>
      <link>https://harvester.github.io/tests/manual/hosts/node_labeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/node_labeling/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1416
Verify Items Host labels can be assigned during installation via config-create / config-join YAML. Host labels can be managed post installation via the Harvester UI. Host label information can be accessed in Rancher Virtualization Management UI. Case: Label node when installing Install Harvester with config file and os.labels option Navigate to Host details then navigate to Labels in Config Check additional labels should be displayed Case: Label node after installed Install Harvester with at least 2 nodes Navigate to Host details then navigate to Labels in Config Use edit config to modify labels Reboot the Node and wait until its state become active Navigate to Host details then Navigate to Labels in Config Check modified labels should be displayed Case: Node&amp;rsquo;s Label availability Install Harvester with at least 2 nodes Navigate to Host details then navigate to Labels in Config Use edit config to modify labels Reboot the Node and wait until its state become active Navigate to Host details then Navigate to Labels in Config Check modified labels should be displayed Install Rancher with any nodes Navigate to Virtualization Management and import former created Harvester Wait Until state become Active Click Name field to visit dashboard repeat step 2-7, and both compare from Harvester&amp;rsquo;s dashboard (accessing via Harvester&amp;rsquo;s VIP) </description>
    </item>
    
    <item>
      <title>Nodes with cordoned status should not be in VM migration list</title>
      <link>https://harvester.github.io/tests/manual/hosts/nodes-with-cordoned-status-should-not-be-in-vm-migration-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/nodes-with-cordoned-status-should-not-be-in-vm-migration-list/</guid>
      <description>Related issues: #1501 Nodes with cordoned status should not be in the selection list for VM migration Category: Host Verification Steps Create multiple VMs on two of the nodes Set the idle node to cordoned state Edit any config of VM, click migrate Check the available node in the migration list Expected Results Node set in cordoned state will not show up in the available migration list</description>
    </item>
    
    <item>
      <title>Power down a node out of three nodes available for the Cluster</title>
      <link>https://harvester.github.io/tests/manual/deployment/negative-power-off-one-node-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/negative-power-off-one-node-cluster/</guid>
      <description> Create a three nodes cluster for Harvester. Power down an added node. Expected Results On power down the node, the status of the node should become down. Harvester system system should be still up. </description>
    </item>
    
    <item>
      <title>Power down and power up the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-power-down-power-up-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-power-down-power-up-node/</guid>
      <description>Create two vms on a cluster. Power down the node. Try to migrate a VM from the down node to active node. Leave the 2nd vm as it is. Power on the node Expected Results The 1st VM should be migrated to other node on manually doing it. The 2nd VM should be accessible once the node is up. Known bugs https://github.com/harvester/harvester/issues/982</description>
    </item>
    
    <item>
      <title>Power down the management node.</title>
      <link>https://harvester.github.io/tests/manual/deployment/negative-power-down-management-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/negative-power-down-management-node/</guid>
      <description> Create a three nodes cluster for Harvester. Power down the first node which was added to the cluster. Expected Results On power down the node, the status of the node should become down. Harvester system system should be still up. </description>
    </item>
    
    <item>
      <title>Power down the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-power-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-power-down-node/</guid>
      <description> Create two vms on a cluster. Power down the node. Try to migrate a VM from the down node to active node. Leave the 2nd vm as it is. Expected Results The 1st VM should be migrated to other node on manually doing it. The 2nd VM should be recovered from the lost node </description>
    </item>
    
    <item>
      <title>Provision RKE2 cluster with resource quota configured</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/provision-rke2-cluster-with-resource-quota-configured/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/provision-rke2-cluster-with-resource-quota-configured/</guid>
      <description>Related issues: #1455 Node driver provisioning fails when resource quota configured in project
Related issues: #1449 Incorrect naming of project resource configuration
Category: Rancher Integration Environment setup Install the latest rancher from docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Test Scenarios Scenario 1:
Project with resource quota: CPU Limit / CPU Reservation: 6000 / 6144 Memory Limit / Memory Reservation: 6000 / 6144 Scenario 2:</description>
    </item>
    
    <item>
      <title>PXE instll without iso_url field</title>
      <link>https://harvester.github.io/tests/manual/deployment/1439-pxe-install-without-iso-url-field/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/1439-pxe-install-without-iso-url-field/</guid>
      <description> Related issues: #1439 PXE boot installation doesn&amp;rsquo;t give an error if iso_url field is missing Environment setup This is easiest to test with the vagrant setup at https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester
edit https://github.com/harvester/ipxe-examples/blob/main/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2#L27 to be blank Verification Steps Run the vagrant ./setup.sh from the vagrant repo Expected Results You should get an error in the console for the VM when installing </description>
    </item>
    
    <item>
      <title>PXE instll without iso_url field</title>
      <link>https://harvester.github.io/tests/manual/hosts/1439-pxe-install-without-iso-url-field/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/1439-pxe-install-without-iso-url-field/</guid>
      <description> Related issues: #1439 PXE boot installation doesn&amp;rsquo;t give an error if iso_url field is missing Environment setup This is easiest to test with the vagrant setup at https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester
edit https://github.com/harvester/ipxe-examples/blob/main/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2#L27 to be blank Verification Steps Run the vagrant ./setup.sh from the vagrant repo Expected Results You should get an error in the console for the VM when installing </description>
    </item>
    
    <item>
      <title>Rancher import harvester enhancement</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/rancher-import-harvester-enhacement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/rancher-import-harvester-enhacement/</guid>
      <description>Related issues: #1330 Http proxy setting download image Category: Rancher Integration Environment setup Install the latest rancher from docker command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Installed a 3 nodes harvester cluster Import harvester to rancher in virtualization management Enable node driver and create cloud credential Provision a RKE2 cluster in rancher Confirm RKE2 cluster is fully operated, can explore it Shutdown all 3 nodes server machine Wait for 10 minutes Power on all harvester nodes server machines Confirm harvester is fully operated Confirm RKE2 vm is back to running Check the RKE2 cluster status in rancher Expected Results The RKE2 cluster in rancher should turn back to Running with no error after harvester server node machine is fully power off and power on.</description>
    </item>
    
    <item>
      <title>Rancher Resource quota management</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/resource_quota/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/resource_quota/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1450
Verify Items Project&amp;rsquo;s Resource quotas can be updated correctly Namespace Default Limit should be assigned as the Project configured Namespace moving between projects should work correctly Case: Create Namespace with Resource quotas Install Harvester with any nodes Install Rancher Login to Rancher, import Harvester from Virtualization Management Access Harvester dashboard via Virtualization Management Navigate to Project/Namespaces, Create Project A with Resource quotas Create Namespace N1 based on Project A The Default value of Resource Quotas should be the same as Namespace Default Limit assigned in Project A Modifying resource limit should work correctly (when increasing/decreasing, the value should increased/decreased) After N1 Created, Click Edit Config on N1 resource limit should be the same as we assigned Increase/decrease resource limit then Save Click Edit Config on N1, resource limit should be the same as we assigned Click Edit Config on N1, then increase resource limit exceeds Project A&amp;rsquo;s Limit Click Save Button, error message should shown.</description>
    </item>
    
    <item>
      <title>Reboot a cluster and check VIP</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/1669-reboot-cluster-check-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/1669-reboot-cluster-check-vip/</guid>
      <description> Related issues: #1669 Unable to access harvester VIP nor node IP after reboot or fully power cycle node machines (Intermittent) Verification Steps Enable VLAN with NIC harvester-mgmt Create VLAN 1 Disable VLAN Enable VLAN again shutdown node 3, 2, 1 server machine Wait for 15 minutes Power on node 1 server machine, wait for 20 seconds Power on node 2 server machine, wait for 20 seconds Power on node 3 server machine Check if you can access VIP and each node IP Expected Results VIP should load the page and show on every node in the terminal </description>
    </item>
    
    <item>
      <title>Reboot a cluster and check VIP</title>
      <link>https://harvester.github.io/tests/manual/hosts/1669-reboot-cluster-check-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/1669-reboot-cluster-check-vip/</guid>
      <description> Related issues: #1669 Unable to access harvester VIP nor node IP after reboot or fully power cycle node machines (Intermittent) Verification Steps Enable VLAN with NIC harvester-mgmt Create VLAN 1 Disable VLAN Enable VLAN again shutdown node 3, 2, 1 server machine Wait for 15 minutes Power on node 1 server machine, wait for 20 seconds Power on node 2 server machine, wait for 20 seconds Power on node 3 server machine Check if you can access VIP and each node IP Expected Results VIP should load the page and show on every node in the terminal </description>
    </item>
    
    <item>
      <title>Reboot host that is in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-reboot-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-reboot-host/</guid>
      <description>For Host that is in maintenance mode and turned on Reboot host Expected Results Host should reboot Maintenance mode label in hosts list should go from yellow to red to yellow Known Bugs https://github.com/harvester/harvester/issues/1272</description>
    </item>
    
    <item>
      <title>Reboot node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-reboot-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-reboot-node/</guid>
      <description> Create a vm on the cluster. Reboot the node where the vm exists. Reboot the node where there is no vm Expected Results On rebooting the node, once the node is back up and Harvester is started, the host should become available on the cluster. </description>
    </item>
    
    <item>
      <title>Reboot the management node/added node.</title>
      <link>https://harvester.github.io/tests/manual/deployment/negative-reboot-management-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/negative-reboot-management-node/</guid>
      <description> Create a three nodes cluster for Harvester. RebootÂ the management node/added node. Expected Results Once the node is up after reboot, the node should become available in the cluster. </description>
    </item>
    
    <item>
      <title>Recover cordon and maintenace node after harvester node machine reboot</title>
      <link>https://harvester.github.io/tests/manual/hosts/recover-cordon-or-maintenace-node-after-harvester-node-machine-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/recover-cordon-or-maintenace-node-after-harvester-node-machine-reboot/</guid>
      <description>Related issues: #1493 When hosts are stuck in maintenance mode and the cluster is unstable you can&amp;rsquo;t access the UI Category: Host Verification Steps Create 3 virtual machine on 3 harvester nodes Cordon 1st and 2nd node, Enable maintenance mode on 1st and 2nd node We can&amp;rsquo;t cordon and enable maintenance node on the remaining node Reboot 1st and 2nd node bare machine Wait for harvester machine back to service Login dashboard Disable maintenance mode on 1st and 2nd node Expected Results Cordon node and enter maintenance mode, after machine reboot, user can login harvester dashboard.</description>
    </item>
    
    <item>
      <title>Remove a management node from a 3 nodes cluster and add it back to the cluster by reinstalling it</title>
      <link>https://harvester.github.io/tests/manual/hosts/remove-management-node-then-reinstall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/remove-management-node-then-reinstall/</guid>
      <description> From a HA cluster with 3 nodes Delete one of the nodes after the node promotion(all 3 nodes are management nodes) Reinstall the removed node with the same node name and IP The rejoined node will be promoted to master automatically Expected Results The removed node should be able to rejoin the cluster without issues Comments Purpose is to cover this scenario: https://github.com/harvester/harvester/issues/1040 Check the job promotion with the command kubectl get jobs -n harvester-system If a node is stuck in the removing status, you likely face to this issue, execute this command as workaround: kubectl get node -o name &amp;lt;nodename&amp;gt; | xargs -i kubectl patch {} -p &#39;{&amp;quot;metadata&amp;quot;:{&amp;quot;finalizers&amp;quot;:[]}}&#39; --type=merge </description>
    </item>
    
    <item>
      <title>Remove a node from the existing cluster</title>
      <link>https://harvester.github.io/tests/manual/deployment/remove-node-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/remove-node-cluster/</guid>
      <description>Remove node from the Harvester cluster using the Harvester UI Expected Results The components of Harvester should get cleaned up from the node.</description>
    </item>
    
    <item>
      <title>Remove unavailable node with VMs on it</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-remove-unavailable-node-with-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-remove-unavailable-node-with-vm/</guid>
      <description>Create VMs on a host. Turn off Host Remove Host from hosts list Expected Results VMs should migrate to new host Known Bugs https://github.com/harvester/harvester/issues/983</description>
    </item>
    
    <item>
      <title>Restart/Stop VM with in progress Backup</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1702-do-not-allow-restart-or-stop-vm-when-backup-is-in-progress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1702-do-not-allow-restart-or-stop-vm-when-backup-is-in-progress/</guid>
      <description> Related issues: #1702 Don&amp;rsquo;t allow restart/stop vm when backup is in progress Verification Steps Create a VM. Create a VMBackup for it. Before VMBackup is done, stop/restart the VM. Verify VM can&amp;rsquo;t be stopped/restarted. </description>
    </item>
    
    <item>
      <title>Restore backup create new vm (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-create-new-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-create-new-vm/</guid>
      <description> Create a new file before restoring the backup and add some data Stop the VM where the backup was taken Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Validate that new file is no longer present on machine Expected Results Backup should restore VM should update to previous backup File should no longer be present </description>
    </item>
    
    <item>
      <title>Restore backup create new vm in another namespace</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-create-new-vm-in-another-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-create-new-vm-in-another-namespace/</guid>
      <description> Create a VM vm in namespace default. Create a file ~/test.txt with content test. Create a VMBackup default-vm-backup for it. Create a new namepsace new-ns. Create a VMRestore restore-default-vm-backup-to-new-ns in new-ns namespace based on the VMBackup default-vm-backup to create a new VM. Expected Results A new VM in new-ns namespace should be created. It should have the file ~/test.txt with content test. </description>
    </item>
    
    <item>
      <title>Restore Backup for VM that was live migrated (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-for-vm-live-migrated/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-for-vm-live-migrated/</guid>
      <description> Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Validate that new file is no longer present on machine Expected Results Backup should restore VM should update to previous backup File should no longer be present </description>
    </item>
    
    <item>
      <title>Restore backup replace existing VM with backup from same VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-replace-existing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-replace-existing/</guid>
      <description> Create a new file before restoring the backup and add some data Stop the VM Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Validate that new file is no longer present on machine Expected Results Backup should restore VM should update to previous backup File should no longer be present </description>
    </item>
    
    <item>
      <title>Restore First backup in chained backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-first-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-first-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Validate that files didn&amp;rsquo;t change Restore to backup 1 Validate that md5sum -c file1.</description>
    </item>
    
    <item>
      <title>Restore last backup in chained backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-last-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-last-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Validate that files didn&amp;rsquo;t change Restore to backup 3 Validate that md5sum -c file1-2.</description>
    </item>
    
    <item>
      <title>Restore middle backup in chained backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-middle-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-middle-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Validate that files didn&amp;rsquo;t change Restore to backup 2 Validate that md5sum -c file1-2.</description>
    </item>
    
    <item>
      <title>Run multiple instances of the console</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/run-multiple-instances-console/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/run-multiple-instances-console/</guid>
      <description> Open up the console on two browsers to simulate multiple connections Login with both browsers create a new file on both instances Edit the file from the other instance and save Verify that you can see the changes from the other instance Expected Results You should be able to login from multiple browsers File should create File should update You should be able to see changes from all instances </description>
    </item>
    
    <item>
      <title>Set backup target S3</title>
      <link>https://harvester.github.io/tests/manual/advanced/set-s3-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/set-s3-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose S3 Set valid S3 target Save Expected Results login should complete Settings should save You should not get an error message </description>
    </item>
    
    <item>
      <title>Set backup-target NFS</title>
      <link>https://harvester.github.io/tests/manual/advanced/set-nfs-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/set-nfs-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose NFS Set valid NFS target Save Expected Results login should complete Settings should save You should not get an error message </description>
    </item>
    
    <item>
      <title>Set backup-target NFS invalid target</title>
      <link>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-nfs-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-nfs-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose NFS Set invalid NFS target Save Expected Results login should complete Settings should save You should get an error message </description>
    </item>
    
    <item>
      <title>Set backup-target S3 invalid target</title>
      <link>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-s3-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-s3-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose S3 Set invalid S3 target Save Expected Results login should complete Settings should save You should get an error message </description>
    </item>
    
    <item>
      <title>Set maintenance mode on the last available node shouldn&#39;t be allowed</title>
      <link>https://harvester.github.io/tests/manual/hosts/set-maintenance-mode-on-the-last-available-node-shouldnt-be-allowed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/set-maintenance-mode-on-the-last-available-node-shouldnt-be-allowed/</guid>
      <description>Related issues: #1014 Trying to set maintenance mode on the last available node shouldn&amp;rsquo;t be allowed Category: Host Verification Steps Create 3 vms located on node2 and node3 Open host page
Set node 3 into maintenance mode
Wait for virtual machine migrate to node 2
Set node 2 into maintenance mode
wait for virtual machine migrate to node 1
Set node 2 into maintenance mode
Expected Results Within 3 nodes and 3 virtual machines testing environment.</description>
    </item>
    
    <item>
      <title>Shut down host in maintenance mode and verify label change</title>
      <link>https://harvester.github.io/tests/manual/hosts/1272-shutdown-host-in-maintenance-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/1272-shutdown-host-in-maintenance-mode/</guid>
      <description> Related issues: #1272 Shut down a node with maintenance mode should show red label Verification Steps Open host page Set a node to maintenance mode Turn off host vm of the node Check node status Turn on host Check node status Expected Results The node should go into maintenance mode The node label should go red When turned on the node status should go back to yellow </description>
    </item>
    
    <item>
      <title>SSL Certificate</title>
      <link>https://harvester.github.io/tests/manual/advanced/ssl-certificate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/ssl-certificate/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/761
Verify Items generated kubeconfig is able to access kubenetes API new node able to join the cluster using the configured Domain Name create node with ssl-certificates settings is working as expected. Case: Kubeconfig Install Harvester with at least 2 nodes Generate self-signed TLS certificates from https://www.selfsignedcertificate.com/ with specific name Navigate to advanced settings, edit ssl-certificates settings Update generated .cert file to CA and Public Certificate, .key file to Private Key Relogin with domain name Navigate to Support page, then Click Download KubeConfig, file should named local.</description>
    </item>
    
    <item>
      <title>Start Host in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-start-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-start-host/</guid>
      <description>For Host that is in maintenance mode and turned off Start host Expected Results Host should turn on Maintenance mode label in hosts list should go from red to yellow Known bugs https://github.com/harvester/harvester/issues/1272</description>
    </item>
    
    <item>
      <title>Start VM and stop node Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm-and-stop-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm-and-stop-node/</guid>
      <description> Start the VM In a multi-node setup disconnect/shutdown the node where the VM is running Expected Results You should not be able to start the VM </description>
    </item>
    
    <item>
      <title>Start VM Negative (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Start the VM Expected Results You should not be able to start the VM </description>
    </item>
    
    <item>
      <title>Stop VM Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-stop-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-stop-vm/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Stop the VM Expected Results The VM list should quickly update to not running, or some other error state </description>
    </item>
    
    <item>
      <title>Support volume hot plug live migrate</title>
      <link>https://harvester.github.io/tests/manual/live-migration/support-volume-hot-unplug-live-migrate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/support-volume-hot-unplug-live-migrate/</guid>
      <description> Related issues: #1401 Support volume hot-unplug Category: Storage Environment setup Setup an airgapped harvester
Create an 3 nodes harvester cluster with large size disks Verification Steps Scenario2: Live migrate VM not have hot-plugged volume before, do hot-plugged the unplugged. Create a virtual machine Create several volumes (without image) Add volume, hot-plug volume to virtual machine Open virtual machine, find hot-plugged volume Click Detach volume Add volume again Migrate VM from one node to another Detach volume Add unplugged volume again Expected Results Can hot-plug volume without error Can hot-unplug the pluggable volumes without restarting VM The de-attached volume can also be hot-plug and mount back to VM </description>
    </item>
    
    <item>
      <title>Support Volume Hot Unplug</title>
      <link>https://harvester.github.io/tests/manual/volumes/support-volume-hot-unplug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/support-volume-hot-unplug/</guid>
      <description> Related issues: #1401 Support volume hot-unplug Category: Storage Environment setup Setup an airgapped harvester
Create an 3 nodes harvester cluster with large size disks Scenario1: Live migrate VM already have hot-plugged volume to new node, then detach (hot-unplug) it Verification Steps Create a virtual machine Create several volumes (without image) Add volume, hot-plug volume to virtual machine Open virtual machine, find hot-plugged volume Click de-attach volume Add volume again Expected Results Can hot-plug volume without error Can hot-unplug the pluggable volumes without restarting VM The de-attached volume can also be hot-plug and mount back to VM </description>
    </item>
    
    <item>
      <title>Switch the vlan interface of harvester node</title>
      <link>https://harvester.github.io/tests/manual/network/switch-the-vlan-interface-of-harvester-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/switch-the-vlan-interface-of-harvester-node/</guid>
      <description>Related issues: #1464 VM pods turn to the terminating state after switching the VLAN interface Category: Network Verification Steps User ipxe-example to build up 3 nodes harvester Login harvester dashboard -&amp;gt; Access Settings Enable vlan network with harvester-mgmt NIC interface Create a VM using harvester-mgmt Disable vlan network Enable vlan network and select bond0 interface Check host and vm is working Directly switch network interface from bond0 to harvester-mgmt without disable it.</description>
    </item>
    
    <item>
      <title>Take host out of maintenance mode that has been rebooted	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-rebooted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-rebooted/</guid>
      <description> For host in maintenance mode that has been rebooted take host out of maintenance mode Expected Results Host should go to Active Label shbould go green </description>
    </item>
    
    <item>
      <title>Take host out of maintenance mode that has not been rebooted (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-not-rebooted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-not-rebooted/</guid>
      <description> For host in maintenance mode that has not been rebooted take host out of maintenance mode Expected Results Host should go to Active Label shbould go green </description>
    </item>
    
    <item>
      <title>Target Harvester by setting the variable kubeconfig with your kubeconfig file in the provider.tf file (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-variasble/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-variasble/</guid>
      <description> Define the kubeconfig variable in the provider.tf file terraform { required_providers { harvester = { source = &amp;#34;registry.terraform.io/harvester/harvester&amp;#34; version = &amp;#34;~&amp;gt; 0.1.0&amp;#34; } } } provider &amp;#34;harvester&amp;#34; { kubeconfig = &amp;#34;/path/of/my/kubeconfig&amp;#34; } Check if you can interact with the Harvester by creating resource like a SSH key Execute the terraform apply command Expected Results The resource should be created Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Check if you can see your resource in the Harvester WebUI </description>
    </item>
    
    <item>
      <title>Target Harvester with the default kubeconfig located in $HOME/.kube/config (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-home/</guid>
      <description> Make sure the kubeconfig is defined in the file $HOME/.kube/config Check if you can interact with the Harvester by creating resource like a SSH key Execute the terraform apply command Expected Results The resource should be created Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Check if you can see your resource in the Harvester WebUI </description>
    </item>
    
    <item>
      <title>Temporary network disruption</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-network-disruption/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-network-disruption/</guid>
      <description> Create a vms on the cluster. Disable network of a node for sometime. e.g. 5 sec, 5 mins Expected Results VM should be accessible after the network is up. </description>
    </item>
    
    <item>
      <title>Test a deployment with ALL resources at the same time (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/deployment-all-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/deployment-all-resources/</guid>
      <description>Re-use the previous generated TF files and group them all either in one directory or in the same file Generates a speculative execution plan with terraform plan command Create the resources with terraform apply command Check that all resources are correctly created/running on the Harvester cluster Destroy the resources with the command terraform destroy Expected Results Refer to the harvester_ssh_key resource expected results</description>
    </item>
    
    <item>
      <title>Test aborting live migration</title>
      <link>https://harvester.github.io/tests/manual/live-migration/abort-live-migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/abort-live-migration/</guid>
      <description> On a VM that is turned on select migrate Start the migration Abort the migration Expected Results You should see the status move to migrating You should see the status move to aborting migration You should see the status move to running The VM should pass health checks </description>
    </item>
    
    <item>
      <title>Test NTP server timesync</title>
      <link>https://harvester.github.io/tests/manual/hosts/1535-test-ntp-timesync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/1535-test-ntp-timesync/</guid>
      <description> Related issues: #1535 NTP daemon in host OS Environment setup This should be on at least a 3 node setup that has been running for several hours that had NTP servers setup during install
Verification Steps SSH into nodes and verify times are close Verify NTP is active with sudo timedatectl status Expected Results Times should be within a minute of each other NTP should show as active </description>
    </item>
    
    <item>
      <title>Test NTP server timesync</title>
      <link>https://harvester.github.io/tests/manual/misc/1535-test-ntp-timesync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/misc/1535-test-ntp-timesync/</guid>
      <description> Related issues: #1535 NTP daemon in host OS Environment setup This should be on at least a 3 node setup that has been running for several hours that had NTP servers setup during install
Verification Steps SSH into nodes and verify times are close Verify NTP is active with sudo timedatectl status Expected Results Times should be within a minute of each other NTP should show as active </description>
    </item>
    
    <item>
      <title>Test the harvester_clusternetwork resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-clusternetwork-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-clusternetwork-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    
    <item>
      <title>Test the harvester_image resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-image-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-image-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    
    <item>
      <title>Test the harvester_network resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-network-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-network-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    
    <item>
      <title>Test the harvester_ssh_key resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-ssh-key-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-ssh-key-resource/</guid>
      <description>These following steps must be done for every resources, for avoiding repetitions, look at the detailed instructions at the beginning of the page.
Import a resource Generates a speculative execution plan with terraform plan command Create the resource with terraform apply command Use terraform plan again Use terraform apply again Destroy the resource with the command terraform destroy Expected Results The resource is well imported in the terraform.tfstate file and you can print it with the terraform show command The command should display the difference between the actual status and the configured status Plan: 1 to add, 0 to change, 0 to destroy.</description>
    </item>
    
    <item>
      <title>Test the harvester_virtualmachine resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-virtualmachine-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-virtualmachine-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    
    <item>
      <title>Test the harvester_volume resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-volume-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-volume-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    
    <item>
      <title>Test zero downtime for live migration download test</title>
      <link>https://harvester.github.io/tests/manual/live-migration/zero-downtime-download-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/zero-downtime-download-test/</guid>
      <description> Connect to VM via console Start a large file download Live migrate VM to new host Verify that file download does not fail Expected Results Console should open VM should start to migrate File download should </description>
    </item>
    
    <item>
      <title>Test zero downtime for live migration ping test</title>
      <link>https://harvester.github.io/tests/manual/live-migration/zero-downtime-ping-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/zero-downtime-ping-test/</guid>
      <description> Continually ping VM Verify that ping is getting a response Live migrate VM to new host Verify that ping continues Expected Results Ping should get response VM should start to migrate Ping should not get any dropped packets </description>
    </item>
    
    <item>
      <title>Timeout option for support bundle</title>
      <link>https://harvester.github.io/tests/manual/advanced/support_bundle_timeout/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/support_bundle_timeout/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1585
Verify Items An Timeout Option can be configured for support bundle Error message will display when reach timeout Case: Generate support bundle but hit timeout Install Harvester with at least 2 nodes Navigate to Advanced Settings, modify support-bundle-timeout to 2 Navigate to Support, Click Generate Support Bundle, and force shut down one of the node in the mean time. 2 mins later, the function will failed with an Error message pop up as the snapshot </description>
    </item>
    
    <item>
      <title>toggle harvester node driver with the harvester global flag</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/toggle-harvester-node-driver-with-harvester-global-flag/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/toggle-harvester-node-driver-with-harvester-global-flag/</guid>
      <description>Related issue: #1465 toggle harvester node driver with the harvester global flag Category: Rancher Integration Environment setup Install rancher 2.6.3 by docker docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Environment preparation as above steps Open global setting -&amp;gt; feature flag in rancher Check harvester feature flag Open cluster management -&amp;gt; Driver page Check harvester node driver Deactivate harvester feature flag Activate harvester feature flag Deactivate harvester node driver Activate harvester node driver Deactivate both harvester flag and node driver Activate harvester feature flag Expected Results Harvester feature flag will be enabled by default and turned on harvester node driver accordingly If the feature flag was turned off, nothing will change to the Harvester node driver.</description>
    </item>
    
    <item>
      <title>Try to add a network with no name (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-add-network-no-name/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/negative-add-network-no-name/</guid>
      <description> Navigate to the networks page in harvester Click Create Don&amp;rsquo;t add a name Add a VLAN ID Click Create Expected Results You should get an error that says you need to add a name </description>
    </item>
    
    <item>
      <title>Turn off host that is in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-turn-off-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-turn-off-host/</guid>
      <description>Put host in maintenance mode Migrate VMs Wait for VMs to migrate Wait for any vms to migrate off Shut down Host Expected Results Host should start to go into maintenance mode Any VMs should migrate off Host should go into maintenance mode host should shut down Maintenance mode label in hosts list should go red Known bugs https://github.com/harvester/harvester/issues/1272</description>
    </item>
    
    <item>
      <title>UI enables option to display password on login page</title>
      <link>https://harvester.github.io/tests/manual/authentication/ui_password_show_btn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/ui_password_show_btn/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1550
Verify Items Password field in login page can be toggle show/hide Case: Toggle of Password field install harvester with any nodes setup password logout then login with password toggled </description>
    </item>
    
    <item>
      <title>Update image labels after deleting source VM</title>
      <link>https://harvester.github.io/tests/manual/images/1602-update-labels-on-image-after-vm-delete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/1602-update-labels-on-image-after-vm-delete/</guid>
      <description> Related issues: #1602 exported image can&amp;rsquo;t be deleted after vm removed Verification Steps create vm &amp;ldquo;vm-1&amp;rdquo; create a image &amp;ldquo;img-1&amp;rdquo; by export the volume used by vm &amp;ldquo;vm-1&amp;rdquo; delete vm &amp;ldquo;vm-1&amp;rdquo; update image &amp;ldquo;img-1&amp;rdquo; labels Expected Results image &amp;ldquo;img-1&amp;rdquo; will be updated </description>
    </item>
    
    <item>
      <title>Update image labels after deleting source VM</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/1602-update-labels-on-image-after-vm-delete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/1602-update-labels-on-image-after-vm-delete/</guid>
      <description> Related issues: #1602 exported image can&amp;rsquo;t be deleted after vm removed Verification Steps create vm &amp;ldquo;vm-1&amp;rdquo; create a image &amp;ldquo;img-1&amp;rdquo; by export the volume used by vm &amp;ldquo;vm-1&amp;rdquo; delete vm &amp;ldquo;vm-1&amp;rdquo; update image &amp;ldquo;img-1&amp;rdquo; labels Expected Results image &amp;ldquo;img-1&amp;rdquo; will be updated </description>
    </item>
    
    <item>
      <title>Upload Cloud Image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/upload-cloud-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/upload-cloud-image/</guid>
      <description> Upload image to images page Create new vm with image using appropriate template Run VM health checks Expected Results Image should upload Health checks should pass </description>
    </item>
    
    <item>
      <title>Upload image that is invalid</title>
      <link>https://harvester.github.io/tests/manual/images/negative-upload-invalid-image-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/negative-upload-invalid-image-file/</guid>
      <description>steTry to upload invalid image file to images page Something like dmg, or tar.gzps Expected Results You should get an error Known Bugs https://github.com/harvester/harvester/issues/1425</description>
    </item>
    
    <item>
      <title>Upload ISO Image</title>
      <link>https://harvester.github.io/tests/manual/images/upload-iso-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/upload-iso-image/</guid>
      <description> Upload image to images page Create new vm with image using appropriate template Run VM health checks Expected Results Image should upload Health checks should pass </description>
    </item>
    
    <item>
      <title>Use a non-admin user</title>
      <link>https://harvester.github.io/tests/manual/node-driver/non-admin-user/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/non-admin-user/</guid>
      <description>create harvester user ltang, password ltang add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster Expected Results The status of the created cluster shows active The status of the corresponding vm on harvester active The information displayed on rancher and harvester matches the template configuration Test Data Harvester Node Template HARVESTER OPTIONS Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Use template to create cluster through virtualization management</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/use-template-to-create-cluster-through-virtualization-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/use-template-to-create-cluster-through-virtualization-management/</guid>
      <description>Related issue: #1620 User is unable to use template to create cluster through virtualization management Category: Rancher Integration Environment setup Install rancher 2.6.3 by docker docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps Import harvester from rancher through harvester settings Access harvester from rancher virtualization management page Open Virtual Machine page Click create Check Use VM Template Select one of the template Create VM according to the template Expected Results Access harvester from Rancher, on virtual machine page can load default three template to create VM.</description>
    </item>
    
    <item>
      <title>Validate network connectivity external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/validate-network-external-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/validate-network-external-vlan/</guid>
      <description> Create a new VM Make sure that the network is set to the external VLAN with bridge as the type Ping VM Attempt to SSH to VM Expected Results VM should be created You should be able to ping the VM from an external network You should be able to SSH to VM </description>
    </item>
    
    <item>
      <title>Validate network connectivity invalid external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-network-connectivity-invalid-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/negative-network-connectivity-invalid-vlan/</guid>
      <description> Create a new VM Make sure that the network is set to the external VLAN with bridge as the type and a VLAN ID that isn&amp;rsquo;t valid for your network Ping VM Attempt to SSH to VM Expected Results VM should be created You should not be able to ping the VM from an external network You should not be able to SSH to VM </description>
    </item>
    
    <item>
      <title>Validate network connectivity management network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/validate-network-management-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/validate-network-management-network/</guid>
      <description> Create a new VM Make sure that the network is set to the management network with masquerade as the type Ping VM Attempt to SSH to VM Expected Results VM should be created You should not be able to ping the VM from an external network You should not be able to SSH to VM </description>
    </item>
    
    <item>
      <title>Validate QEMU agent installation</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/1235-check-qemu-installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/1235-check-qemu-installation/</guid>
      <description> Related issues: #1235 QEMU agent is not installed by default when creating VMs Verification Steps Creat openSUSE VM Start VM check for qemu-ga package Create Ubuntu VM Start VM Check for qemu-ga package Expected Results VMs should start Packages should be present </description>
    </item>
    
    <item>
      <title>Validate volume shows as in use when attached (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/validate-volume-shows-in-use-while-attached/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/validate-volume-shows-in-use-while-attached/</guid>
      <description> Navigate to Volumes and check for a volume in use by a VM Verify that the state says In Use Expected Results State should show correctly </description>
    </item>
    
    <item>
      <title>Verify &#34;Add Node Pool&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/verify-add-node-pool/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/verify-add-node-pool/</guid>
      <description> Create a cluster of 3 nodes, One node with etcd, Control Plane, Worker, the other two with Worker The cluster is created successfully, use the command kubectl get node to view the node roles Expected Results The status of the created cluster shows active show the 3 created node status running in harvester&amp;rsquo;s vm list the information displayed on rancher and harvester matches the template configuration Check that the node role is correct </description>
    </item>
    
    <item>
      <title>Verify and Configure Networking Connection (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-network-connection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-network-connection/</guid>
      <description>Provide the hostName Select management NIC bond Select the IPv4 (Automatic and Static) Expected Results This value of hostname should be overwritten by DHCP if DHCP supplies a hostname for the system. If DHCP doesn&amp;rsquo;t offer a hostname and this value is empty, a random hostname will be generated.</description>
    </item>
    
    <item>
      <title>Verify Configuring SSH keys</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-ssh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-ssh/</guid>
      <description>Provide SSH keys while installing the Harvester. Verify user is able to login the node using that ssh key. Expected Results User should be able to login to the node using that ssh key.</description>
    </item>
    
    <item>
      <title>Verify Configuring via HTTP URL</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-http-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-http-config/</guid>
      <description>Provide the remote Harvester config, you can find an example of the config I&amp;rsquo;m using in the deployment test plan description Expected Results Check that all values are taking into account If you are using my config file, check: the node must be off after the installation the nvme and kvm modules are loaded the fileÂ /etc/test.txt exists with the correct rights the systcl values the env variableÂ test_env should exist dns configured in /etc/resolv.</description>
    </item>
    
    <item>
      <title>Verify Enabling maintenance mode</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-enabling-maintenance-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/verify-enabling-maintenance-mode/</guid>
      <description> Navigate to the Hosts page and select the node Click Maintenance Mode Expected Results The existing VM should get migrated to other nodes. Verify the CRDs to see the maintenance mode is enabled. Comments Needs other test cases to be added If VM migration fails How does live migration work What happens if there are no schedulable resources on nodes Check CRDs on hosts On going into maintenance mode kubectl get virtualmachines &amp;ndash;all-namespaces Kubectl get virtualmachines/name -o yaml On coming out of maintenance mode kubectl get virtualmachines &amp;ndash;all-namespaces Kubectl get virtualmachines/name -o yaml Check that maintenance mode host isn&amp;rsquo;t schedulable Fully provision all nodes and try to create a VM It should fail Migration with maintenance mode What if migration gets stuck, can you cancel VMs going to different hosts Canceling maintenance mode P1 Put in maintenance mode Check migration of VMs Check status of VMs modify filesystem on VMs Check status of host Take host out of maintenance mode Check status of host Migrate VMs back to host Check filesystem Create new VMs on host Check status of VMs </description>
    </item>
    
    <item>
      <title>Verify network data template</title>
      <link>https://harvester.github.io/tests/manual/misc/1634-terms-and-conditions-link/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/misc/1634-terms-and-conditions-link/</guid>
      <description> Related issues: #1634 Welcome screen asks to agree to T&amp;amp;Cs for using Rancher not Harvester Verification Steps Install Harvester Go to management page and see last line (before Continue button) Verify link to SUSE EULA https://www.suse.com/licensing/eula/ Expected Results Link should go to SUSE EULA </description>
    </item>
    
    <item>
      <title>Verify network data template</title>
      <link>https://harvester.github.io/tests/manual/templates/1655-network-data-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/templates/1655-network-data-template/</guid>
      <description> Related issues: #1655 When using a VM Template the Network Data in the template is not displayed Verification Steps Create new VM template with network data in advanced settings network: version: 1 config: - type: physical name: interface0 subnets: - type: static address: 10.84.99.0/24 gateway: 10.84.99.254 Create new VM and select template Verify that network data is in advanced network config Expected Results network data should show </description>
    </item>
    
    <item>
      <title>Verify operations like Stop, restart, pause, download YAML, generate template (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/</guid>
      <description> Take an existing VM and Press the appropriate buttons for the associated operations Stop Restart Pause Download YAML Generate Template Expected Results All operations should complete successfully </description>
    </item>
    
    <item>
      <title>Verify SSH key was added from Github during install</title>
      <link>https://harvester.github.io/tests/manual/authentication/verify-github-ssh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/verify-github-ssh/</guid>
      <description> Add ssh key from Github while installing the Harvester. Login Harvester with github. Expected Results User should be able to logout/login successfully. </description>
    </item>
    
    <item>
      <title>Verify that vm-force-reset-policy works</title>
      <link>https://harvester.github.io/tests/manual/advanced/1661-vm-force-reset-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/1661-vm-force-reset-policy/</guid>
      <description> Related issues: #1661 vm-force-deletion-policy for vm-force-reset-policy Environment setup Setup an airgapped harvester
Create a 3 node harvester cluster Verification Steps Navigate to advanced settings and edit vm-force-reset-policy Set reset policy to 60 Create VM Run health checks Shut down node that is running VM Check for when it starts to migrate to new Host Expected Results It should migrate after 60 seconds </description>
    </item>
    
    <item>
      <title>Verify that vm-force-reset-policy works</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/1660-volume-unit-vm-details/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/1660-volume-unit-vm-details/</guid>
      <description> Related issues: #1660 The volume unit on the vm details page is incorrect Verification Steps Create new .1G volume Create new VM Create with raw-image template Add opensuse base image Add .1G Volume Verify size in VM details on volume tab Expected Results Size should show as .1G </description>
    </item>
    
    <item>
      <title>Verify that vm-force-reset-policy works</title>
      <link>https://harvester.github.io/tests/manual/volumes/1660-volume-unit-vm-details/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/1660-volume-unit-vm-details/</guid>
      <description> Related issues: #1660 The volume unit on the vm details page is incorrect Verification Steps Create new .1G volume Create new VM Create with raw-image template Add opensuse base image Add .1G Volume Verify size in VM details on volume tab Expected Results Size should show as .1G </description>
    </item>
    
    <item>
      <title>Verify that VMs stay up when disks are evicted</title>
      <link>https://harvester.github.io/tests/manual/volumes/1334-evict-disks-check-vms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/1334-evict-disks-check-vms/</guid>
      <description>Related issues: #1334 Volumes fail with Scheduling Failure after evicting disc on multi-disc node Verification Steps Created 3 node Harvester setup with ipxe example in KVM/libvirt Added formatted disk to node0 VM Created three VMs on node0 Created large files on three VMs to see where they were located with dd if=/dev/urandom of=file1.txt count=5192 bs=1M Checked Longhorn to be sure that some VMs were on new disk Deleted disk from Harvester Checked Longhorn to be sure that disk was marked for eviction Verified that VMs were still available while evicting replicas by running commands from serial console/SSH Verified that disk was removed from Longhorn and VMS were still up.</description>
    </item>
    
    <item>
      <title>Verify the external link at the bottom of the page</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-bottom-links/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/ui/verify-bottom-links/</guid>
      <description> Click all the external links available at the bottom of the page - Docs, Forums, Slack, File an issue. Click the Generate support bundle at the bottom of the page Expected Results The external links should take user to correct URL in new tab in the browser. The support bundle should be generated once the Generate support bundle. The progress should be shown while the bundle is getting generated. The Generated bundle should have all components logs and Yaml </description>
    </item>
    
    <item>
      <title>Verify the Filter on the Host page</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-filter-on-host-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/verify-filter-on-host-page/</guid>
      <description> Enter name of a host and verify the nodes get filtered out. Expected Results The edited name should be reflected on the host. </description>
    </item>
    
    <item>
      <title>Verify the Harvester UI URL</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-url/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/ui/verify-url/</guid>
      <description> Navigate to the Harvester UI and verify the URL. Verify the Harvester icon on the left top corner Expected Results The URL should be the management ip + /dashboard redirect to login page if not login redirect to dashboard page if already login </description>
    </item>
    
    <item>
      <title>Verify the info of the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-node-info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/verify-node-info/</guid>
      <description> Navigate to the hosts tab and verify the following. State Name Host IP CPU Memory Storage Size Age Expected Results All the data/status shown on the page should be correct. </description>
    </item>
    
    <item>
      <title>Verify the installation confirmation screen</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-installation-confirmation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-installation-confirmation/</guid>
      <description>Verify all the details shown on the screen Expected Results The info should reflect all the user filled data.</description>
    </item>
    
    <item>
      <title>Verify the Installer Options</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-installer-options/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-installer-options/</guid>
      <description> Verify the following options available while installing the Harvester is working Installation target Cluster token Password VIP NTP Address Expected Results Should show all the disks available. Verify the min and max length acceptable for cluster token. Verify the password rule </description>
    </item>
    
    <item>
      <title>Verify the left side menu</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-left-menu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/ui/verify-left-menu/</guid>
      <description> Check all the menu at the left side of the screen. Verify the preference and logout option is available at the right top of the screen Expected Results The menu should have Dashboard, Hosts, Virtual machines, Volumes, Images and Advance. The Advance menu should have sub menu Templates, backups, network, SSH keys, Users, Cloud config templates, Settings. Clicking on the menu should take user to the respective pages </description>
    </item>
    
    <item>
      <title>Verify the links which navigate to the internal pages</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-internal-links/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/ui/verify-internal-links/</guid>
      <description> Click the links available on the pages like on dashboard - host, virtual machines etc Verify the events and resources tabs presents in the pages e.g. - Dashboard, Virtual machines Expected Results The internal link should take user to the correct page in the same tab opened in the browser </description>
    </item>
    
    <item>
      <title>Verify the options available for image</title>
      <link>https://harvester.github.io/tests/manual/images/verify-options-available-for-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/verify-options-available-for-image/</guid>
      <description> Create vm with YAML using the menu option. Download Yaml Verify the downloaded Yaml file. Clone the Image Expected Results All user-specified fields must match what show on GUI: Namespace Name Description URL Labels </description>
    </item>
    
    <item>
      <title>Verify the Proxy configuration</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-proxy/</guid>
      <description>Provide a valid proxy address, verify it works after installation is complete. Provide empty proxy address. Expected Results For empty proxy address, by default DHCP should provide the management url and it should navigate to the Harvester UI.</description>
    </item>
    
    <item>
      <title>Verify the state for Powered down node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-verify-state-powered-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-verify-state-powered-down-node/</guid>
      <description> Power down the node and check the state of the node in the cluster Expected Results The node state should show unavilable </description>
    </item>
    
    <item>
      <title>VIP configured in a VLAN network should be reached</title>
      <link>https://harvester.github.io/tests/manual/network/vip-configured-on-vlan-network-should-be-reached/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/vip-configured-on-vlan-network-should-be-reached/</guid>
      <description> Related issue: #1424 VIP configured in a VLAN network can not be reached Category: Network Environment Setup The network environment must have vlan network configured and also have DHCP server prepared on your testing vlan Verification Steps Enable virtual network with harvester-mgmt Open Network -&amp;gt; Create a virtual network Provide network name and correct vlan id Open Route, use the default auto setting Create a VM and use the created route SSH to harvester node Ping the IP of the created VM Create a virutal network Provide network name and correct vlan id Open Route, use the manual setting Provide the CIDR and Gateway value Repeat step 5 - 7 Expected Results Check the auto route vlan can be detected with running status Check the manual route vlan can be detected with running status Check the VM can get IP based on auto or manual vlan route Check can ping VM IP from harvester node </description>
    </item>
    
    <item>
      <title>VIP is accessibility with VLAN enabled on management port</title>
      <link>https://harvester.github.io/tests/manual/network/vip_vlan_mgmtport/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/vip_vlan_mgmtport/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1722
Verify Items VIP should be accessible when VLAN enabled on management port Case: Single Node enables VLAN on management port Install Harvester with single node Login to dashboard then navigate to Settings Edit vlan to enable VLAN on harvester-mgmt reboot the node after reboot, login to console Run the command should not contain any output sudo -s kubectl get pods -A --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39; | grep harvester-network-controller-manager | xargs kubectl logs -n harvester-system | grep &amp;quot;Failed to update lock&amp;quot; Repeat step 4-6 with 10 times, should not have any error </description>
    </item>
    
    <item>
      <title>VIP Load balancer verification (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-vip-load-balancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-vip-load-balancer/</guid>
      <description> Install Harvester on one Node Install with VIP pulling from DHCP Verify that IP is assigned via DHCPÂ Add at least one additional node Use VIP address as management address for adding node Finish install of additional nodes Create new VM Connect to VM via web console Expected Results Install of all nodes should complete New nodes should show up in hosts list via web UI at VIP VMs should create Console should open </description>
    </item>
    
    <item>
      <title>virtualmachineimages.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/virtualmachineimages.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/virtualmachineimages.harvesterhci.io/</guid>
      <description>GUI Create an image from GUI Create another image with the same name. The operation should fail with admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: A resource with the same name exists kube-api Create an image from the manifest: $ cat image.yaml --- apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: generateName: image- namespace: default spec: sourceType: download displayName: cirros-0.5.1-x86_64-disk2.img url: http://192.168.2.106/cirros-0.5.1-x86_64-disk.img $ kubectl create -f image.yml virtualmachineimage.harvesterhci.io/image-8dkbq created Try to create an image with the same manifest: $ kubectl create -f image.</description>
    </item>
    
    <item>
      <title>virtualmachinerestores.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/virtualmachinerestores.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/virtualmachinerestores.harvesterhci.io/</guid>
      <description>GUI Setup a backup target Create a backup from a VM. Assume the VM name is vm-test Wait until backup is done Restore the backup to a VM, enter vm-test in the Virtual Machine Name field kube-api $ cat restore.yaml 1 --- apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineRestore metadata: name: restore-aaaa namespace: default spec: newVM: false target: apiGroup: kubevirt.io kind: VirtualMachine name: &amp;#34;&amp;#34; virtualMachineBackupName: test $ kubectl create -f restore.yaml Expected Results GUI The operation should fail with admission webhook &amp;ldquo;validator.</description>
    </item>
    
    <item>
      <title>virtualmachinetemplateversions.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/virtualmachinetemplateversions.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/virtualmachinetemplateversions.harvesterhci.io/</guid>
      <description>kube-api List default templates: $ kubectl get virtualmachinetemplateversions.harvesterhci.io -n harvester-public GUI Go to Advanced -&amp;gt; Templates page Create a new template and set it as the default version Try to delete the default version Expected Results kube-api Default templates should exist: NAME TEMPLATE_ID VERSION AGE iso-image-base-version 1 39m raw-image-base-version 1 39m windows-iso-image-base-version 1 39m GUI Creating a new template should succeed Deleting the default version of a template should fail with: admission webhook &amp;ldquo;validator.</description>
    </item>
    
    <item>
      <title>VM Backup with metadata</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/vm_backup_metadata/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/vm_backup_metadata/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/988
Verify Items Metadata should be removed along with VM deleted Metadata should be synced after backup target switched Metadata can be used in new cluster Case: Metadata create and delete Install Harvester with any nodes Create an image for VM creation Setup NFS/S3 backup target Create a VM, then create a backup named backup1 File default-backup1.cfg should be exist in the backup target path &amp;lt;backup root&amp;gt;/harvester/vmbackups Delete the VM Backup backup1 File default-backup1.</description>
    </item>
    
    <item>
      <title>VM on error state</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/vm_on_error_state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/vm_on_error_state/</guid>
      <description>Ref:
https://github.com/harvester/harvester/issues/1446 https://github.com/harvester/harvester/issues/982 Verify Items Error message should displayed when VM can&amp;rsquo;t be scheduled VM&amp;rsquo;s state should be changed when host is down Case: Create a VM that no Node can host it Install Harvester with any nodes download a image to create VM create a VM with over-commit (consider to over-provisioning feature, double or triple the host resource would be more reliable.) VM should shows Starting state, and an alart icon shows aside.</description>
    </item>
    
    <item>
      <title>VM scheduling on Specific node</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/vm_schedule_on_node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/vm_schedule_on_node/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1350
Verify Items Node which is not active should not be listed in Node Scheduling list Case: Schedule VM on the Node which is Enable Maintenance Mode Install Harvester with at least 2 nodes Login and Navigate to Virtual Machines Create VM and Select Run VM on specific node(s)... All Active nodes should in the list Navigate to Host and pick node(s) to Enable Maintenance Mode Make sure Node(s) state changed into Maintenance Mode Repeat step 2 and 3 Picked Node(s) should not in the list Revert picked Node(s) to back to state of Active Repeat step 2 to 4 </description>
    </item>
    
    <item>
      <title>VM&#39;s CPU maximum limitation</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/vm_cpu_limits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/vm_cpu_limits/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1565
Verify Items VM&amp;rsquo;s maximum CPU amount should not have limitation. Case: Create VM with large CPU amount Install harvester with any nodes Create image for VM creation Create a VM with vCPU over than 100 Start VM and verify lscpu shows the same amount </description>
    </item>
    
    <item>
      <title>Volume size should be editable on derived template</title>
      <link>https://harvester.github.io/tests/manual/templates/derived_template_configure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/templates/derived_template_configure/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1711
Verify Items Volume size can be changed when creating a derived template Case: Update volume size on new template derived from exist template Install Harvester with any Nodes Login to Dashboard Create Image for Template Creation Create Template T1 with Image Volume and additional Volume Modify Template T1 with update Volume size Volume size should be editable Click Save, then edit new version of T1 Volume size should be updated as expected </description>
    </item>
    
  </channel>
</rss>
